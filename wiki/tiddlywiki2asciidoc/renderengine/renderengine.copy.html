== 
:Author: 
:Date Created: 
:Date Changed: 
:Count Changes: 
// 



<div title="CommandDefinition" modifier="Ichthyostega" modified="200907220310" created="200906140124" tags="SessionLogic spec draft decision design" changecount="10">
Commands can be identified and accessed _by name_ -- consequently there needs to be an internal command registry, including a link to the actual implementing function, thus allowing to re-establish the connection between command and implementing functions when de-serialising a persisted command. To create a command, we need to provide the following informations
* operation function actually implementing the command
* function to ,undo<<renderengine-UndoManager>> the effect of the command
* function to capture state to be used by UNDO.
* a set of actual parameters to bind into these functions (closure).

=== Command definition object
The process of creating a command by providing these building blocks is governed by a CommandDef helper object. According to the ,fluent definition style<<renderengine-http:_en.wikipedia.org/wiki/Fluent_interface>>, the user is expected to invoke a chain of definition functions, finally leading to the internal registration of the completed command object, which then might be dispatched or persisted. For example
[source,cpp]
----
CommandDefinition ("test.command1")
     .operation (command1::operate)          _ provide the function to be executed as command
     .captureUndo (command1::capture)        _ provide the function capturing Undo state
     .undoOperation (command1::undoIt)       _ provide the function which might undo the command
     .bind (obj, val1,val2)                  _ bind to the actual command parameters (stores command internally)
     .executeSync();                         _ convenience call, forwarding the Command to dispatch.
----

=== Operation parameters
While generally there is _no limitation_ on the number and type of parameters, the set of implementing functions and the +bind(...)+ call are required to match. Inconsistencies will be detected by the compiler. In addition to taking the _same parameters as the command operation,_ the +captureUndo()+ function is required to return (by value) a _memento_ type, which, in case of invoking the +undo()+-function, will be provided as additional parameter. To summarise:
<<renderengine-!Function<<renderengine-><<renderengine-!ret(params)<<renderengine-
<<renderengine- operation<<renderengine- void <<renderengine-(P1,..PN)<<renderengine-
<<renderengine- captureUndo<<renderengine- MEM <<renderengine-(P1,..PN)<<renderengine-
<<renderengine- undoOperation<<renderengine- void <<renderengine-(P1,..PN,MEM)<<renderengine-
<<renderengine- bind<<renderengine- void <<renderengine-(P1,..PN)<<renderengine-
Usually, parameters should be passed _by value_ -- with the exception of target object(s), which are typically bound as MObjectRef, causing them to be resolved at commad execution time (late binding).



<div title="CommandHandling" modifier="Ichthyostega" modified="200907212323" created="200906072048" tags="SessionLogic spec draft decision design img" changecount="26">
Organising any *mutating* operations executable by the user (via GUI) by means of the ,command pattern<<renderengine-http:_en.wikipedia.org/wiki/Command_pattern>> can be considered _state of the art_ today. First of all, it allows to discern the specific implementation operations to be called on one or several objects within the HighLevelModel from the operation requested by the user, the latter being rather a concept. A command can be labeled clearly, executed under controlled circumstances, allowing transactional behaviour.


=== Defining commands
image:../Structure of Commands../uml/fig134021.png[] Basically, a command could contain arbitrary operations, but we'll assume that it causes a well defined mutation within the HighLevelModel, which can be *undone*. Thus, when defining a command, we need to specify not only a function doing the mutation, but also another function which might be called later to reverse the effect of the action. Besides, the action operates on a number of *target* objects and additionally may require a set of *parameter* values. These are to be stored explicitly within the command object, thus creating a *closure* -- the operation _must not_ rely on other hidden parameters (maybe with the exception of very generic singleton system services?).

Theoretically, defining the +undo+ operation might utilise two different approaches:
* specifying an _inverse operation,_ known to cancel out the effect of the command
* capturing a _state memento,_ which can later be played back to restore the state found prior to executing the command.
While obviously the first solution is much simpler to implement on behalf of the command framework, the second solution has distinct advantages, especially in the context of an editing application: there might be rounding or calculation errors, the inverse might be difficult to define correctly, the effect of the operation might depend on circumstances, be random, or might even trigger a resolution operation to yield the final result. Thus the decision is within Lumiera Proc-Layer to _favour state capturing_ -- but in a modified, semi-manual and not completely exclusive way.

=== Undo state
While the usual »Memento« implementation might automatically capture the whole model (resulting in a lot of data to be stored and some uncertainty about the scope of the model to be captured), in Lumiera we rely instead on the client code to provide a *capture function*and a *playback function* alongside with the actual operation. To help with this task, we provide a set of standard handlers for common situations. This way, operations might capture very specific information, might provide an 'intelligent undo' to restore a given semantic instead of just a fixed value -- and moreover the client code is free actually to employ the 'inverse operation' model in special cases where it just makes more sense than capturing state.

=== Handling of commands
A command may be ,defined<<renderengine-CommandDefinition>> completely from scratch, or it might just serve as a CommandPrototype with specific targets and parameters. The command could then be serialised and later be recovered and re-bound with the parameters, but usually it will be handed over to the ProcDispatcher, pending execution. When *invoking*, the handling sequence is to ,log the command<<renderengine-SessionStorage>>, then call the *undo capture function*, followed from calling the actual *operation function*. After success, the logging and ,undo registration<<renderengine-UndoManager>> is completed. In any case, finally the *result signal* (a functor previously stored within the command) is emitted.

By design, commands are single-serving value objects; executing an operation repeatedly requires creating a collection of command objects, one for each invocation. While nothing prevents you from invoking the command operation functor several times, each invocation will overwrite the undo state captrued by the previous invocation. Thus, each command instance should bes seen as the promise (or later the trace) of a single operation execution. In a similar vein, the undo capturing should be defined as to be self sufficient, so that invoking just the undo functor of a single command performes any necessary steps to restore the situation found before invoking the corresponding mutation functor -- of course only _with respect to the topic covered by this command._ So, while commands provide a lot of flexibility and allow to do a multitude of things, certainly there is an intended CommandLifecycle.
-> more on possible ,command usage scenarios<<renderengine-CommandUsage>>



<div title="CommandLifecycle" modifier="Ichthyostega" modified="200907240010" created="200907210135" tags="SessionLogic spec draft design img" changecount="10">
[<img[Structure of Commands../uml/fig135173.png[]
While generally the command framework was designed to be flexible and allow a lot of different use cases, execution paths and to serve various goals, there is an *intended lifecycle* -- commands are expected to go through several distinct states.

The handling of a command starts out with a *command ID* provided by the client code. Command IDs are unique (human readable) identifiers and should be organised in a hierarchical fashion. When provided with an ID, the CommandRegistry tries to fetch an existing command definition. In case this fails, we enter the ,command definition stage<<renderengine-CommandDefinition>>, which includes specifying functions to implement the operation, state capturing and UNDO. When all these informations are available, the entity is called a *command definition*. Conceptually, it is comparable to a _class_ or _meta object._

By *binding* to specific operation arguments, the definition is _armed up_ and becomes a real *command*. This is similar to creating an instance from a class. Behind the scenes, storage is allocated to hold the argument values and any state captured to create the ability to UNDO the command's effect later on.

A command is operated or executed by passing it to an *execution pattern* -- there is a multitude of possible execution patterns to choose from, depending on the situation. 
[red]+WIP+
When a command has been executed (and maybe undone), it's best to leave it alone, because the UndoManager might hold a reference. Anyway, a *clone of the command* could be created, maybe bound with different arguments and treated separately from the original command.



<div title="CommandUsage" modifier="Ichthyostega" modified="200908091509" created="200907212338" tags="SessionLogic draft dynamic" changecount="8">
_for now (7/09) I'll use this page to collect ideas how commands might be used..._

* use a command for getting a log entry and an undo possibility automatically
* might define, bind and then execute a command at once
* might define it and bind it to a standard set of parameters, to be used as a prototype later.
* might just create the definition, leaving the argument binding to the actual call site
* execute it and check the success/failure result
* just enqueue it, without caring for successful execution
* place it into a command sequence bundle
* repeat the execution

===== a command definition....
* can be created from scratch, by ID
* can be re-accessed, by ID
* can't be modified once defined (this is to prevent duplicate definitions with the same ID)
* but can be dropped, which doesn't influence already existing dependent command instances
* usually will be the starting point for creating an actual command by _binding_

===== a command instance....
* normally emerges from a definition by binding arguments
* the first such binding will create a named command registration
* but subsequent accesses by ID or new bindings create an anonymous clone
* which then in turn might then be registered explicitly with a new ID
* anonymous command instances are managed by referral and ref-counting

===== an execution pattern....
* can only be defined in code by a class definition, not at runtime
* subclasses the HandlingPattern interface and uses an predefined ID (enum).
* a singleton instance is created on demand, triggered by referring the pattern's ID
* is conceptually _stateless_ -- of course there can be common configuration values
* is always invoked providing a concrete command instance to execute
* is configured into the command instance, to implement the command's invocation
* returns a duck-typed _result_ object




<div title="ConManager" modifier="Ichthyostega" modified="200810060300" created="200806050208" tags="def Builder" changecount="4">
The Connection Manager is a service for wiring connections and for querying information and deriving decisions regarding various aspects of data streams and the possibility of connections. The purpose of the Connection Manager is to isolate the ,Builder>>, which is client of this information and decision services, from the often convoluted details of type information and organizing a connection.

=== control connections
my intention was that it would be sufficient for the builder to pass an connection request, and the Connection Manager will handle the details of establishing a control/parameter link.
[red]+TODO: handling of parameter values, automation and control connections still need to be designed+

=== data connections
Connecting data streams of differing type involves a StreamConversion. Mostly, this aspect is covered by the ,stream type system<<renderengine-StreamType>>. The intended implementation will rely partially on ,rules<<renderengine-ConfigRules>> to define automated conversions, while other parts need to be provided by hard wired logic. Thus, regarding data connections, the ConManager can be seen as a specialized Facade and will delegate to the  -> ,stream type manager<<renderengine-STypeManager>>
* retrieve information about capabilities of a stream type given by ID 
* decide if a connection is possible
* retrieve a _strategy_ for implementing a connection



<div title="ConfigQuery" modifier="Ichthyostega" modified="200804110335" created="200801181308" tags="def" changecount="5">
Configuration Queries are requests to the system to 'create or retrieve an object with _this and that _ capabilities'. They are resolved by a rule based system ([red]+planned feature+) and the user can extend the used rules for each Session. Syntactically, they are stated in *prolog* syntax as a conjunction (=logical and) of *predicates*, for example +stream(mpeg), pipe(myPipe)+. Queries are typed to the kind of expected result object: +Query<Pipe> (+stream(mpeg)+)+ requests a pipe excepting/delivering mpeg stream data -- and it depends on the current configuration what +mpeg+ means. If there is any stream data producing component in the system, which advertises to deliver +stream(mpeg)+, and a pipe can be configured or connected with this component, then the ,defaults manager<<renderengine-DefaultsManagement>> will create/deliver a ,Pipe<<renderengine-PipeHandling>> object configured accordingly.
-> ,Configuration Rules system<<renderengine-ConfigRules>>
-> ,accessing and integrating configuration queries<<renderengine-ConfigQueryIntegration>>



<div title="ConfigQueryIntegration" modifier="Ichthyostega" modified="200804110350" created="200804070247" tags="overview draft impl img" changecount="17">
* planning to embed a YAP Prolog engine
* currently just integrated by a table driven mock
* the baseline is a bit more clear by now (4/08)

-> see also ConfigRules
-> see also DefaultsManagement

=== Use cases
[<img[when to run config queries../uml/fig131717.png[]

The key idea is that there is a Rule Base -- partly contained in the session (building on a stock of standard rules supplied with the application). Now, whenever there is the need to get a new object, for adding it to the session or for using associated with another object -- then instead of creating it by a direct hard wired ctor call, we issue a ConfigQuery requesting an object of the given type with some _capabilities_ defined by predicates. The same holds true when loading an existing session: some objects won't be loaded back blindly, rather they will be re-created by issuing the config queries again. Especially important is the case of (re)creating a ,processing pattern<<renderengine-ProcPatt>> guiding how to wire up the processing pipeline for some given media.

At various places, instead of requiring a fixed set of capabilities, it is possible to request a 'default configured' object instead, specifying just those capabilities we really need to be configured in a specific way. This is done by using the ,Defaults Manager<<renderengine-DefaultsManagement>> accessible on the ,Session>> interface. Such a default object query may either retrieve an already existing object instance, run further config queries, and finally result in the invocation of a factory for creating new objects -- just as necessary and guarded by the rules.

@@clear(left):display(block):@@

=== Components and Relations
image:../participating classes../uml/fig131461.png[]

<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>

Access point is the interface +ConfigRules+, which allowes to resolve a ConfigQuery resulting in an object with properties condigured such as to fulfill the query. This whole subsystem employes quite some generic programming, because actually we don't deal with 'objects', but rather with similar instantiations of the same functionality for a collection of different object types. For the purpose of resolving these queries, the actual kind of object is not so much of importance, but on the caller sinde, of course we want to deal with the result of the queries in a typesafe manner.
Examples for _participating object kinds_ are ,pipes<<renderengine-Pipe>>, ,processing patterns<<renderengine-ProcPatt>>, effect instances, ,tags<<renderengine-Tag>>, ,labels<<renderengine-Label>>, ,automation data sets<<renderengine-AutomationData>>,...
@@clear(right):display(block):@@
For this to work, we need each of the _participating object types_ to implement a generic interface +TypeHandler+, which will provide actual C/C++ implementations for the predicates usable on objects of this type within the Prolog rules. The implementation has to make sure that alongside with each config query, there are additional _type constraints_ to be regarded. For example, if the client code runs a +Query<Pipe>+, an additional _type guard_ (implemented by a predicate +type(pipe)+ has to be inserted, so only rules and facts in accordance with this type will be used for resolution.


=== when querying for a ,+default+<<renderengine-DefaultsManagement>> object
[<img[colaboration when issuing a defaults query../uml/fig131845.png[]

@@clear(left):display(block):@@


<div title="ConfigRules" modifier="Ichthyostega" modified="200804110334" created="200801171352" tags="overview spec" changecount="13">
Many features can be implemented by specifically configuring and wiring some unspecific components. Rather than tie the client code in need of some given feature to these configuration internals, in Lumiera the client can _query _ for some kind of object providing the _needed capabilities. _ Right from start (summer 2007), Ichthyo had the intention to implement such a feature using sort of a *declarative database*, e.g. by embedding a Prolog system. By adding rules to the basic session configuration, users should be able to customize the semi-automatic part of Lumiera's behaviour to great extent.

,Configuration Queries<<renderengine-ConfigQuery>> are used at various places, when creating and adding new objects, as well when building or optimizing the render engine node network.
* Creating a ,pipe<<renderengine-PipeHandling>> queries for a default pipe or a pipe with a certain stream type
* Adding a new ,track<<renderengine-TrackHandling>> queries for some default placement configuration, e.g. the pipe it will be plugged to.
* when processing a ,wiring request<<renderengine-WiringRequest>>, connection possibilities have to be evaluated.
* actually building such a connection may create additional degrees of freedom, like panning for sound or layering for video.

=== anatomy of a Configuration Query
The query is given as a number of logic predicates, which are required to be true. Syntactically, it is a string in prolog syntax, e.g. +stream(mpeg)+, where +stream+ is the _predicate, _ meaning here 'the stream type is...?' and +mpeg+ is a _term _ denoting an actual property, object, thing, number etc -- the actual kind of stream in the given example. Multible comma separated predicates are combined with logical +and+. Terms may be _variable _ at start, which is denoted syntactically by starting them with a uppercase letter. But any variable term need to be _bound _  to some known value while computing the solution to the query, otherwise the query fails. A failed query is treated as a local failure, which may cause some operation being aborted or just some other possibility being chosen.
Queries are represented by instantiations of the +Query<TYPE>+ template, because their actual meaning is 'retrieve or create an object of TYPE, configured such that...!'. At the C++ side, this ensures type safety and fosters programming against interfaces, while being implemented rule-wise by silently prepending the query with the predicate +object(TYPE)+

=== executing a Configuration Query
Actually posing such an configuration query, for example to the ,Defaults Manager in the Sessison<<renderengine-DefaultsManagement>>, may trigger several actions: First it is checked against internal object registries (depending on the target object type), which may cause the delivery of an already existing object (as reference, clone, or smart pointer). Otherwise, the system tries to figure out an viable configuration for a newly created object instance, possibly by issuing recursive queries. In the most general case this may silently impose additional decisions onto the _execution context _ of the query -- by default the session.

=== Implementation
At start and for debugging/testing, there is an *dummy* implementation using a map with predefined queries and answers. But for the real system, the idea is to embed a *YAP Prolog* engine to run the queries. This includes the task of defining and loading a set of custom predicates, so the rule system can interact with the object oriented execution environment, for example by transforming some capability predicate into virtual calls to a corresponding object interface. We need a way for objects to declare some capability predicates, together with a functor that can be executed on an object instance (and further parameters) in the cause of the evaluation of some configuration query. Type safety and diagnostics play an important role here, because effectively the rule base is a pool of code open for arbitray additions from the user session.
-> ,considerations for a Prolog based implementation<<renderengine-QueryImplProlog>>
-> ,accessing and integrating configuration queries<<renderengine-ConfigQueryIntegration>>
-> see +src/common/query/mockconfigrules.cpp+ for the table with the hard wired (mock) answers


[red]+WARN+ there is an interference with the (planned) Undo function: a totally correct implementation of +Undo+ would need to remember and restore the internal state of the query system (similar to backtracking). But, more generally, such an correct implementation is not achievable, because we are never able to capture and control the whole state of a real world system doing such advanced things like video and sound processing. Seemingly we have to accept that after undoing an action, there is no guarantee we can re-do it the same way as it was first time.



<div title="Controller" modifier="Ichthyostega" modified="200712090624" created="200706220319" tags="def" changecount="4">
Here, in the context of the Render Engine, the Controller component is responsible for triggering and coordinating the build process and for activating the backend and the Render Engine configuration created by the Builder to carry out the actual rendering. There is another Controller in the backend, the PlaybackController, which is in charge of the playback/rendering/display state of the application, and consequently will call this (Proc-Layer) Controller to get the necessary Render Engine.

=== Facade
This is an very important external Interface, because it links together all three Layers of our current architecture. It can be used by the backend to initiate ,Render Processes (=StateProxy)<<renderengine-StateProxy>> and it will probably be used by the Dispatcher for GUI actions as well...



<div title="CurrentSession" modifier="Ichthyostega" modified="200709272058" created="200709272057" tags="decision design" changecount="2">
The question is where to put all the state-like information ,associated with the current session<<renderengine-SessionOverview>>. Because this is certainly 'global', but may depend on the session or need to be configured differently when loading another session. At the moment (9/07) Ichthyo considers the following solution:
* represent all configuration as ,Asset>>s
* find a way [red]+TODO+ how to reload the contents of the ,AssetManager>>.
* completely hide the Session object behind a *PImpl* smart pointer, so the session object can be switched when reloading.
* the ,Fixture>> acts as isolation layer, and all objects refered from the Fixture are refcounting smart pointers. So, even when the session gets switched, the old objects remain valid as long as needed.


<div title="DefaultTiddlers" modifier="Ichthyostega" modified="200802031805" created="200706172308" changecount="6">
,ProcLayer and Engine>>


<div title="DefaultsManagement" modifier="Ichthyostega" modified="200803212244" created="200801121708" tags="def spec" changecount="12">
For several components and properties there is an implicit default value or configuration; it is stored alongside with the session. The intention is that defaults never create an error, instead, they are to be extended silently on demand. Objects configured according to this defaults can be retrieved at the ,Session>> interface by a set of overloaded functions +Session::current->default(Query<TYPE> ("query string"))+, where the _query string _ defines a capability query similar to what is employed for pipes, stream types, codecs etc. This query mechanism is implemented by ,configuration rules<<renderengine-ConfigRules>>

=== what is denoted by +default+?
+default(Obj)+ is a predicate expressing that the object +Obj+ can be considered the default setup under the given conditions. Using the _default_ can be considered as a shortcut for actually finding a exact and unique solution. The latter would require to specify all sorts of detailed properties up to the point where only one single object can satisfy all conditions. On the other hand, leaving some properties unspecified would yield a set of solutions (and the user code issuing the query had to provide means for selecting one soltution from this set). Just falling back on the _default_ means that the user code actually doesn't care for any additional properties (as long as the properties he _does_ care for are satisfied). Nothing is said specifically on _how_ this default gets configured; actually there can be rules _somewhere,_ and, additionally, anything encountered once while asking for a default can be re-used as default under similar circumstances.
-> ,implementing defaults<<renderengine-DefaultsImplementation>>



<div title="DesignDecisions" modifier="Ichthyostega" modified="200809230234" created="200801062209" tags="decision design discuss" changecount="20">
Along the way of working out various ,implementation details<<renderengine-ImplementationDetails>>, decisions need to be made on how to understand the different facilities and entities and how to tackle some of the problems. This page is mainly a collection of keywords, summaries and links to further the discussion. And the various decisions should allways be read as proposals to solve some problem at hand...

*Everything is an object* -- of course, that's a _no-brainer _ todays. Rather, important is what is not 'an object', meaning it can't be arranged arbitrarily
* we have one and only one global ,Session>> which directly contains a collection of multiple ,EDLs<<renderengine-EDL>> and is associated with a globally managed collection of ,assets<<renderengine-Asset>> (no, we don't have scoped variables here; if e.g. a media has been 'opened', it is just plain globally known as asset.)
* we have some global ,pipes<<renderengine-Pipe>>, which are treated in a unique manner and don't behave like other objects (e.g. effects)
* we have a ,Fixture>> which acts as isolation layer towards the render engine and is (re)built automatically.

We *separate* processing (rendering) and configuration (building). We have a ,Builder>> which creates a network of ,render nodes<<renderengine-ProcNode>>, to be processed by _pulling data _ from some ,Pipe>>

*Objects are ,placed<<renderengine-Placement>> rather* than assembled, connected, wired, attached. This is more of a rule-based approach and gives us one central metaphor and abstraction, allowing us to treat everything in an uniform manner. You can place it as you like, and the builder tries to make sense out of it, silently disabling what doesn't make sense.
An ,EDL>> is just a collection of configured and placed objects (and has no additional, fixed structure). ,Tracks<<renderengine-Track>> form a mere organisational grid, they are grouping devices not first-class entities (a track doesn't _have_ a pipe or _is_ a video track and the like; it can be configured to behave in such manner by using placements though). ,Pipes<<renderengine-Pipe>> are hooks for making connections and are the only facility to build processing chains. We have global pipes, and each clip is built around a lokal ,source port<<renderengine-ClipSourcePort>> -- and that's all. No special 'media viewer' and 'arranger', no special role for media sources, no commitment to some fixed media stream types (video and audio). All of this is sort of pushed down to be configuration, represented as asset of some kind. For example, we have ,processing pattern<<renderengine-ProcPatt>> assets to represent the way of building the source network for reading from some media file (including codecs treated like effect plugin nodes)

*State* is rigorously *externalized* and operations are to be *scheduled*, to simplify locking and error handling. State is either treated similar to media stream data (as addressable and cacheable data frame), or is represented as 'parameter' to be served by some ,parameter provider<<renderengine-ParamProvider>>. Consequently, ,Automation>> is just another kind of parameter, i.e. a function -- how this function is calculated is an encapsulated implementation detail (we don't have 'bezier automation', and then maybe a 'linear automation', a 'mask automation' and yet another way to handle transitions)

Deliberately there is an limitaion on the flexibility of what can be added to the system via Plugins. We allow configuration and parametrisation to be extended and we allow processing and data handling to be extended, but we disallow extensions to the fundamental structure of the system by plugins. They may provide new implementations for already known subsystems, but they can't introduce new subsystems not envisioned in the general design of the application.
* thus fixed assortments include: the possbile kinds of MObject and ,Asset>>, the possible automation parameter data types, the supported kinds of plugin systems
* while plugins may extend: the supported kinds of media, the ,media handling libraries<<renderengine-MediaImplLib>> used to deal with those media, the session storage backends, the behaviour of placments



<div title="DesignGoals" modifier="Ichthyostega" modified="200805300046" created="200706210557" tags="design" changecount="20">
This +proc-Layer+ and +Render-Engine+ implementation started out as a design-draft by ,Ichthyo<<renderengine-mailto:Ichthyostega@web.de>> in summer 2007. The key idea of this design-draft is to use the ,Builder Pattern<<renderengine-http:_en.wikipedia.org/wiki/Builder_pattern>> for the Render Engine, thus separating completely the _building_ of the Render Pipeline from _running,_ i.e. doing the actual Render. The Nodes in this Pipeline should process Video/Audio and do nothing else. No more decisions, tests and conditional operations when running the Pipeline. Move all of this out into the configuration of the pipeline, which is done by the Builder.

=== Why doesn't the current Cinelerra-2 Design succeed?
The design of Cinelerra-2 basically follows a similar design, but 'fails because of two reasons'
. too much differentiation is put into the class hierarchy instead of configuring Instances differently.<br>This causes overly heavy use of virtual functions and -- in order to ameliorate this -- falling back to hard wired branching
. far too much coupling and back-coupling to the internals of the EDL, forcing a overly rigid structure on the latter

=== Try to learn from the Problems of the current Cinelerra-2 codebase
* build up an ,Node Abstraction<<renderengine-ProcNode>> powerful enough to express _all necessary Operations_ without the need to recure on the actual object type
* need to redesign the internals of the EDL in a far more open manner. Everything is an ,M-Object<<renderengine-MObjects>> which is ,placed<<renderengine-Placement>> in some manner
* strive at a StrongSeparation between EDL and Render Engine, encapsulate and allways implement against interfaces


=== Design Goals
As always, the main goal is _to cut down complexity_ by the usual approach to separate into small manageable chunks.

To achieve this, here we try to separate *Configuration* from *Processing*. Further, in Configuration we try to separate the *high level view* (users view when editing) from the *low level view* (the actual configuration effective for the calculations). Finally, we try to factor out and encapsulate *State* in order to make State explicit.

The main tool used to implement this separation is the ,Builder Pattern<<renderengine-http:_en.wikipedia.org/wiki/Builder_pattern>>. Here especially we move all decisions and parametrization into the BuildProcess. The Nodes in the render pipeline should process Video/Audio and do nothing else. All decisions, tests and conditional operations are factored out of the render process and handled as configuration of the pipeline, which is done by the Builder. The actual color model and number of ports is configured in by a pre-built wiring descriptor. All Nodes are of equal footing with each other, able to be connected freely within the limitations of the necessary input and output. OpenGL and renderfarm support can be configured in as an alternate implementation of some operations together with an alternate signal flow (usable only if the whole Pipeline can be built up to support this changed signal flow), thus factoring out all the complexities of managing the data flow between core and hardware accelerated rendering. We introduce separate control data connections for the ,automation data<<renderengine-Automation>>, separating the case of true multi-channel-effects from the case where one node just gets remote controlled by another node (or the case of two nodes using the same automation data).

Another pertinent theme is to make the basic building blocks simpler, while on the other hand gaining much more flexibility for combining these building blocks. For example we try to unfold any 'internal-multi' effects into separate instances (e.g. the possibility of having an arbitrary number of single masks at any point of the pipeline instead of having one special masking facility encompassing multiple sub-masks. Similarly, we treat the Objects in the EDL in a more uniform manner and gain the possibility to ,place<<renderengine-Placement>> them in various ways.




<div title="DisplayFacade" modifier="Ichthyostega" modified="200904302316" created="200902080703" tags="spec" changecount="2">
LayerSeparationInterface provided by the GUI.
Access point especially for the playback. A render- or playback process uses the DisplayFacade to push media data up to the GUI for display within a viewer widget of full-screen display. This can be thought off as a callback mechanism. In order to use the DisplayFacade, client code needs a DisplayerSlot (handle), which needs to be set up by the UI first and will be provided when starting the render or playback process.



<div title="DisplayService" modifier="Ichthyostega" created="200902080711" tags="def" changecount="1">
A service within the GUI to manage output of frames generated by the lower layers of the application.
*providing the actual implementation of the DisplayFacade
*creating and maintaining of ,displayer slots<<renderengine-DisplayerSlot>>, connected to viewer widgets or similar
</pre>
</div>
<div title="DisplayerSlot" modifier="Ichthyostega" modified="200902080707" created="200902080705" tags="def" changecount="2">
An output port wired up to some display facility or viewer widget within the UI. For the client code, each slot is represented by a handle, which can be used to lock into this slot for an continuous output process. Managed by the DisplayService



<div title="EDL" modifier="Ichthyostega" modified="200811011755" created="200706210610" tags="def" changecount="9">
<<renderengine-<
[red]+WARNING: Naming is currently being discussed (11/08)+
* ,EDL>> probably will be called *Sequence* (or maybe *Arrangement*)
* ,Session>> maybe renamed to *Project*
* there seems to be a new entity called ,Timeline>> which holds the global Pipes
<<renderengine-<


+EDL+ is a short-hand for ++E++dit ++D++ecision ++L++'ist. The use of this term can be confusing; for the usual meaning see the definition in ,Wikipedia<<renderengine-http:_en.wikipedia.org/wiki/Edit_decision_list>>

Cinelerra uses this term in a related manner but with a somewhat shifted focus (and we just stick to this usage here): In Lumiera the EDL is comprised of the whole set of clips and other media objects parametrized and placed onto the tracks by the user. It is the result of the user's _editing efforts._

In this usage, the EDL in most cases will be almost synonymous to the ,Session<<renderengine-SessionOverview>>, just the latter emphasizes more the state aspect, as it can be thought as the current EDL contents contained in a file or data structure together with additional Option values and settings for the GUI. The Session is what you save and load, while the EDL rather denotes a structured collection of Objects placed in time.

The EDL within the Session is just a _logical grouping_, allowing for lots of flexibility. Actually, we can have several EDLs within one Session, and these EDLs can be linked together or not, they may be in sequence or may constitue a logical grouping of clips used simultanously in compositional work etc. Multiple EDLs can use the same or different tracks, and tracks as well are only an organisational (grouping) device. But at any time, we have exactly one ,Fixture>>, derived automatically from all EDLs and containing the content actually to be rendered.

-> see considerations about ,the role of Tracks and Pipes in conjunction with the EDL<<renderengine-TrackPipeEDL>>



<div title="EditingOperations" modifier="Ichthyostega" modified="200710111508" created="200709251610" tags="design decision" changecount="5">
These are the tools provided to any client of the Proc layer for handling and manipulating the entities in the EDL. When defining such operations, _the goal should be to arrive at some uniformity in the way things are done._ Ideally, when writing client code, one should be able to guess how to achieve some desired result.

=== guiding principle
The approach taken to define any operation is based primarily on the *OO-way of doing things*: entities operate themselfs. You don't advise some manager, session or other >god class< to manipulate them. And, secondly, the scope of each operation will be as large as possible, but not larger. This often means performing quite some elementary operations -- sometimes a convenience shortcut provided by the higher levels of the application may come in handy -- and basically this gives rise to several different paths of doing the same thing, all of which need to be equivalent.

=== main tasks
* you *create a clip* either from a source media or from another clip (copy, maybe clone?). The new clip always reflects the full size (and other properties) of the source used for creating.
* you can request a clip to *resize*, which always relates to its current dimensions.
* you can *place or attach* the clip to some point or other entity by creating a ,Placement>> from the clip. ( -> ,handling of Placements<<renderengine-PlacementHandling>>)
* you can *adjust* a placement relative to some other placement, meta object (i.e. selection), label or media, and this may cause the placement to resize or even delete the clip as necessary
* you can perform *adjustments* on the whole EDL.
All these operations propagate to directly dependant objects and may schedule global reconfigurations.

=== state, locking, policies
While performing any manipulative task, the state of the object is considered inconsistent, but it is guaranteed to be consistent after any such operation, irrespective of the result. There is no automatic atomicity and, consequently each propagation is considered a separate operation.

==== parallelism
At the level of fine grained operations on individual entities, there is *no support for parallelism*. Individual entities don't lock themselves. Tasks perform locking and are to be scheduled. Thus, we need an isolation layer towards all inherently multithreaded parts of the system, like the GUI or the renderengine.

This has some obvious and some subtle consequences. Of course, manipulating _tasks_ will be queued and scheduled somewhere. But as we rely on object identity and reference semantics for the entities in the EDL, some value changes are visible to  operations going on in parallel threads (e.g. rendering) and each operation on the EDL entities _has to be aware of this._
Consequently, sometimes there needs to be done sort of a *read-copy-update*, i.e. self-replacement by copy followed by manipulation of the new copy, while ongoing processes use the unaltered original object until they receive some sort of reset.

==== undo
Basically, each elementary operation has to record the informations necessary to be undone. It does so by registering a Memento with some central UndoManager facility. This Memento object contains a functor pre-bound with the needed parameter values. (Besides, the UndoManager is free to implement a second level of security by taking independent state snapshots). 
[red]+to be defined in more detail later...+



<div title="EffectHandling" modifier="Ichthyostega" modified="200810170025" created="200810162300" tags="design dynamic" changecount="3">
We have to deal with effects on various different levels. One thing is to decide if an effect is applicable, another question is to find out about variable and automatable parameters, find a module which can be used as an effect GUI and finally to access a processing function which can be executed on a buffer filled with suitable data.

The effect asset (which is a ,processing asset<<renderengine-ProcAsset>>) provides an unified interface for loading external processing modules and querying their properties. As usual with assets, this is the 'bookkeeping view' to start with, but we can create a _processor_ from such an asset, i.e. an instance of the processing facility which can be used and wired into the model. Such a processor is an MObject and can be placed into the session (high level model); moreover it holds a concrete wiring for the various control parameters and it has an active link to the effect GUI module. As every MObject, it could be placed multiple times to affect several pipes, but all those different Placements would behave as being linked together (with respect to the control parameter sources and the GUI)

When building the low-level model, the actual processing code is resolved and a processing function is installed into the processing node representing the effect. This step includes checking of actual ,media type information<<renderengine-StreamType>>, which may result in selecting a specifically tailored implementation function. The parameter wiring on the other hand is _shared_ between the effect MObject, the corresponding GUI module and all low-level processing nodes. Actually, parameters are more of a reference than actually being values: they provide a +getValue()+ function, which also works with automation. This way, any parameter or automation changes are reflected immediately into the produced output.

Initially, only the parameter (descriptors) are present on the effect MObject, while the actual ,parameter providers<<renderengine-ParamProvider>> are created or wired (by the ConManager) on demand.



<div title="Example1" modifier="Ichthyostega" modified="200906071812" created="200706220239" tags="example img" changecount="5">
The >Timeline< is a sequence of MObjects -- here clips -- together with an ExplicitPlacement, locating each clip at a given time and track. (Note: I simplified the time format and wrote frame numbers to make it more clear)
image::Example1: Objects in the EDL/Fixture../uml/fig128773.png[]

----
After beeing processed by the Builder, we get the following Render Engine configuration
[red]+note: please take this only as a 'big picture', the implementation details got a lot more complicated as of 6/08+
image::Example1: generated Render Engine../uml/fig129029.png[]



<div title="Example2" modifier="Ichthyostega" modified="200906071812" created="200706220251" tags="example img" changecount="4">
[red]+TODO: seemingly this example is slightly outdated, as the implementation of placements is now indirect via LocatingPin objects+
This Example showes the _high level_ EDL as well. This needs to be transformed into a Fixture by some facility still to be designed. Basically, each ,Placement>> needs to be queried for this to get the corresponding ExplicitPlacement. The difficult part is to handle possible Placement constraints, e.g. one clip can't be placed at a timespan covered by another clip on the same track. In the current Cinelerra2, all of this is done directly by the GUI actions.

The >Timeline< is a sequence of MObjects -- note: using the same Object instances -- but now with the calculated ExplicitPlacement, locating the clip at a given time and track. The effect is located absolutely in time as well, but because it is the same Instance, it has the pointer to the RelativePlacement, wich basically attaches the effect to the clip. This structure may look complicated, but is easy to process if we go 'backward' and just rely on the information contained in the ExplicitPlacement.
image::Example2: Clip with Effect and generated Fixture for this EDL../uml/fig128901.png[]

----
After beeing processed by the Builder, we get a Render Engine configuration.<br>
It has to be segmented at least at every point with changes in the configuration, but some variations are possible, e.g. we could create a Render Engine for every Frame (as Cinelerra2 does) or we could optimize out some configurations (for example the effect extended beyond the end of the clip)
[red]+note: as of 6/08 this can be taken only as the 'big picture'. Implementation will differ in details, and is more complicated than showed here+
image::Example2: generated Render Engine../uml/fig129157.png[]



<div title="Examples" modifier="MichaelPloujnikov" modified="200706271425" created="200706220233" tags="example" changecount="6">
!MObject assembly
To make the intended use of the classes more clear, consider the following two example Object graphs:
* a video clip and a audio clip placed (explicitly) on two tracks  ->,Example1>>
* a video clip placed relatively, with an attached HUE effect  ->,Example2>>



<div title="ExitNode" modifier="Ichthyostega" created="200706220322" tags="def" changecount="1">
a special ProcNode which is used to pull the finished output of one Render Pipeline (Tree or Graph). This term is already used in the Cinelerra2 codebase. I am unsure at the moment if it is a distinct subclass or rahter a specially configured ProcNode (a general design rule tells us to err in favour of the latter if in doubt).



<div title="ExplicitPlacement" modifier="MichaelPloujnikov" modified="200706271458" created="200706220304" tags="def" changecount="2">
A special kind (subclass) of ,Placement>>. As such it is always linked to a _Subject_, i.e. a MObject. In addition to the properties of a (unspecific) Placement, the ExplicitPlacement specifies a absolute time and track position for locating the Subject



<div title="Factories" modifier="Ichthyostega" modified="200906060436" created="200708100401" tags="impl discuss excludeMissing" changecount="23">
We use Factories
* for centralizing ,memory management<<renderengine-MemoryManagement>>
* to support polymorphism (of course...)

=== Requirements
* request the actual placement/allocation from the backend
* allways hand out a smart-pointer
* encapsulate / make configurable the smart-pointer type
* install a callback into the smart-pointer for destroying the resource.
* redirect the destroying request to the backend

=== Implementation Questions
* how much genericity? (Ichthyo is rather inclined not to overdo this one. Often it is preferable to have repeated implementations follow a well known pattern, if this leads to short and simple implementations, while the complete general solution will be much more contrived).
* how to specify the actual type needed?
* how to implement the cases where a subtype needs to be selected (abstract factory pattern). Embody this into the Factory, pass it in as a Strategy or treat the Factory just as a simple service taking an explicit type-ID and providing the new object?

==== chosen design
My main goal is to have an easy-to-use interface for the implementer of individual classes using this factory mechanism. The intended use should mimic the standard use of operator new, and at the same time there should be a possibility to configure »real« Factory behaviour in.

For this reason I make Factory a Functor, so it can be incorporated as a member into another class, while still looking like a function call to the client code. The users of this factory template typically either parametrize it with existing smart-pointer types, or they may chose to create a specialized Factory derived from this Factory template. After typically hiding this configuration behind a typedef, the user adds a static field to the class intended to use the Factory and initializes this field with the concrete Factory (this may pass internal IDs to the constructor of the Factory and from there on to the underlying Allocator).

[source,cpp]
----
.include "common/factory.hpp"

  class Product
    {
      int wow_;

    public:
      typedef lumiera::factory::RefcntFactory<Product> Factory;
      static Factory create;
      
      Product() : wow_(42) {} ;
    };
  
  /** storage for a static Factory instance 
   *  for creating refcounting Ptrs to Product objects 
   */
  Product::Factory Product::create;  _ <----note this is a ctor call
----

Now, the clients of +class Product+ can create ref-counting pointers to Product-objects by doing a fully qualified +create()+ functor call:

[source,cpp]
----
  std::tr1::shared_ptr<Product> huii = Product::create ();  _ <----will invoke the default Product() ctor 
  std::tr1::shared_ptr<Product> pfuii = huii;
----

Some further details
* the product class (or base class) cares for using an custom allocator via an overloaded +operator new(size_t)+ if applicable.
* thus, when Functor or any derived class issues a new XX(), our custom Allocator gains control
* the Functor-behaviour relies on a custom +operator()+ which can be overridden in derived classes to take various parameter lists.
* additionally, such a Factory class derived from Functor can do specific magic and e.g. create some subclass
* and, as the created smart-pointer is a template template parameter, such a custom Functor can create all sorts of Proxies, wrappers and the like
* a special case of this factory use is the ,Singleton>> factory, which is used a lot within the Proy-Layer code



<div title="Fixture" modifier="Ichthyostega" modified="200712100439" created="200706220324" tags="def" changecount="4">
a specially configured EDL
 * all MObjects have their position, length and configuration set up ready for rendering.
 * compound objects (e.g. multichannel clips) have been resolved to single non-compound basic objects.
 * every MObject is associated with an ExplicitPlacement, which declares a fixed position (Time, Track)
 * this ExplicitPlacements are contained in a ordered List called the Timeline

As the builder and thus render engine _only consults the fixture,_ while all editing operations finally propagate to the fixture as well, we get an isolation layer between the high level part of the Proc layer (editing, object manipulation) and the render engine. Creating the Fixture can be seen as a preprocessing step to simplify the build process. For this reason, the process of ,(re)building the fixture<<renderengine-PlanningBuildFixture>> has been designed together with the ,Builder>>


<div title="Frame" modifier="Ichthyostega" modified="200706220333" created="200706220332" tags="def" changecount="2">
This term has _two meanings, _so care has to be taken for not confusing them.
. in general use, a Frame means one full image of a video clip, i.e an array of rows of pixels. For interlaced footage, one Frame contains two halfimages, commonly called Fields. (Cinelerra2 confuses this terms)
. here in this design, we use Frame as an abstraction for a buffer of raw media data to be processed. If in doubt, we should label this +Dataframe+.
* one video Dataframe contains a single video frame
* one audio Dataframe contains a block of raw audio samples
* one OpenGL Dataframe could contain raw texture data (but I am lacking expertise for this topic)



<div title="GAVL" modifier="Ichthyostega" created="200809220251" tags="def" changecount="1">
The *Gmerlin Audio Video Library*.  -> see ,homepage<<renderengine-http:_gmerlin.sourceforge.net/gavl.html>>
Used within Lumiera as a foundation for working with raw video and audio media data



<div title="GOPs" modifier="Ichthyostega" modified="200706220333" created="200706220301" tags="def" changecount="2">
++G++roup +of+ ++P++ictures: several compressed video formats don't encode single frames. Normally, such formats are considered mere _delivery formates_ but it was one of the key strenghts of Cinelrra from start to be able to do real non linear editing on such formats (like the MPEG2-ts unsed in HDV video). The problem of course is that the data backend needs to decode the whole GOP to be serve  single raw video frames.

For this Lumiera design, we could consider making GOP just another raw media data frame type and integrate this decoding into the render pipeline, similar to an effect based on several source frames for every calculated output frame.

->see in ,Wikipedia<<renderengine-http:_en.wikipedia.org/wiki/Group_of_pictures>>



<div title="GuiCommunication" modifier="Ichthyostega" modified="200812050555" created="200812050543" tags="GuiIntegration draft" changecount="2">
All communication between Proc-Layer and GUI has to be routed through the respective LayerSeparationInterfaces. Following a fundamental design decision within Lumiera, these interface are _intended to be language agnostic_ -- forcing them to stick to the least common denominator. Which creates the additional problem of how to create a smooth integration without forcing the architecture into functional decomposition style. To solve this problem, we rely on the well known solution of using a 'business facade' and delegation proxies.
Thus, the Proc-Layer exposes (one or several) facade interfaces for the GUI to use it's functionality, and similarily the GUI provides a ,notification facade<<renderengine-GuiNotificationFacade>> for pushing back status information created as result of the edit operations, the build process and the render tasks.

=== anatomy of the Proc/GUI interface
* the GuiFacade is used as a general lifecycle facade to start up the GUI and to set up the LayerSeparationInterfaces.
* GuiFacade is implemented by a class _in core_ and exposes a notification proxy implementing GuiNotificationFacade, which actually is wired through the InterfaceSystem to forward to the corresponding implementation facade object within the GUI
* similarly, starting the fundamental subsystems within proc installs Interfaces into the InterfaceSystem and exposes them through proxy objects



<div title="GuiFacade" modifier="Ichthyostega" created="200902080719" tags="def spec" changecount="1">
special LayerSeparationInterface which serves the main purpose to load the GuiStarterPlugin, thus bringing up the Lumiera GTK UI at application start.



<div title="GuiIntegration" modifier="Ichthyostega" modified="200904242106" created="200812050532" tags="overview" changecount="3">
Considering how to interface to and integrate with the GUI Layer. Running the GUI is _optional,_ but it requires to be ,started up<<renderengine-GuiStart>>, installing the necessary LayerSeparationInterfaces. As an example how to integrate the GUI with the lower layers, in 1/2009 we created an PlayerDummy, which 'pulls' dummy frames from the (not yet existing) engine and displays them within an XV viewer widget.

Probably the most important aspect regarding the GUI integration is how to get ,access to and operate on the Session<<renderengine-SessionInterface>>. More specifically, this includes ,referring to individual objects<<renderengine-MObjectRef>>.


<div title="GuiNotificationFacade" modifier="Ichthyostega" created="200902080659" tags="spec" changecount="1">
LayerSeparationInterface provided by the GUI.
Access point for the lower layers to push information and state changes (aynchronously) to the GUI. Actually, most operations within Lumiera are initiated by the user through the GUI. In the course of such actions, the GUI uses the services of the lower layer and typically recieves an synchronous response. In some exceptional cases, these operations may cause additional changes to happen asynchronously from the GUI's perspective. For example, an edit operation might trigger a re-build of the low-level model, which then detects an error.



<div title="GuiStart" modifier="Ichthyostega" modified="200902080720" created="200812050525" tags="GuiIntegration" changecount="9">
Starting up the GUI is optional and is considered part of the Application start/stop and lifecycle.
* main and AppState activate the lifecyle methods on the GuiSubsysDescriptor, accessible via the GuiFacade
* loading a GuiStarterPlugin actually
** loads the GUI (shared lib)
** creates instances of all the GUI services available through LayerSeparationInterfaces
*** GuiNotificationFacade
*** DisplayFacade
** Finally, the GTK-main() is invoked.

=== public services
The GUI provides a small number of public services, callable through LayerSeparationInterfaces. Besides that, the main purpose of the GUI of course is user interaction. Effectively the behaviour of the whole system is driven by GUI events to a large extent. These events are executed within the event handling thread (GTK-main-Thread) and may in turn invoke services of the lower layers, again through the respective LayerSeparationInterfaces.
But the question of special interest here is how the _public services_ of the GUI are implemented and made accessible for the lower Layers. Layer isolation is an issue here. If implemented in a rigorous manner, no facility within one layer may invoke implementation code of another layer directly. In practice, this tends to put quite some additional burden on the implementer, without and obvious benefit. Thus we decided to lower the barrier somewhat: while we still require that all service invoking calls are written against an public LayerSeparationInterface, actually the GUI (shared lib) is _linked_ against the respective shared libs of the lower layers, thus especially enabling the exchange of iterators, closures and functor objects.

Note that we retain strict isolation in the other direction: no part of the lower layers is allowed to call directly into the GUI. Thus it's especially interesting how access to some GUI public service from the lower layers works in detail.
* when the GUI plugin starts, instances of the Services implementing those public service interfaces are created.
* these service objects in turn hold an InstanceHandle, which cares to register and open the corresponding C Language Interface
* additionally this InstanceHandle is configured such as to create an 'facade proxy' object, which is implemented within liblumieracommon.so
Now, when invoking an operation on some public interface, the code in the lower layers actually executes an implementation of this operation _on the facade proxy,_ which in turn forwards the call through the CL interface into the GUI, where it is actually implemented by the corresponding service object instance.



<div title="GuiStarterPlugin" modifier="Ichthyostega" created="200902080716" tags="def GuiIntegration" changecount="1">
A specially configured LumieraPlugin, which actually contains or loads the complete code of the (GTK)GUI, and additionally is linked dynamically against the application core lib. During the ,UI startup process<<renderengine-GuiStart>>, loading of this Plugin is triggered from +main()+. Actually this causes spawning of the GTK event thread and execution of the GTK main loop.



<div title="HighLevelModel" modifier="Ichthyostega" modified="200906071811" created="200808152311" tags="spec design discuss img" changecount="33">
While the low-level model holds the data used for carrying out the actual media data processing (=rendering), the high-level model is what the user works upon when performing edit operations through the GUI (or script driven in >headless mode<). Its building blocks and combination rules determine largely what structures can be created within the ,Session>>.
On the whole, it is a collection of ,media objects<<renderengine-MObjects>> stuck together and arranged by ,placements<<renderengine-Placement>>.

Basically, the structure of the high-level model is is a very open and flexible one -- every valid connection of the underlying object types is allowed -- but the transformation into a low-level node network for rendering follows certain patterns and only takes into account any objects reachable while processing the session data in accordance to these patterns. Taking into account the parameters and the structure of these objects visited when building, the low-level render node network is configured in detail.

The fundamental metaphor or structural pattern is to create processing *pipes*, which are a linear chain of data processing modules, starting from an source port and providing an exit point. ,Pipes<<renderengine-Pipe>> are a _concept or pattern,_ they don't exist as objects. Each pipe has an input side and an output side and is in itself something like a Bus treating a single ,media stream<<renderengine-StreamType>> (but this stream may still have an internal structure, e.g. several channels related to a spatial audio system). Other processing entities like effects and transitions can be placed (attached) at the pipe, resulting them to be appended to form this chain. Optionally, there may be a *wiring plug*, requesting the exit point to be connected to another pipe. When omitted, the wiring will be figured out automatically.
Thus, when making an connection _to_ a pipe, output data will be sent to the _source port_ (input side) of the pipe, wheras when making a connection _from_ a pipe, data from it's exit point will be routed to the destination. Incidentally, the low-level model and the render engine employ _pull-based processing,_ but this is rather of no relevance for the high-level model.



image::draw/high-level1.png[]
Normally, pipes are limited to a _strictly linear chain_ of data processors ('effects') working on a single data stream type, and consequently there is a single *exit point* which may be wired to an destination. As an exception to this rule, you may insert wire tap nodes (probe points), which explicitly may send data to an arbitrary input port; they are never wired automatically. It is possible to create cyclic connections by such arbitrary wiring, which will be detected by the builder and flagged as an error.

While pipes have a rather rigid and limited structure, it is allowed to make several connections to and from any pipe -- even connections requiring an stream type conversion. It is not even necessary to specify _any_ output destination, because then the wiring will be figured out automatically by searching the context and finally using some general rule. Connecting multiple outputs to the input of another pipe automatically creates a *mixing step* (which optionally can be controlled by a fader). Several pipes may be joined together by a *transition*, which in the general case simultaneously treats N media streams. Of course, the most common case is to combine two streams into one output, thereby also mixing them. Most available transition plugins belong to this category, but, as said, the model isn't limited to this simple case, and moreover it is possible to attach several overlapping transitions covering the same time interval.

Individual Media Objects are attached, located or joined together by *Placements*. A ,Placement>> is a handle for a single MObject (implemented as a refcounting smart-ptr) and contains a list of placement specifications, called LocatingPin. Adding an placement to the session acts as if creating an _instance._ (it behaves like a clone in case of multiple placements of the same object). Besides absolute and relative placement, there is also the possibility of a placement to stick directly to another MObject's placement, e.g. for attaching an effect to a clip or to connect an automation data set to an effect. This _stick-to placement_ creates sort of a loose clustering of objects: it will derive the position from the placement it is attached to. Note that while the length and the in/out points are a _property of the MObject,_ it's actual location depends on how it is _placed_ and thus can be maintained quite dynamically. Note further that effects can have an length on their own, thus by using these attachement mechaics, the wiring and configuration within the high-level model can be quite time dependant.

image:../draw/high-level2.png[]



Actually a *clip* is handled as if it was comprised of local pipe(s). In the example shown here, a two-channel clip has three effects attached, plus a wiring plug. Each of those attachments is used only if applicable to the media stream type the respective pipe will process. As the clip has two channels (e.g. video and audio), it will have two *source ports* pulling from the underlying media. Thus, as showed in the drawing to the right, by chaining up any attached effect applicable to the respective stream type defined by the source port, effectively each channel (sub)clip gets its own specifically adapted processing pipe.

@@clear(right):display(block):@@
==== Example of an complete Session
image::draw/high-level3.png[]
The Session contains several independent ,EDL>>s plus an output bus section (*global Pipes*). Each EDL holds a collection of MObjects placed within a *tree of tracks*. 
Within Lumiera, tracks are a rather passive means for organizing media objects, but aren't involved into the data processing themselves. The possibility of nesting tracks allows for easy grouping. Like the other objects, tracks are connected together by placements: A track holds the list of placements of its child tracks. Each EDL holds a single placement pointing to the root track. 

As placements have the ability to cooperate and derive any missing placement specifications, this creates a hierarchical structure throughout the session, where parts on any level behave similar if applicable. For example, when a track is anchored to some external entity (label, sync point in sound, etc), all objects placed relatively to this track will adjust and follow automatically. This relation between the track tree and the individual objects is especially important for the wiring, which, if not defined locally within an MObject's placement, is derived by searching up this track tree and utilizing the wiring plug locating pins found there, if applicable. In the default configuration, the placement of an EDL's root track contains a wiring plug for video and another wiring plug for audio. This setup is sufficient for getting every object within this EDL wired up automatically to the correct global output pipe. Moreover, when adding another wiring plug to some sub track, we can intercept and reroute the connections of all objects creating output of this specific stream type within this track and on all child tracks.

Besides routing to a global pipe, wiring plugs can also connect to the source port of an *meta-clip*. In this example session, the outputs of EDL-2 as defined by locating pins in it's root track's placement, are directed to the source ports of a ,meta-cllip<<renderengine-VirtualClip>> placed within EDL-1. Thus, within EDL-1, the contents of EDL-2 appear like a pseudo-media, from which the (meta) clip has been taken. They can be adorned with effects and processed further completely similar to a real clip.

Finally, this example shows an *automation* data set controlling some parameter of an effect contained in one of the global pipes. From the effect's POV, the automation is simply a ParamProvider, i.e a function yielding a scalar value over time. The automation data set may be implemented as a bézier curve, or by a mathematical function (e.g. sine or fractal pseudo random) or by some captured and interpolated data values. Interestingly, in this example the automation data set has been placed relatively to the meta clip (albeit on another track), thus it will follow and adjust when the latter is moved.



<div title="ImplementationDetails" modifier="Ichthyostega" modified="200904252305" created="200708080322" tags="overview" changecount="33">
This wiki page is the entry point to detail notes covering some technical decisions, details and problems encountered in the course of the implementation of the Lumiera Renderengine, the Builder and the related parts.

* ,Packages, Interfaces and Namespaces<<renderengine-InterfaceNamespaces>>
* ,Memory Management Issues<<renderengine-MemoryManagement>>
* ,Creating and registering Assets<<renderengine-AssetCreation>>
* ,Creating new Objects<<renderengine-ObjectCreation>>
* ,Multichannel Media<<renderengine-MultichannelMedia>>
* ,Editing Operations<<renderengine-EditingOperations>>
* ,Handling of the current Session<<renderengine-CurrentSession>>
* ,collecting Ideas for Implementation Guidelines<<renderengine-ImplementationGuidelines>>
* ,using the Visitor pattern?<<renderengine-VisitorUse>> -- resulting in ,»Visiting-Tool« library implementation<<renderengine-VisitingToolImpl>>
* ,Handling of Tracks and render Pipes in the EDL<<renderengine-TrackPipeEDL>>. ,Handling of Tracks<<renderengine-TrackHandling>> and ,Pipes<<renderengine-PipeHandling>>
* ,getting default configured<<renderengine-DefaultsManagement>> Objects relying on ,rule-based Configuration Queries<<renderengine-ConfigRules>>
* ,integrating the Config Query system<<renderengine-ConfigQueryIntegration>>
* ,identifying the basic Builder operations<<renderengine-BasicBuildingOperations>> and ,planning the Implementation<<renderengine-PlanningNodeCreatorTool>>
* ,how to handle »attached placement«<<renderengine-AttachedPlacementProblem>>
* working out the ,basic building situations<<renderengine-BuilderPrimitives>> and ,mechanics of rendering<<renderengine-RenderMechanics>>
* how to classify and ,describe media stream types<<renderengine-StreamType>> and how to ,use them<<renderengine-StreamTypeUse>>
* the ,identification of frames and nodes<<renderengine-NodeFrameNumbering>>
* the relation of ,Project, Timelines and Sequences<<renderengine-TimelineSequences>>
* how to ,start the GUI<<renderengine-GuiStart>> and how to ,communicate<<renderengine-GuiCommunication>>
* build the first LayerSeparationInterfaces
* decide on SessionInterface and create ,Session datastructure layout<<renderengine-SessionDataMem>>



<div title="ImplementationGuidelines" modifier="Ichthyostega" modified="200906060443" created="200711210531" tags="discuss draft" changecount="12">
!Observations, Ideas, Proposals
*this page is a scrapbook for collecting ideas* -- please don't take anything noted here too literal. While writing code, I observe that I (ichthyo) follow certain informal guidelines, some of which I'd like to note down because they could evolve into general style guidelines for the proc layer code.
* avoid doing anything non-local during the startup phase or shutdown phase of the application. consequently, always prefer using an explicit lumiera::Singleton<T> over using an static instance directly, thus yielding better control when the ctor and dtor will be invoked.
* write error handling code only if the error situation can be actually _handled_ at this place. Otherwise, be prepared for exceptions just passing by and thus handle any resources by 'resource acquisition is initialisation' (RAII). Remember: error handling defeats decoupling and encapsulation
* (almost) never +delete+ an object directly, use +new+ only when some smart pointer is at hand.
* when user/client code is intended to create objects, make the ctor protected and provide a factory member called +create+ instead, returning a smart pointer
* similarly, when we need just one instance of a given service, make the ctor protected and provide a factory member called +instance+, to be implemented by the lumiera::,Singleton>> factory.
* whenever possible, prefer this (lazy initialised ,Singleton>>) approach and avoid static initialisation magic
* avoid asuming anything that can't be enforced by types, interfaces or signatures; this means: be prepared for open possibilities
* prefer +const+ and initialisation code over assignment and active changes (inspired by functional programming)



<div title="InstanceHandle" modifier="Ichthyostega" created="200902090320" changecount="2">
An RAII class, used to manage a ,facade interface between layers<<renderengine-LayerSeparationInterface>>.
The InstanceHandle is created by the service implementation and will automatically
* either register and open a *C Language Interface*, using Lumiera's InterfaceSystem
* or load and open a LumieraPlugin, also by using the InterfaceSystem
* and optionally also create and manage a facade proxy to allow transparent access for client code

-> see ,detailed description here<<renderengine-LayerSeparationInterfaces>>



<div title="InterfaceNamespaces" modifier="Ichthyostega" modified="200708120302" created="200708080338" tags="impl decision discuss" changecount="15">
Because we rely on strong decoupling and separation into self contained components, there is not much need for a common quasi-global namespace. Operations needing the cooperation of another subsystem will be delegated or even dispatched, consequently implementation code needs only the service acces points from 'direct cooperation partner' subsystems. Hierarchical scopes besides classes are needed only when multiple subsystems share a set of common abstractions. Interface and Implementation use separate namespaces.

=== common definitions
. surely there will be the need to use some macros (albeit code written in C++ can successfully avoid macros to some extent)
. there will be one global *Application State* representation (and some application services)
. we have some lib facilities, especially a common ,Time>> abstraction
For these we have one special (bilingual) header file +lumiera.h+, which places in its C++ part some declarations into 'namespace lumiera'. These declarations should be pulled into the specific namespaces or (still better) into the implementation _on demand_. There is no nesting (we deliberately don't want an symbol appearing _automatically_ in every part of the system).

=== subsystem interface and facade
These large scale interfaces reside in special namespaces +XXX_interface+ (where +XXX+ is the subsystem). The accompanning _definitions_ depend on the implementation namespace and are placed in the top-level source folder of the corresponding subsystem.

-> Example: ,Interfaces/Namespaces of the Session Subsystem(s)<<renderengine-InterfacesSession>>

=== contract checks and test code
From experiences with other middle scale projects, I prefer having the test code in a separate tree (because test code easily doubles the number of source files). But of course it should be placed into the same namespace as the code being checked, or (better?) into a nested namespace +test+. It is esp. desirable to have good coverage on the contracts of the subsystem interfaces and mayor components (while it is not always feasible or advisable to cover every implementation detail).

-> see also ,testsuite documentation in the main wiki<<renderengine-index.html.TestSuite>>



<div title="InterfacesSession" modifier="Ichthyostega" modified="200906071811" created="200708080639" tags="img" changecount="8">
* Subdir src/proc contains Interface within namespace proc_interface
* Subdir src/proc/mobject contains commonly used entities (namespace mobject)
** nested namespace controller
** nested namespace builder
** nested namespace session
* Subdir src/proc/engine (namespace engine) uses directly the (local) interface components StateProxy and ParamProvider; triggering of the render process is initiated by the controller and can be requested via the controller facade. Normally, the playback/render controller located in the backend will just use this interface and won't be aware of the build process at all.

image::Example: Interfaces/Namespaces of the Session-Subsystems../uml/fig130053.png[]



<div title="LayerSeparationInterface" modifier="Ichthyostega" modified="200904302314" created="200902080635" tags="def" changecount="2">
A major _business interface_ -- used by the layers for interfacing to each other; also to be invoked externally by scripts.
-> ,overfiew and technical details<<renderengine-LayerSeparationInterfaces>>



<div title="LayerSeparationInterfaces" modifier="Ichthyostega" modified="200902090419" created="200812050619" tags="overview spec img" changecount="22">
Lumiera uses a 3-layered architecture. Separation between layers is crucial. Any communication between the layers regarding the normal operation of the application should _at least be initiated_ through *Layer abstraction interfaces* (Facade Interfaces). This is a low-impact version of layering, because, aside from this triggering, direct cooperation of parts within the single Lumiera process is allowed, under the condition that is is implemented using additional abstractions (interfaces with implementation level granularity). We stick to the policy of _disallowing direct coupling of implementations located in different layers._

image:../Anatomy of a Layer Separation Interface../uml/fig132869.png[]
The goal is for the interface to remain fairly transparent for the client and to get an automatic lifecycle management. 
To implement such a structure, each layer separation interface actually is comprised of several parts:
* an C Language Interface (+CL Interface+) to be installed into the InterfaceSystem
* a *facade interface* defining the respective abstractions in terms of the client side impl. language (C or C++)
* a *service implementation* directly addressed by the implementation instantiated within the InterfaceSystem
* a *facade proxy* object on the client side, which usually is given inline alongside with the CL interface definition.

=== opening and closing
Handling the lifecycle can be tricky, because client- and service-side need to carry out the opening and closing operations in sync and observing a specific call order to ensure calls get blocked already on the client side unless the whole interface compound is really up and running. To add to this complexity, plugins and built-in interfaces are handled differently regarding the question who is in charge of the lifecycle: interfaces are installed on the service side, whereas the loading of plugins is triggered from client side by requesting the plugin from the loader.
Anyway, interfaces are resources which best should be managed automatically. At least within the C++ part of the application we can ensure this by using the InstanceHandle template. This way the handling of plugins and interfaces can be unified and the opening and closing of the facade proxy happens automatically.

The general idea is, that each facade interface actually provides access to a specific service; there will always be a single implementation object somewhere, which can be thought of as acting as _the_ service. This service-providing object will then contain the mentioned InstanceHandle; thus, its lifecycle becomes identical with the service lifecycle.
* when the service relies on a ,plugin<<renderengine-LumieraPlugin>>, this service providing object (containing the InstanceHandle) needs to sit at the service accessing side (as the plugin may not yet be loaded and someone has to pull it up).
* otherwise, when the service is just an interface to an already loaded facility, the service providing object (containing the InstanceHandle) will live on the service providing side, not the client side. Then the ctor of the service providing object needs to be able to access an interface instance definition ( -> InterfaceSystem); the only difference to the plugin case is to create the InstanceHandle with a different ctor, passing the interface and the implementing instance.

=== outline of the major interfaces
<<renderengine-!Layer<<renderengine-><<renderengine-!Interface <<renderengine-
<<renderengine-GUI<<renderengine-GuiFacade<<renderengine-UI lifecycle  -> GuiStart<<renderengine-
<<renderengine-<<renderengine-GuiNotificationFacade<<renderengine-status/error messages, asynchronous object status change notifications, trigger shutdown<<renderengine-
<<renderengine-<<renderengine-DisplayFacade<<renderengine-pushing frames to a display/viewer<<renderengine-
<<renderengine-Proc<<renderengine-SessionFacade<<renderengine-session lifecycle<<renderengine-
<<renderengine-<<renderengine-EditFacade<<renderengine-edit operations, object mutations<<renderengine-
<<renderengine-<<renderengine-PlayerDummy<<renderengine-player mockup, maybe move to backend?<<renderengine-
<<renderengine-_Lumiera's major interfaces_<<renderengine-c




<div title="LoadingMedia" modifier="Ichthyostega" modified="200806030203" created="200709220005" tags="design spec" changecount="4">
Opening and accessing media files on disk poses several problems, most of which belong to the domain of Lumiera's data backend. Here, we focus on the questions related to making media data available to the EDL and the render engine. Each media will be represented by an MediaAsset object, which indeed could be a compound object (in case of MultichannelMedia). Building this asset object thus includes getting informations from the real file on disk. For delegating this to the backend, we use the following query interface:
* +queryFile(char* name)+ requests accessing the file and yields some (opaque) handle when successful.
* +queryChannel(fHandle, int)+ will then be issued in sequence with ascending index numbers, until it returns +NULL+.
* the returned struct (pointer) will provide the following information:
** some identifier which can be used to create a name for the corresponding media (channel) asset
** some identifier characterizing the access method (codec) needed to get at the media data. This should be rather a high level description of the media stream type, e.g. +H264+
** some (opaque) handle usable for accessing this specific stream. When the render engine later on pulls data for this channel, it will pass this handle down to the backend.

[red]+to be defined in more detail later...+

-> see +MediaAccessFacade+ for a (preliminary) interface definitioin
-> see +MediaAccessMock+ for a mock/test implementaion



<div title="LocatingPin" modifier="Ichthyostega" created="200710181527" changecount="1">
Used to actually implement the various kinds of ,Placement>> of MObjects. LocatingPin is the root of a hierarchy of different kinds of placing, constraining and locating a Media Object. Basically, this is an instance of the *state pattern*: The user sees one Placement object with value semantics, but when the properties of the Placement are changed, actually a LocatingPin object (or rather a chain of LocatingPins) is changed within the Placement. Subclasses of LocatingPin implement different placing/constraining behaviour:
* +FixedLocation+ places a MObject to a fixed temporal position and track
* +RelativeLocation+ is used to atach the MObject to some other anchor MObject
* _additional constraints, placement objectives, range restrictions, pattern rules will follow..._


<div title="MObject" modifier="Ichthyostega" modified="200905130114" created="200706220312" tags="def classes" changecount="3">
All sorts of 'things' to be placed and manipulated by the user in the EDL. This interface abstracts the details and just supposes
* the media object has a duration
* it is allways _placed_ in some manner, i.e. it is allways accessed via a ,Placement>>
* [red]+and what else?+

-> ,overview of the MObject hierarchy<<renderengine-MObjects>>



<div title="MObjectRef" modifier="Ichthyostega" modified="200906020120" created="200904242107" tags="SessionLogic GuiIntegration design draft discuss" changecount="21">
*The Problem of referring to an ,MObject>>* stems from the object _as a concept_ encompassing a wider scope then just the current implementation instance. If the object was just a runtime entity in memory, we could use a simple (language) reference or pointer. Actually, this isn't sufficient, as the object reference will pass LayerSeparationInterfaces, will be handed over to code not written in the same implementation language, will be included in an *UNDO* record for the UndoManager, and thus will need to be serialized and stored permanently within the SessionStorage.
Moreover ,MObject instances<<renderengine-MObject>> have a 2-level structure: the core object holds just the properties in a strict sense, i.e. the properties which the object _owns._ Any properties due to putting the object into a specific context, i.e. all relation properties are represented as ,Placement>> of the object. Thus, when viewed from the client side, a reference to a specific MObject _instance,_ actually denotes a _specific_ Placement of this object into the Session.

=== Requirements
* just the reference allone is sufficient to access the placement _and_ the core object.
* the reference needs to be valid even after internal restructuring in the object store.
* there must be a way to pass these references through serialisation/deserialisation
* we need a plain-C representation of the reference, which ideally should be incorporated into the complete implementation
* references should either be integrated into memory management, or at least it should be possible to detect dangling references
* it should be possible to make handling of the references completely transparent, if the implementation language supports doing so.

=== Two models
For the implementation of the object references, as linked to the memory management in general, there seem to be the following approaches
* the handle is a simple table index. Consequently we'll need a garbage collection to deal with deleted objects. This model has several flavours
** using an generation ID to tag the index handle, keep an translation table on each GC, mapping old indices to new ones and raise an error when encountering an outdated handle
** use a specific datastructure allowing the handles to remain valid in case of cleanup/reorganisation. (Hashtable or similar)
** keep track of all index handles and rewrite them on GC
* handles are refcounting smart pointers. This solution is C++ specific, albeit more elegant and minimalistic. As all the main interfaces should be accessible via C bindings, we'd need to use a replacement mechanism on the LayerSeparationInterfaces, which could be mapped to or handled by an C++ smart-ptr. (We used a similar approach for the PlayerDummy design study)

Obviously, the second approach has quite some appeal -- but, in order to use it, we'd have to mitigate its drawbacks: it bears the danger of creating a second separate code path for C language based clients, presumably receiving lesser care, maintenance and stress testing. The mentioned solution worked out earlier this year for the player mockup (1/2009) tries at least partially to integrate the C functionality, as the actual implementation is derived from the C struct used as handle, thus allowing to use the same pointers for both kinds of interface, and in turn by doing the de-allocation by a call through the C dtor function, which is installed as deleter function with the boost::smart_ptr used as base class of the handle. Conceptually, the interface on this handle is related to the actual implementation refered to by the handle (handle == smart_ptr) as if the latter was a subclass of the former. But on the level of the implementation, there is no inheritance relation between the handle and the referent, and especially this allows to define the handle's interface in terms of abstract interface types usable on the client side, while the referent's interface operates on the types of the service implementation. Thus, the drawback of using a C language interface is turned into the advantage of completely separating implementation and client.

=== Implementation concept
Presumably, none of the both models is usable as-is; rather we try to reconstruct the viable properties of both, starting out with the more elegant second model. Thus, basically the *reference is a smart-ptr* referring to the core object. Additionally, it incorporates a *systematic ID denoting the location of the placement*. This ID without the smart-ptr part is used for the C-implementation, making the full handle implementation a shortcut for an access sequence, which first querries the placement from the Session, followed by dereferencing the placement to get at the core object. Thus, the implementation builds upon another abstraction, the  -> PlacementRef, which in turn assumes for an index within the implementation of the ,session datastructure<<renderengine-SessionDataMem>> to track and retrieve the actual Placement.


=== using MObject references
MObject references have a distinct lifecycle: usually, they are created _empty_ (invalid ref, inactive state), followed by activating them by attachment to an existing placement within the session. Later on, the reference can be closed (detached, deactivated). Activation can be done either directly by a +Placement<MO>&amp;+, or indirectly by any +Placement::ID+ tag or even plain LUID denoting a placement known to the PlacementIndex embedded within the ,Session>>. Activation can fail, because the validity of the reference is checked. In this respect, they behave exactly like PlacementRef, and indeed are implemented on top of the latter. From this point onward, the referred MObject is kept alive by the reference -- but note, this doesn't extend to the placement, which still may be modified or removed within the session without further notice. [red]+TODO investigate synchronisation guarantees or a locking mechanism+ Thus, client code should never store direct references to the placement.

MObject references have value semantics, i.e. they don't have an identity and can be copied feely. Dereferencing yields a direct (language) ref to the MObject, while the placement can be accessed by a separate function +getPlacement()+. Moreover, the MObjectRef instance provides a direct API to some of the most common query functions you could imagine to call on both the object and the placement (i.e. to find out about the start time, length, ....)

[red]+WIP+



<div title="MObjects" modifier="Ichthyostega" modified="200906071811" created="200706190636" tags="overview img" changecount="17">
The MObjects Subsystem contains everything related to the ,EDL>> and the various Media Objects placed within. It is complemented by the Asset Management (see  -> ,Asset>>). Examples for ,MObjects <<renderengine-MObject>>( -> def) being:
* audio/video clips
* ,effects and plugins<<renderengine-EffectHandling>>
* special facilities like mask and projector
* ,Automation>> sets
* labels and other (maybe functional) markup

This Design strives to achieve a StrongSeparation between the low-level Structures used to carry out the actual rendering and the high level Entities living in the EDL and being manipulated by the user. In this high level view, the Objects are grouped and located by ,Placements<<renderengine-Placement>>, providing a flexible and open way to express different groupings, locations and ordering constraints between the Media Objects.
-> EditingOperations
-> PlacementHandling
-> SessionOverview


image::Classess related to the EDL../uml/fig128133.png[]


<div title="MainMenu" modifier="Ichthyostega" modified="200802031758" created="200706172305" changecount="10">
*,Lumiera<<renderengine-index.html>>*
,Proc-Layer<<renderengine-ProcLayer and Engine>>
,MObjects>>
,Implementation<<renderengine-ImplementationDetails>>
,Admin>>
<<renderengine-fullscreen>>


<div title="ManagementAssetRelation" modifier="Ichthyostega" modified="200711220525" created="200708100337" tags="impl decision" changecount="12">
Problem is: when removing an Asset, all corresponding MObjects need to disappear. This means, besides the obvious Ref-Link (MObject referring to an asset) we need backlinks or a sort of registry. And still worse: we need to remove the affected MObject from the object network in the EDL and rebuild the Fixture...

-> for a general design discussion see ,Relation of Clip and Asset<<renderengine-RelationClipAsset>>

_Currently_ Ichthyo considers the following approach:
* all references between MObjects and Assets are implemented as 'refcounting' boost::shared_ptr
* the opposite direction is also a 'strong reference', effectively keeping the clip-MO alive even if it is no longer in use in the session (note this is a cyclic dependency that needs to be actively broken on deletion). This design decision is based on  logical considerations ( -> see 'deletions, Model-2' ,here<<renderengine-RelationClipAsset>>). This back-link is implemented by a Placement which is stored internally within the asset::Clip, it is needed for clean deletion of Assets, for GUI search functions and for adding the Clip to the EDL (again after been removed, or multiple times as if cloned).
* MObjects and Assets implement an +unlink()+ function releasing any internal links causing circular dependencies. It is always implemented such as to drop _optional_ associations while retaining those associations mandatory for fulfilling the objects contract.
* Instead of a delete, we call this unlink() function and let the shared_ptr handle the actual deletion. Thus, even if the object is already unlinked, it is still valid and usable as long as some other entity holds a smart-ptr. An ongoing render process for example can still use a clip asset and the corresponding media asset linked as parent, but this media asset's link to the dependant clip has already been cleared (and the media is no longer registered with the AssetManager of course).
* so the back-link from dependant asset up to the parent asset is mandatory for the child asset to be functional, but preferably it should be +const+ (only used for information retrieval)
* the whole hierarchy has to be crafted accordingly, but this isn't much of a limitation
* we don't use a registry, rather we model the real dependencies by individual dependency links. So a MediaAsset gets links to all Clips created from this Asset and by traversing this tree, we can handle the deletion
* after the deletion, the Fixture needs to be rebuilt.
* but any render processes still can have pointers to the Asset to be removed, and the shared_ptr will ensure, that the referred objects stay alive as long as needed.

[red]+let's see if this approach works...+



<div title="ManagementRenderNodes" modifier="Ichthyostega" modified="200810160131" created="200805280200" tags="impl decision" changecount="5">
Contrary to the  ->,Assets and MObjects<<renderengine-ManagementAssetRelation>>, the usage pattern for ,render nodes<<renderengine-ProcNode>> is quite simple: All nodes are created together every time a new segment of the network is being build and will be used together until this segment is re-built, at which point they can be thrown away altogether. While it would be easy to handle the nodes automatically by smart-ptr (the creation is accessible only by use of the +NodeFactory+ anyways), it _seems advisable to care for a bulk allocation/deallocation here._ The reason being not so much the amount of memory (which is expected to be moderate), but the fact the build process can be triggered repeatedly several times a second when tweaking the EDL, which could lead to fragmentation and memory pressure.

*10/2008*: the allocation mechanism can surely be improved later, but for now I am going for a simple implementation based on heap allocated objects owned by a vector or smart-ptrs. For each segment of the render nodes network, we have several families of objects, each of with will be maintained by a separate low-level memory manager (as said, for now implemented as vector of smart-ptrs). Together, they form an AllocationCluster; all objects contained in such a cluster will be destroyed together.



<div title="MediaAsset" modifier="Ichthyostega" modified="200906071810" created="200709021530" tags="def classes img" changecount="8">
The Interface asset::Media is a _key abstraction_ It ties together several concepts and enables to deal with them on the interfaces in a uniform manner. Besides, as every Asset kind it belongs rather to the bookkeeping view: an asset::Media holds the specific properties and parametrisation of the media source it stands for. Regarding the 'inward interface' -- as used from within the ,EDL>> or the ,Render Nodes<<renderengine-ProcNode>>, it is irrelevant if any given asset::Media object stands for a complete media source, just a clip taken from this source or if a placeholder version of the real media source is used instead.
image::Asset Classess../uml/fig130437.png[]

-> see also LoadingMedia



<div title="MediaImplLib" modifier="Ichthyostega" modified="200809251942" created="200809220304" tags="def spec" changecount="5">
The Proc-Layer is designed such as to avoid unnecessary assumptions regarding the properties of the media data and streams. Thus, for anything which is not completely generic, we rely on an abstract ,type description<<renderengine-StreamTypeDescriptor>>, which provides a *Facade* to an actual library implementation. This way, the fundamental operations can be invoked, like allocating a buffer to hold media data.

In the context of Lumiera and especially in the Proc-Layer, 'media implementation library' means
* a subsystem which allows to work with media data of a specific kind
* such as to provide the minimal set of operations
** allocating a frame buffer
** describing the type of the data within such a buffer
* and this subsystem or external library has been integrated to be used through Lumiera by writing adaptation code for accessing these basic operations through the ,implementation facade interface<<renderengine-StreamTypeImplFacade>>
* such a link to an type implementation is registered and maintained by the ,stream type manager<<renderengine-STypeManager>>

=== Problem of the implementation data types
Because we deliberately won't make any asumptions about the implementation library (besides the ones imposed indirectly by the facade interface), we can't integrate the data types of the library first class into the type system. All we can do is to use marker types and rely on the builder to have checked the compatibility of the actual data beforehand. 
It would be possible to circumvent this problem by requiring all supported implementation libraries to be known at compile time, because then the actual media implementation type could be linked to a facade type by generic programming. Indeed, Lumiera follows this route with regards to the possible kinds of MObject or ,Asset>> -- but to the contraty, for the problem in question here, being able to include support for a new media data type just by adding a plugin by far outweights the benefits of compile-time checked implementation type selection. So, as a consequence of this design decision we _note the possibility of the media file type discovery code to be misconfigured_ and select the _wrong implementation library at runtime._ And thus the render engine needs to be prepared for the source reading node of any pipe to flounder completely, and protect the rest of the system accordingly



<div title="MemoryManagement" modifier="Ichthyostega" modified="200808110208" created="200708100225" tags="impl decision discuss" changecount="14">
Of course: Cinelerra currently leaks memory and crashes regularilly. For the newly written code, besides retaining the same level of performance, a main goal is to use methods and techniques known to support the writing of quality code. So, besides the MultithreadConsiderations, a solid strategy for managing the ownership of allocated memory blocks is necessary right from start.

=== Problems
. Memory management needs to work correct in a _fault tolerant environment_. That means that we need to be prepared to _handle on a non-local scale_ some sorts of error conditions (without aborting the application). To be more precise: some error condition arises locally, which leads to a local abort and just the disabling/failing of some subsystem without affecting the application as a whole. This can happen on a regular base (e.g. rendering fails) and thus is 'no excuse for leaking memory'
. Some (not all) parts of the core application are non-deterministic. That means, we can't tie the memory management to any assumptions on behalf of the execution path

=== C++ solution
First of all -- this doesn't concern _every_ allocation. It rather means there are certain _dangerous areas_ which need to be identified. Anyhow, instead of carrying inherent complexities of the problem into the solution, we should rather look for  common solution pattern(s) which help factoring out complexity.

For the case here in question this seems to be the *resource allocation is construction* pattern. Which boils down to basically never using bare pointers when concerned with ownership. Instead, ownership should be handled by smart-pointers.

==== usage scenarios
. 'existence is being used': Objects just live for being referred to in a object network. In this case, use refcounting smart-pointers for every ref. (note: problem with cyclic refs)
. 'entity bound ownership': Objects can be tied to some long living entity in the program, which holds the smart-pointer
* if the existence of these ref-holding entity can be _guaranteed_ (as if by contract), then the other users can build a object network with conventional pointers
* otherwise, when the ref-holding entity _can disappear_ in a regular program state, we need weak-refs and checking (because by our postulate the controlled resource needs to be destructed immediately, otherwise we would have the first case, existence == being used)

===== dangerous uses
* the render nodes  -> ,detail analysis<<renderengine-ManagementRenderNodes>> [red]+TODO+
* the MObjects in the EDL  -> ,detail analysis<<renderengine-ManagementMObjects>> [red]+TODO+
* Asset - MObject relationship.  -> ,detail analysis<<renderengine-ManagementAssetRelation>> [red]+TODO+

===== rather harmless
* Frames (buffers), because they belong to a given ,RenderProcess (=StateProxy)<<renderengine-StateProxy>> and are just passed in into the individual ,ProcNode>>s. This can be handled consistently with conventional methods.
* each StateProxy belongs to one top-level call to the ,Controller-Facade<<renderengine-Controller>>
* similar for the builder tools, which belong to a build process. Moreover, they are pooled and reused.
* the EDL and the defined ,Asset>>s belong together to one Session. If the Session is closed, this means a internal shutdown of the whole ProcLayer, i.e. closing of all GUI representations and terminating all render processes. If these calles are implemented as blocking operations, we can assert that as long as any GUI representation or any render process is running, there is a valid Session and EDL.

=== using Factories
And, last but not least, doing large scale allocations is the job of the backend. Exceptions being long-lived objects, like the Session or the EDL, which are created once and don't bear the danger of causing memory pressure. Besides, the ProcLayer code shouldn't issue 'new' and 'delete' when it comes in hand, rather it should use some centralized ,Factories>> for all allocation and freeing, so we can redirect these calls down to the backend, which may use pooling or special placement allocators or the like. The rationale is, for modern hardware/architectures, care has to be taken with heap allocations, esp. with many small objects and irregular usage patterns.



<div title="MultichannelMedia" modifier="Ichthyostega" modified="200710212326" created="200709200255" tags="design img" changecount="6">
Based on practical experiences, Ichthyo tends to consider Multichannel Media as the base case, while counting media files providing just one single media stream as exotic corner cases. This may seem counter intuitive at first sight; you should think of  it as an attempt to avoid right from start some of the common shortcomings found in many video editors, especially
* having to deal with keeping a 'link' between audio and video clips
* silly limitations on the supported audio setups (e.g. 'sound is mono, stereo or Dolby-5.1')
* unnecessary complexity when dealing with more realistic setups, esp. when working on dialogue scenes
* inability to edit stereoscopic (3D) video in a natural fashion

=== Compound Media
image:../Outline of the Build Process../uml/fig131333.png[]
Basically, each ,media asset<<renderengine-MediaAsset>> is considered to be a compound of several elementary media (tracks), possibly of various different media kinds. Adding support for placeholders (*proxy clips*) at some point in future will add still more complexity (because then there will be even dependencies between some of these elementary media). To handle, edit and render compound media, we need to impose some structural limitations. But anyhow, we try to configure as much as possible already at the 'asset level' and make the rest of the proc layer behave just according to the configuration given with each asset.

So, when creating a clip out of such a compound media asset, the clip has to be a compound of elementary clips mirroring the given media asset's structure. Besides, it should be possible to _detach_ and _attach_ elementary clips from a compound clip. On the other hand, the ,Fixture>> created from the current state of the ,EDL>> is explicit to a great extent. So, in the Fixture we deal only with elementary clips placed to absolute positions, and thus the builder will see only simple non-compound clips and translate them into the corresponding source reading nodes.

=== Handling
* from a Media asset, we can get a ,Processing Pattern (ProcPatt)<<renderengine-ProcPatt>> describing how to build a render pipeline for this media
* we can create a Clip (MObject) from each Media, which will be linked back to the media asset internally.
* moreover, creating a Clip will create and register a Clip asset as well, and this Clip asset will be tied to the original Clip and will show up in some special Category
* media can be compound and the created Clips will mirror this compound structure
* we distinguish elementay (non-compound) Clips from compound clips by concrete subtype. The builder can only handle elementary clips, because he needs to build a separate pipeline for every output channel. So the work of splitting common effect stacks for clips with several channels needs to be done when calculating the Fixture for the current EDL. The Builder expects to be able to build the render nodes corresponding to each entity found in the Fixture one by one.
* the Builder gets at the ProcPatt (descriptor) of the underlying media for each clip and uses this description as a template to build the render pipeline. That is, the ProcPatt specifies the codec asset and maybe some additional effect assets (deinterlace, scale) necessary for feeding media data corresponding to this clip/media into the render nodes network.



<div title="NodeConfiguration" modifier="Ichthyostega" modified="200909041807" created="200909041806" tags="spec Builder Rendering" changecount="2">
Various aspects of the individual ,render node<<renderengine-ProcNode>> are subject to configuration and may influence the output quality or the behaviour of the render process.
* the selection _what_ actual implementation (plugin) to used for a formally defined >,Effect<<renderengine-EffectHandling>><
* the intermediary/common StreamType to use within a ,Pipe>>
* the render technology (CPU, hardware accelerated [red]+ -> Future+)
* the ScheduleStrategy (possibly subdividing the calculation of a single frame)
* if this node becomes a possible CachePoint or DataMigrationPoint in RenderFarm mode
* details of picking a suitable ,operation mode<<renderengine-RenderImplDetails>> of the node (e.g. utilitsing 'in-place' calculation)



<div title="NodeCreatorTool" modifier="Ichthyostega" modified="200909041744" created="200712100626" tags="def" changecount="3">
NodeCreatorTool is a ,visiting tool<<renderengine-VisitorUse>> used as second step in the ,Builder>>. Starting out from a ,Fixture>>, the builder first ,divides the Timeline into segments<<renderengine-SegmentationTool>> and then processes each segment with the NodeCreatorTool to build a render nodes network (Render Engine) for this part of the timeline. While visiting individual Objects and Placements, the NodeCreaterTool creates and wires the necessary ,nodes<<renderengine-ProcNode>>



<div title="NodeFrameNumbering" modifier="Ichthyostega" modified="200810160129" created="200810140254" tags="spec draft" changecount="3">
!Problem of Frame identification

=== Problem of Node numbering
In the most general case the render network may be just a DAG (not just a tree). Especially, multiple exit points may lead down to the same node, and following each of this possible paths the node may be at a different depth on each. This rules out a simple counter starting from the exit level, leaving us with the possibility of either employing a rather convoluted addressing scheme or using arbitrary ID numbers.[red]+...which is what we do for now+



<div title="NodeOperationProtocol" modifier="Ichthyostega" modified="200909031217" created="200806010251" tags="Rendering dynamic" changecount="16">
The ,nodes<<renderengine-ProcNode>> are wired to form a +Directed Acyclic Graph+; each node knows its predecessor(s), but not its successor(s).  The RenderProcess is organized according to the *pull principle*, thus we find an operation +pull()+ at the core of this process. There is no such thing as an 'engine object' calling nodes iteratively or table driven, rather, the nodes themselves issue recursive calls to their predecessor(s). For this to work, we need the nodes to adhere to a specific protocol:
. Node is pulled, with a StateProxy object as parameter (encapsulating the access to the frames or buffers)
. Node may now access current parameter values, using the state accessible via the StateProxy
. using it's _input-output and wiring descriptor,_ the Node creates a StateAdapter wrapping the StateProxy for allocating buffers and accessing the required input
. StateAdapter might first try to get the output frames from the Cache in the Backend. In case of failure, a +process()+ call is prepared by generating +pull()+ call(s) for the input
. as late as possible, typically on return, these recursive pull-calls have allocated a buffer containing the input data.
. when input is ready prior to the +process()+ call, output buffers will be allocated, either from the cache, or (if not caching) from the 'parent' StateAdapter up the callstack.
. after all buffers are available, the StateAdapter issues the +process()+ call back to the originating node, which now may dereference the frame pointers and do its calculations
. finally, when the +pull()+ call returns, 'parent' state originating the pull holds onto the buffers containing the calculated output result.
some points to note:
* the WiringDescriptor is +const+ and precalculated while building (remember another thread may call in parallel)
* when a node is 'inplace-capable', input and output buffer may actually point to the same location
* but there is no guarantee for this to happen, because the cache may be involved (and we can't overwrite the contents of a cache frame)
* generally, a node may have N inputs and M output frames, which are expected to be processed in a single call

-> the ,'mechanics' of the render process<<renderengine-RenderMechanics>>
-> more fine grained ,implementation details<<renderengine-RenderImplDetails>>



<div title="ObjectCreation" modifier="Ichthyostega" modified="200806030153" created="200709030139" tags="impl design" changecount="18">
We have to consider carefully how to handle the Creation of new class instances. Because, when done naively, it can defeat all efforts of separating subsystems, or -- the other extreme -- lead to a _switch-on-typeID_  programming style. We strive at a solution somewhere in the middle by utilizing +Abstract Factories+ on Interface or key abstraction classes, but providing specialized overloads for the different use cases. So in each use case we have to decide if we want to create a instance of some general concept (Interface), or if we have a direct collaboration and thus need the Factory to provide a more specific sub-Interface or even a concrete type.

=== Object creation use cases
!!,Assets<<renderengine-Asset>>
<<renderengine-!Action<<renderengine-><<renderengine-!creates <<renderengine-
<<renderengine-loading a media file<<renderengine-asset::Media, asset::Codec<<renderengine- <<renderengine-
<<renderengine-viewing media<<renderengine-asset::Clip, session::Clip and Placement (on hold)<<renderengine- for the whole Media, if not already existent<<renderengine-
<<renderengine-mark selection as clip<<renderengine-asset::Clip, session::Clip, Placement with unspec. LocatingPin<<renderengine- doesn't add to EDL<<renderengine-
<<renderengine-loading Plugin<<renderengine-asset::Effect<<renderengine- usually at program startup<<renderengine-
<<renderengine-create Session<<renderengine-asset::Track, asset::Pipe<<renderengine- <<renderengine-
-> ,creating and registering Assets<<renderengine-AssetCreation>>
-> ,loading media<<renderengine-LoadingMedia>>

!!,MObjects<<renderengine-MObject>>
<<renderengine-add media to EDL<<renderengine-asset::Clip, session::Clip, Placement with unspecified LocatingPin<<renderengine- creating whole-media clip on-the-fly <<renderengine-
<<renderengine-add Clip to EDL<<renderengine-copy of Placement<<renderengine- creates intependent Placement of existing Clip-MO<<renderengine-
<<renderengine-attach Effect<<renderengine-session::Effect, Placement with RelativeLocation<<renderengine- <<renderengine-
<<renderengine-start using Automation<<renderengine-session::Auto, asset::Dataset, RelativeLocation Placement<<renderengine- <<renderengine-

=== Invariants
when creating Objects, certain invariants have to be maintained. Because creating an Object can be considered an atomic operation and must not leave any related objects in an inconsistent state. Each of our interfaces implies some invariants:
* every Placement has a Subject it places
* MObjects are always created to be placed in some way or the other
* ,Assets<<renderengine-Asset>> manage a dependency graph. Creating a derived Object (e.g. a Clip from a Media) implies a new dependency. ( -> ,memory management<<renderengine-ManagementAssetRelation>> relies on this)



<div title="OpenGL" modifier="Ichthyostega" modified="200706220359" created="200706220345" tags="def discuss" changecount="3">
Cinelerra2 introduced OpenGL support for rendering previews. I must admit, I am very unhappy with this, because
* it just supports some hardware
* it makes building difficult
* it can't handle all color models Cinelerra is capable of
* it introduces a separate codepath including some complicated copying of video data into the textures (and back?)
* it can't be used for rendering

So my judgement would be: in contrary to a realtime/gaming application, for quality video editing it is not worth the effort implementing OpenGL support in all details and with all its complexity. I would accept OpenGL as an option, if it could be pushed down into a Library, so it can be handled and maintained transparently and doesnt bind our limited developer manpower.

But because I know the opinions on this topc are varying (users tend to be delighted if they hear +OpenGL+, because it carries notion of _fast_ and _power_ todays) -- I try to integrate OpenGL as apossibility into this design of the Render Engine. Obviousely, I have the hard requirement that it _must not jeopardize the code structure._

My proposed aproach is to treat OpenGL as a separate video raw data type, requiring separete and specialized ,Processing Nodes<<renderengine-ProcNode>> for all calculations. Thus the Builder could connect OpenGL nodes if it is possible to cover the whole render path for preview and fall back to the normal ProcNodes for all relevant renders



<div title="OperationPoint" modifier="Ichthyostega" modified="200909041742" created="200805270334" tags="def impl Builder" changecount="8">
A low-level abstraction within the ,Builder>> -- it serves to encapsulate the details of making multi-channel connections between the render nodes: In some cases, a node can handle N channels internally, while in other cases we need to replicate the node N times and wire each channel individually. As it stands, the OperationPoint marks the *borderline between high-level and low-level model*: it is invoked in terms of MObjects and other entities of the high-level view, but internally it manages to create ProcNode and similar entities of the low-level model.

The operation point is provided by the current BuilderMould and used by the ,processing pattern<<renderengine-ProcPatt>> executing within this mould and conducting the current build step. The operation point's interface allows _to abstract_ these details, as well as to _gain additional control_ if necessary (e.g. addressing only one of the channels). The most prominent build instruction used within the processing patterns (which is the instruction ++attach++) relies on the aforementioned _approach of abstracted handling,_ letting the operation point determine automatically how to make the connection.

This is possible because the operation point has been provided (by the mould) with informations about the media stream type to be wired, which, together with information accessible at the the ,render node interface<<renderengine-ProcNode>> and from the ,referred processing assets<<renderengine-ProcAsset>>, with the help of the ,connection manager<<renderengine-ConManager>> allows to figure out what's possible and how to do the desired connections. Additionally, in the course of deciding about possible connections, the PathManager is consulted to guide strategic decisions regarding the ,render node configuration<<renderengine-NodeConfiguration>>, possible type conversions and the rendering technology to employ.



<div title="Overview" modifier="Ichthyostega" modified="200906071810" created="200706190300" tags="overview img" changecount="13">
The Lumiera Processing Layer is comprised of various subsystems and can be separated into a low-level and a high-level part. At the low-level end is the ,Render Engine<<renderengine-OverviewRenderEngine>> which basically is a network of render nodes cooperating closely with the Backend Layer in order to carry out the actual playback and media transforming calculations. Whereas on the high-level side we find several different ,Media Objects<<renderengine-MObjects>> that can be placed into the ,EDL>>, edited and manipulated. This is complemented by the ,Asset Management<<renderengine-Asset>>, which is the 'bookkeeping view' of all the different 'things' within each ,Session<<renderengine-SessionOverview>>.

There is rather strong separation between these two levels, and -- <br/>correspondingly you'll encounter the data held within the Processing Layer organized in two different views, the *high-level-model* and the *low-level-model*
* from users (and GUI) perspective, you'll see a ,Session<<renderengine-SessionOverview>> with a timeline-like structure, where various ,Media Objects<<renderengine-MObjects>> are arranged and ,placed<<renderengine-Placement>>. By looking closer, you'll find that there are data connections and all processing is organized around processing chains or ,pipes<<renderengine-Pipe>>, which can be either global (in the Session) or local (in real or ,virtual<<renderengine-VirtualClip>> clips)
* when dealing with the actual calculations in the Engine ( -> see OverviewRenderEngine), you won't find any Tracks, Media Objects or Pipes -- rather you'll find a network of interconnected ,render nodes<<renderengine-ProcNode>> forming the low level model. Each structurally constant segment of the timeline corresponds to a separate node network providing an ExitNode corresponding to each of the global pipes; pulling frames from them means running the engine.
* it is the job of the ,Builder>> create and wire up this render nodes network when provided with a given hig-level-model. So, actually the builder (together with the so called ,Fixture>>) form an isolation layer in the middle, separating the _editing part_ from the _processing part._
image::Block Diagram../uml/fig128005.png[]



<div title="OverviewRenderEngine" modifier="Ichthyostega" modified="200906071810" created="200706190647" tags="Rendering overview img" changecount="20">
Render Engine, ,Builder>> and ,Controller>> are closely related Subsystems. Actually, the ,Builder>> _creates_ a newly configured Render Engine _for every_ RenderProcess. Before doing so, it queries from the Session (or, to be more precise, from the ,Fixture>> within the current Session) all necessary Media Object Placement information. The ,Builder>> then derives from this information the actual assembly of ,Processing Nodes<<renderengine-ProcNode>> comprising the Render Engine. Thus:
 * the source of the build process is a sequence of absolute (explicit) ,Placements<<renderengine-Placement>> called the ,Playlist>>
 * the ,build process<<renderengine-BuildProcess>> is driven, configured and controlled by the ,Controller>> subsystem component. It encompasses the actual playback configuration and State of the System.
 * the resulting Render Engine is a list of ,Processors>>, each configured to calculate a segment of the timeline with uniform properties. Each of these Processors in turn is a graph of interconnected ProcNode.s.

see also: RenderEntities, ,two Examples (Object diagrams)<<renderengine-Examples>> 

image::Overview: Components of the Renderengine../uml/fig128261.png[]



<div title="ParamProvider" modifier="Ichthyostega" modified="200810170040" created="200706220517" tags="def automation" changecount="12">
A ParamProvider is the counterpart for (one or many) ,Parameter>> instances. It implements the value access function made available by the Parameter object to its clients.

To give a concrete example: 
* a Fade Plugin needs the actual fade value for Frame t=xxx
* the Plugin has a Parameter Object (from which we could query the information of this parameter being a continuous float function)
* this Parameter Object provides a getValue() function, which is internally linked (i.e. by configuration) to a _Parameter Provider_
* the actual object implementing the ParamProvider Interface could be a Automation MObject located somewhere in the EDL and would do bezier interpolation on a given keyframe set.
* Param providers are created on demand; while building the Render Engine configuration actually at work, the Builder would have to setup a link between the Plugin Parameter Object and the ParamProvider; he can do so, because he sees the link between the Automation MObject and the corresponding Effect MObject

==== ParamProvider ownership and lifecycle
Actually, ParamProvider is just an interface which is implemented either by a constant or an ,Automation>> function. In both cases, access is via direct reference, while the link to the ParamProvider is maintained by a smart-ptr, which -- in the case of automation may share ownership with the ,Placement>> of the automation data set.

-> see the class diagram for ,Automation>>
-> see EffectHandling



<div title="Parameter" modifier="Ichthyostega" modified="200805300124" created="200706220505" tags="def automation" changecount="1">
Parameters are all possibly variable control values used within the Render Engine. Contrast this with configuration values, which are considered to be fixed and need an internal reset of the application (or session) state to take effect.

A *Parameter Object* provides a descriptor of the kind of parameter, together with a function used to pull the _actual value_ of this parameter. Here, _actual_ has a two-fold meaning:
* if called without a time specification, it is either a global (but variable) system or session parameter or a default value for automated Parameters. (the intention is to treat this cases uniformly)
* if called with a time specification, it is the query for an -- probably interpolated -- ,Automation>> value at this absolute time. The corresponding ParamProvider should fall back transparently to a default or session value if no time varying data is available

[red]+TODO: define how Automation works+



<div title="PathManager" modifier="Ichthyostega" created="200909041748" tags="def Builder" changecount="1">
Facility guiding decisions regarding the strategy to employ for rendering or wiring up connections. The PathManager is querried through the OperationPoint, when executing the connection steps within the Build process.



<div title="Pipe" modifier="Ichthyostega" modified="200804110301" created="200801062110" tags="def decision" changecount="6">
Pipes play an central role within the Proc Layer, because for everything placed and handled within the EDL, the final goal is to get it transformed into data which can be retrieved at some pipe's exit port. Pipes are special facilities, rather like inventory, separate and not treated like all the other objects.
We don't distinguish between 'input' and 'output' ports -- rather, pipes are thought to be *hooks for making connections to*. By following this line of thought, each pipe has an input side and an output side and is in itself something like a *Bus* or *processing chain*. Other processing entities like effects and transitions can be placed (attached) at the pipe, resulting them to be appended to form this chain. Likewise, we can place ,wiring requests<<renderengine-WiringRequest>> to the pipe, meaning we want it connected so to send it's output to another destination pipe. The ,Builder>> may generate further wiring requests to fulfil the placement of other entities.
Thus _Pipes are the basic building blocks_ of the whole render network. We distinguish *global available* Pipes, which are like the sum groups of a mixing console, and the *lokal pipe* or ,source ports<<renderengine-ClipSourcePort>> of the individual clips, which exist only within the duration of the corresponding clip. The design _limits the possible kinds of pipes _ to these two types -- thus we can build local processing chains at clips and global processing chains at the global pipes of the session and that's all we can do. (because of the flexibility which comes with the concept of ,placements<<renderengine-Placement>>, this is no real limitation)

The GUI can connect the viewer(s) to some pipe (and moreover can use ,probe points<<renderengine-ProbePoint>> placed like effects and connected to some pipe), and likewise, when starting a *render*, we get the opportunity to specify the pipes to pull the data from. Pulling data from some pipe is the (only) way to activate the render nodes network reachable from this pipe.

-> ,Handling of Tracks<<renderengine-TrackHandling>>
-> ,Handling of Pipes<<renderengine-PipeHandling>>



<div title="PipeHandling" modifier="Ichthyostega" modified="200805250303" created="200801101352" tags="spec" changecount="11">
!Identification
Pipes are distinct objects and can be identified by their asset IDs. Besides, as for all ,structural assets<<renderengine-StructAsset>> there are extended query capabilities, including a symbolic pipe-id and a media (stream) type id. Any pipe can accept and deliver exactly one media stream kind (which may be inherently structured though, e.g. spatial sound systems or stereoscopic video)

=== creating pipes
Pipe assets are created automatically by being used and referred. The ,Session>> holds a collection of global pipes ([red]+todo: implementation missing as of 5/08+), and further pipes can be created by using a new pipe reference in some placement. Moreover, every clip has an (implicit) ,source port<<renderengine-ClipSourcePort>>, which will appear as pipe asset when first used (referred) while ,building<<renderengine-BuildProcess>>. Note that creating a new pipe implies using a ,processing pattern<<renderengine-ProcPatt>>, which will be queried from the ,Defaults Manager<<renderengine-DefaultsManagement>> (resulting in the use of some preconfigured pattern or maybe the creation of a new ProcPatt object if necessary)

=== removal
Deleting a Pipe is an advanced operation, because it includes finding and 'detaching' all references, otherwise the pipe will leap back into existence immediately. Thus, global pipe entries in the Session and pipe references in ,locating pins<<renderengine-LocatingPin>> within any placement have to be removed, while clips using a given source port will be disabled. [red]+todo: implementation deferred+

=== using Pipes
there is not much you can do directly with a pipe asset. It is an point of reference, after all. Any connection to some pipe is only temporarily done by a placement in some part of the timeline, so it isn't stored with the pipe. You can edit the (user visible) description an you can globally disable a pipe asset. The pipe's ID and media stream type of course are fixed, because any connection and referral (via the asset ID) is based on them. Later on, we should provide a +rewire(oldPipe, newPipe)+ to search any ref to the +oldPipe+ and try to rewrite it to use the +newPipe+, possibly with a new media stream type.
Pipes are integrated with the ,management of defaults<<renderengine-DefaultsManagement>>. For example, any pipe uses implicitly some ,processing pattern<<renderengine-ProcPatt>> -- it may default to the empty pattern. This feature enables to apply some standard wiring to the pipes (e.g a fader for audio, similar to the classic mixing consoles). This _is _ a global property of the pipe, but -- contrary to the stream type -- this pattern may be switched



<div title="Placement" modifier="Ichthyostega" modified="200905090045" created="200706220306" tags="def" changecount="9">
A Placement represents a _relation:_ it is always linked to a _Subject_ (this being a ,Media Object<<renderengine-MObject>>) and has the meaning to _place_ this Subject in some manner, either relatively to other Media Objects, or by some Constraint or simply absolute at (time, output). The latter case is especially important for the build process and thus represented by a special ,Sub-Interface ExplicitPlacement<<renderengine-ExplicitPlacement>>. Besides this simple cases, Placements can also express more specific kinds of 'locating' an object, like placing a sound source at a pan position or placing a video clip at a given layer (above or below another video clip)

So basically placements represent a query interface: you can allways ask the placement to find out about the position of the related object in terms of (time, output), and -- depending on the specific object and situation -- also about these additional ,placement derived dimensions<<renderengine-PlacementDerivedDimension>> like sound pan or layer order or similar things which also fit into the general concept of 'placing' an object.

The fact of being placed in the ,Session<<renderengine-SessionOverview>>/,EDL>>is constitutive for all sorts of ,MObject>>s, without Placement they make no sense. Thus -- technically -- Placements act as *smart pointers*. Of course, there are several kinds of Placements and they are templated on the type of MObject they are refering to. Placements can be _aggregated_ to increasingly constrain the resulting 'location' of the refered MObject. See  -> ,handling of Placements<<renderengine-PlacementHandling>> for more details

=== Placements as instance
Effectively, the placement of a given MObject into the Session acts as setting up an concrete instance of this object. This way, placements exhibit a dual nature. When viewed on themselves, like any reference or smart-pointer they behave like values. But, by adding a placement to the session, we again create a unique distinguishable entity with reference semantics: there could be multiple placements of the same object but with varying placement properties. Such a placement-bound-into-the-session is denoted by an generic placement-ID or (as we call it)  -> PlacementRef; behind the scenes there is a PlacementIndex keeping track of those 'instances' -- allowing us to hand out the PlacementRef (which is just an opaque id) to client code outside the Proc-Layer and generally use it as an shorthand, behaving as if it was an MObject instance



<div title="PlacementDerivedDimension" modifier="Ichthyostega" modified="200805260223" created="200805260219" tags="def spec" changecount="3">
For any ,media object<<renderengine-MObject>> within the session, we can allways at least query the time (reference/start) point and the output destination from the ,Placement>>, by which the object is being handled. But the simple act of placing an object in some way, can -- depending on the context -- create additional degrees of freedom. To list some important examples:
* placing a video clip overlapping with other clips on other tracks creates the possibility for the clip to be above another clip or to be combined in various other ways with the other clips at the same time position
* placing a mono sound object plugged to a stereophoic output destination creates the freedom to define the pan position
The Placement interface allows to query for these additional _parameter values derived from the fact of being placed._

=== defining additional dimensions
probably a LocatingPin but... [red]+TODO any details are yet unknown as of 5/08+

=== querying additional dimensions
basically you resolve the Placement, yielding an ExplicitPlacement... [red]+TODO but any details of how additional dimensions are resolved is still undefined as of 5/08+



<div title="PlacementHandling" modifier="Ichthyostega" modified="200805260212" created="200710100124" tags="design impl" changecount="13">
,Placement>>s are at the very core of all ,editing operations<<renderengine-EditingOperations>>, because they act as handles (smart pointers) to access the ,media objects<<renderengine-MObject>> to be manipulated. Moreover, Placements are the actual content of the EDL(s) and Fixture and thus are small objects with _value semantics_. Many editing tasks include finding some Placement in the EDL or directly take a ref to some Placement. By acting on the Placement object, we can in some cases change parameters of the way the media object is placed (e.g. adjust an offset), while by dereferencing the Placement object, we access the _real_ media object (e.g. for trimming its length). Placements are *templated* on the type of the actual MObject they refer to, thus defining the interface/methods usable on this object.

Actually, the way each Placement locates its subject is implemented by one or several small LocatingPin objects, where subclasses of LocatingPin implement the various different methods of placing and resolving the final location. Notably, we can give a FixedLocation or we can atach to another MObject to get a RelativeLocation, etc. In the typical use case, these LocatingPins are added to the Placement, but never retrieved directly. Rather the Placement acts as a *query interface* for determining the location of the related object. Here, 'location' can be thought of as encompassing multiple dimenstions at the same time. An object can be
* located at a specific point in time
* related to and plugged into a specific output or global bus
* defined to have a position within some ,context-dependant additional dimensions<<renderengine-PlacementDerivedDimension>> like
** the pan position, either on the stereophoic base, or within a fully periphoic (spatial) sound system
** the layer order and overlay mode for video (normal, additive, subtractive, masking)
** the stereoscopic window position (depth parameter) for 3D video
** channel and parameter selection for MIDI data

Placements have _value semantics,_ i.e. we don't stress the identity of a placement object (MObjects on the other hand _do have_ a distinguishable identity): initially, you create a Placement parametrized to some specific kind by adding ,LocatingPin>>s (fixed, relative,...) and possibliy you use a subclass of +Placement<MObject>+ to encode additional type information, say +Placement<Clip>+, but later on, you treat the placement polymorphically and don't care about its kind. The sole purpose of the placement's kind is to select some virtual function implementing the desired behaviour. There is no limitation to one single Placement per MObject, indeed we can have several different Placements of the same MObject (from a users point of view, these behave like being clones). Besides, we can *aggregate* additional ,LocatingPin>>s to one Placements, resulting in their properties and constraints being combined to yield the actual position of the referred MObject.

=== design decisions
* the actual way of placing is implemented similar to the *State Pattern* by small embedded LocatingPin objects.
* these LocatingPin objects form a *decorator* like chain
* resolving into an ExplicitPlacement traverses this chain
* _overconstraining_ a placement is not an error, we just stop traversing the chain (ignoring the remaining additional Placements) at the moment the position is completely defined. 
* we provide subclasses to be able to form collections of e.g. +Placement<Effect>+, but we don't stress polymorphism here. Instead we stick to value semantics and explicitly *allow slicing* (all subclasses have the same size and layout and differ only in their vtable contents). This is justified, because for the polymorphical treating of MObjects the visitor mechanism used by the builder and the EDL is sufficiant, and I don't want to add another layer of complexity by making Placement something like +boost::variant+.
* Why is the question how to access a MObject subinterface a Problem?
*# we want the EDL/Fixture to be a collection of Placements. This means, either we store pointers, or Placement needs to be _one_ unique type!
*# but if Placement is _a single type_, then we can get only MObjects from a Placement.
*# then either we had to do everything by a visitor (which gets the concrete subtype dynamically), or we'd end up switching on type.




<div title="PlacementIndex" modifier="Ichthyostega" modified="200905241504" created="200905090053" tags="SessionLogic spec impl draft" changecount="15">
An implementation facility used to keep track of individual Placements and their relations.
Especially, the ,Session>> maintains such an index, allowing to use the (opaque) PlacementRef tags for referring to a specific 'instance' of an MObject, _placed_ in a unique way into the current session. And, moreover, this index allows for one placement referring to another placement, so to implement a _relative_ placement mode. Because there is an index behind the scenes, it is possible to actually access such a referral in the reverse direction, which is necessary for implementing the desired placement behaviour (if an object instance used as anchor is moved, all objects placed relatively to it have to move accordingly, which necessitates finding those other objects).

=== questions [red]+WIP+
What implementation approach to take for the index. This of course largely depends on the usage pattern, which in turn is not-yet existent. To start with, we need a preliminary implementation!

Using a *flat hashtable* would allow to access a Placement denoted by ID in O(1). This way we could get the Placement, but nothing more. So, additionally we'd have to set up an data record holding additional information:
* the ,scope<<renderengine-PlacementScope>> containing this placement
* especially the path 'up' from this scope, which is used for resolving any queries
* relations to other placements

Alternatively, we could try to use a *structure based index*. Following this idea, we could try to avoid the mentioned description record by folding any of the contained informations into the surrounding data structure:
* the scope would be obvious from the index, resp. from the path used to resolve this index
* any other informations, especially the relations, would be folded into the placement
* this way, the 'index' could be reduced to being the session data structure itself.


=== supported operations
The placement index is utilized by editing operations, by executing the build process. Besides these core operations it allows for resolving PlacementRef objects. This latter functionality is used by all kinds of relative placements and for dealing with them while building, but it is also used to resolve ,object reference tags<<renderengine-MObjectRef>>, which possibly may have been handed out via an external API or may have crossed layer boundaries. From these use cases we derive the following main operations to support:
* find the actual ,Placement>> object for a given ID
* find the _scope_ a given placement resides in. More specifically, find the ,placement defining this scope<<renderengine-PlacementScope>>
* find (any/all) other placements referring to a given placement
* add a new placement to a scope given as parameter
* remove a placement from index

_does a placement need to know it's own ID?_  Obviously, there needs to be a way to find out the ID for a given placement, especially in the following situations:
* for most of the operations above, when querying some additional informations from index for a given placement
* to create a PlacementRef (this is a variant of the 'shared ptr from this'-problem)
On second sight, this problem seems to be quite nasty, because either we have to keep a second index table for the reversed lookup (memory address -> ID), or have to tie the placement by a back-link when adding it to the index/session data structure, or (at least) it forces us to store a copy of the ID _within_ the placement itself. The last possibility seems to create the least impact; but implementing it this way effectively gears the implementation towards a hashtable based approach.

=== implementation [red]+WIP+
Consequently, we incorporate a random hash (implemented as +LUID+) into the individual placement, this way creating an distinguishable _placement identity,_ which is retained on copying of the placement. The actual ID tag is complemented by a compile time type (template parameter), thus allowing to pass on additional context information through API calls.



<div title="PlacementRef" modifier="Ichthyostega" modified="200906020130" created="200905090032" tags="def spec draft" changecount="12">
A generic reference mechanism for Placements, as added to the current session.
While this reference itself is not tied to the actual memory layout (meaning it's _not_ a disguised pointer), the implementation relies on a ,placement index facility<<renderengine-PlacementIndex>> for tracking and retrieving the actual Placement implementation object. As a plus, this approach allows to _reverse_ the relation denoted by such a reference, inasmuch it is possible to retrieve all other placements referring to a given target placement. But for an (external) user, this link to an index implementation is kept transparent and implicit.

=== implementation [red]+WIP+
From the usage context it is clear that the PlacementRef needs to incorporate a simple ID as the only actual data in memory, so it can be downcasted to a POD and passed as such via LayerSeparationInterfaces. And, of course, this ID tag should be the one used by PlacementIndex for organising the Placement index entries, thus enabling the PlacementRef to be passed immediately to the underlying index for resolution. Thus, this design decision is interconnected with the implementation technique used for the index ( -> PlacementIndex). From the requirement of the ID tag to be contained in a fixed sized storage, and also from the expected kinds of queries Ichthyo (5/09) choose to incorporate a +LUID+ as a random hash immediately into the placement and build the ID tag on top of it. 

=== using placement references
Placement references can be created directly from a given placement, or just from an +Placement::ID+ tag. Creation and dereferencing can fail, because the validity of the reference is checked with the index. Placement references have value semantics. Dereferencing searches the denoted Placement via index, yielding a direct (language) ref.

Placement references mimic the behaviour of a real placement, i.e. they proxy the placement API (actually the passive, query-related part of placement's API functions), while directly forwarding calls to the pointee (MObejct or subclass) when using +operator->()+. They can be copied and especially allow a lot of assignments (from placement, placement-ID or even plain LUID), even including a dynamic downcast on the pointee. Bottom line is that a placement ref can pretty much be used in place of a language ref to a real placement, which is crucial for implementing MObjectRef.
[red]+WIP+



<div title="PlacementScope" modifier="Ichthyostega" modified="200905210132" created="200905120304" tags="SessionLogic spec draft" changecount="5">
MObjects are attached into the ,Session>> by adding a ,Placement>>. Because this especially includes the case of _grouping or container objects,_ e.g. tracks or ,meta-clips<<renderengine-VirtualClip>>, any placement may optionally define and root a scope, and every placement is at least contained in one encompassing scope -- of course with the exception of the absolute top level, which can be thought off as being contained in a scope of handling rules.

Thus, while the ,sequences (former called EDL)<<renderengine-EDL>> act as generic container holding a pile of placments, actually there is a more fine grained structure based on the nesting of the tracks, which especially in Lumiera's HighLevelModel belong to the sequence (they aren't a property of the top level timeline as one might expect). Building upon these observations, we actually require each addition of a placement to specify a scope. The implementation of this tie-to-scope is provided by the same mechanism as utilized for relative placements, i.e. an directional placement relation.

=== Kinds of scopes
There is only a limited number of situations constituting a scope
* conceptually, the very top level is a scope of general rules. It is implemented as +Placement<Binding>+, where ,the binding<<renderengine-BindingMO>> is a meta-object representing the relation.
* similarily, the link binding a ,Sequence<<renderengine-EDL>> into either a (top-level) timeline or as virtual media into a VirtualClip is rather special.
* each sequence has at least one (manadtory) top-level placement holding its root track
* tracks may contain nested sub tracks.
* clips and (track-level) effects likewise are associated with an enclosing track.
* an important special case of relative placement is when an object is ,attached<<renderengine-AttachedPlacementProblem>> to another leading object, like e.g. an effect modifying a clip

In any case, adding a placement to a scope is to be implemented such as to ensure the presence of a suitable LocatingPin, serving to tie the placement into the scope. This LocatingPin in turn will manage a relation to the scope-defining placement, and by doing so, it will insert the relation information into the ,index<<renderengine-PlacementIndex>>. The remaining problem for the implementation is to find a suitable factory function to set up the mentioned pin.



<div title="PlanningBuildFixture" modifier="Ichthyostega" modified="200801061937" created="200712100445" tags="impl Builder draft" changecount="11">
_This page is a scrapbook for working out the implementation of how to (re)build the ,Fixture>>_
Structurally, (re)building the Fixture rather belongs to ,Session>>/,EDLs<<renderengine-EDL>>, but it is implemented very similar to the render engine build process: by treating all MObjects found in the various EDLs with a common ,visiting tool<<renderengine-VisitorUse>>, this tool collects a simplified view with everyting made explicit, which can be pulled of as Fixture, i.e. (special kind of) EDL
afterwards.
* there is a _gathering phase_ and a _solving phase_, the gathering is done by visiting.
* during the gathering phase, there *need to be a lock* preventing any other edit operation.
* the solving is delegated to the individual Placements. It is effectively a +const+ operation creating a ExplicitPlacement (copy)
* thus the Fixture contains these newly created ExplicitPlacements, refering to MObjects shared with the original Placements within the EDLs

===== prerequisites
* Session and EDLs exist.
* Pipes exist and are configured

===== postconditions
* the Fixture contains one sorted timeline of ExplicitPlacement instances
* Anything in this list is actually to be rendered
* [red]+TODO: how to store and group the effects?+
* any meta-clips or other funny things have been resolved to normal clips with placement
* any multichannel clips has been broken down to elementary clips [red]+TODO: what is 'elementary'. e.g. stereo sound streams?+
* any globally or otherwise strangely placed effects have been attached either to a clip or to some pipe
* we have one unified list of tracks

<<renderengine-tasksum start>>
<<renderengine-taskadder below>>
<<renderengine-task >> work out how to get the processing of effects chained to some clip right
<<renderengine-task >> work out how to handle multichannel audio (and stereo video)

=== gathering phase
==== preparing
<<renderengine-task>>what data collections to build?

==== treating a Track
<<renderengine-task>>work out how to refer to pipes and do other config
<<renderengine-task>>get some uniqe identifier and get relevant properties

==== treating a +Placement<Clip>+
<<renderengine-task>>check the direct enablement status
<<renderengine-task>>asses the compound status, maybe process recursively

==== treating an +Placement<Effect>+
<<renderengine-task>>find out the application point [red]+really?+

=== solving phase
<<renderengine-task>>trigger solving on all placements
<<renderengine-task>>sort the resulting ExplicitPlacements

<<renderengine-tasksum end>>



<div title="PlanningNodeCreatorTool" modifier="Ichthyostega" modified="200810170221" created="200712090659" tags="impl Builder draft" changecount="2">
_This page is a scrapbook for working out the implementation of the builder_

* NodeCreatorTool is a ,visiting tool<<renderengine-VisitorUse>>
* the render engine to be built is contained as state within this tool object while it is passed around
===== prerequisites
* Session and EDLs exist.
* Pipes exist and are configured
* Fixture contains ExplicitPlacement for every MObject to be rendered, and nothing else

<<renderengine-tasksum start>>
<<renderengine-taskadder>>

==== preparing
We need a way of addressing existing ,pipes<<renderengine-Pipe>>. Besides, as the Pipes and Tracks are referred by the Placements we are processing, they are guaranteed to exist.

==== treating a Pipe
<<renderengine-task>>get the ,processing pattern<<renderengine-ProcPatt>> of the pipe by accessing the underlying pipe asset.
<<renderengine-task>>process this ProcPatt recursively

==== treating a processing pattern
<<renderengine-task>>[red]+finally go ahead and define what a ProcPatt need to be...+

==== treating a +Placement<Clip>+
<<renderengine-task>>get the ProcPatt of the underlying media (asset)
<<renderengine-task>>process the ProcPatt recursively
<<renderengine-task>>access the ClipSourcePort (which may be created on-the-fly)
<<renderengine-task>>enqueue an WiringRequest for connecting the source pipeline to the source port
<<renderengine-task>>process the clip's render pipe recursively (thus adding the camera etc.)
<<renderengine-task>>enqueue an WiringRequest for any placement to some pipe for this clip.

* Note: we suppose
** all wiring requests will be done after the processing of entities
** all effects placed to this clip will be processed after this clip (but before the wiring requests)

==== treating an +Placement<Effect>+
<<renderengine-task>>[red]+how to assure that effecs are processed after clips/pipes??+
<<renderengine-task>>find out the application point
<<renderengine-task>>build a transforming node for the effect and insert it there

==== postprocessing
<<renderengine-task>>sort and group the assembled list of ,wiring requests<<renderengine-WiringRequest>> by pipes

<<renderengine-tasksum end>>



<div title="PlanningSessionInMem" modifier="Ichthyostega" modified="200907212231" created="200904252258" tags="impl SessionLogic draft" changecount="33">
_This page is a scrapbook for working out the implementation of the ,Session Datastructure in Memory<<renderengine-SessionDataMem>>_
This is a difficult untertaking, because there are several dependencies (most of which aren't fully designed yet as of 5/09)
* the GUI uses an adapted version of the HighLevelModel; these datastructures are intended to be backed by the Session
* of course, the structure of the HighLevelModel itself creates constraints on the session's data handling
* the persistent SessionStorage depends on the requirements of the session in memory
* the (currently as of 1/09 just half-way designed) ,Builder>> relies heavily on the data within the session.

===== requirements
* can discover and ennumerate the structure
* can ennumerate the contents (objects) of a specific scope
* provide ,references<<renderengine-MObjectRef>> as result of these query/discovery operations

===== invariants
* memory management consistency
* support ongoing render processes
* [red]+TODO: hrmpf+

<<renderengine-tasksum start>>
<<renderengine-taskadder below>>
<<renderengine-task 3 3 3>> how to deal with UNDO
<<renderengine-task 2 2 2>> how to deal with CommandDefinition
<<renderengine-task >> outline the CommandLifecycle
<<renderengine-task 5 15 15>> implement a system for defining parameters and targets
<<renderengine-task >> implement a system for generating operations
<<renderengine-task 2 2 1>> define the general structure of operations
<<renderengine-task >> specify the editing operations

=== Interface
<<renderengine-task 1 1 1>> evaluate models for the ,reference problem<<renderengine-MObjectRef>>
<<renderengine-task 2 3 3>> implement an hash-ID
<<renderengine-task 1 2 2>> incorporate this ID into class Placement
<<renderengine-task 2 2>> define PlacementRef and MObjectRef behaviour
<<renderengine-task 1 0>> wire up ,Session>> interface as a LayerSeparationInterface
<<renderengine-task 1 1 0>> implement PlacementRef
<<renderengine-task 2 2 0>> implement MObjectRef

=== Datastructure
<<renderengine-task>>investigate the best granularity for object collections
<<renderengine-task >> define a record to be used within the index
<<renderengine-task >> implement basic index operations
<<renderengine-task 1>> implement adding a placement relation

<<renderengine-tasksum end>>



<div title="PlayerDummy" modifier="Ichthyostega" modified="200906071810" created="200901300209" tags="GuiIntegration dynamic img" changecount="17">
*Joelholdsworth* and *Ichthyo* created this player mockup in 1/2009 to find out about the implementation details regarding integration and colaboration between the layers. There is no working render engine yet, thus we use a DummyImageGenerator for creating faked yuv frames to display. Within the GUI, there is a PlaybackController hooked up with the transport controls on the timeline pane.
. first everything was contained within PlaybackController, which spawns a thread for periodically creating those dummy frames
. then, a PlayerService was factored out, now implemented within Proc-Layer (probably to be relocated into the backend for the final version). A new LayerSeparationInterface called *DummyPlayer* was created and set up as a ,Subsystem>> within main().
. the next step was to support multiple playback processes going on in parallel. Now, the PlaybackController holds an smart-handle to the PlayProcess currently generating output for this viewer, and invokes the transport control functions and the pull frame call on this handle.
. then, also the tick generation (and thus the handling of the thread which pulls the frames) was factored out and pushed down into the mentioned PlayProcess. For this to work, the PlaybackController now makes a display slot available on the public GUI DisplayFacade interface, so the PlayProcessImpl can push up the frames for display within the GUI
image::Overview to the dummy player operation<<renderengine-draw/PlayerArch1.png[]

=== when playing...
As a prerequisite, a viewer has to be prepared within the GUI. A XV video display widget is wired up to a sigc++ signal slot, using the Glib::Dispatcher to forward calls from the play process thread to the GTK main event loop thread. All of this wiring actually is encapsulated as a DisplayerSlot, created and registered with the DisplayService.

When starting playback, the display slot handle created by these preparations is used to create a PlayProcess on the DummyPlayer interface. Actually
* a PlayProcessImpl object is created down within the player implementation
* this uses the provided slot handle to actually _allocate_ the display slot via the Display facade. Here, _allocating_ means registering and preparing it for output by _one single_ PlayProcess. For the latter, this allocation yields an actually opened display handle.
* moreover, the PlayProcessImpl aquires an TickService instance, which is still trotteling (not calling the periodic callback)
* probably, a real player at this point would initiate a rendering process, so he can fetch the actual output frames periodically.
* on the 'upper' side of the DummyPlayer facade, an lib::Handle object is created to track and manage this PlayProcesImpl instance
The mentioned handle is returned to the PlaybackController within the GUI, which uses this handle for all further interactions with the Player. The handle is ref counting and has value semantics, so it can be stored away, passed as parameter and so on. All such handles corresponding to one PlayProcess form a family; when the last goes out of scope, the PlayProcess terminates and deallocates any resources. Conceptually, this corresponds to pushing the 'stop' button. Handles can be deliberately disconnected by calling +handle.close()+ -- this has the same effect as deleting a handle (when all are closed or deleted the process ends).

All the other play control operations are simply forwarded via the handle and the PlayProcessImpl. For example, 'pause' corresponds to setting the tick frequency to 0 (thus temporarily discontinuing the tick callbacks). When allocating the display slot in the course of creating the PlayProcessImpl, the latter only accesses the Display facade. It can't access the display or viewer directly, because the GUI lives within an plugin; lower layers aren't allowed to call GUI implementation functions directly. Thus, within the Display facade a functor (proxy) is created to represent the output sink. This (proxy) Displayer can be used within the implementation of the perodic callback function. As usual, the implementation of the (proxy) Displayer can be inlined and doesn't create runtime overhead. Thus, each frame output call has to pass though two indirections: the function pointer in the Display facade interface, and the Glib::Dispatcher.

=== rationale
There can be multiple viewer widgets, to be connected dynamically to multiple play-controllers. (the latter are associated with the timeline(s)). Any playback can require multiple playback processes to work in parallel. The playback controller(s) should not be concerned with managing the play processes, which in turn should neither care for the actual rendering, nor manage the display frequency and synchronisation issues. Moreover, the mentioned parts live in different layers and especially the GUI needs to remain separated from the core. And finally, in case of a problem within one play process, it should be able to unwind automatically, without interfering with other ongoing play processes.



<div title="Playlist" modifier="Ichthyostega" modified="200706250727" created="200706220456" tags="def" changecount="3">
Playlist is a sequence of individual Render Engine Processors able to render a segment of the timeline. So, together these Processors are able to render the whole timeline (or part of the timeline if only a part has to be rendered).

_Note, we have yet to specify how exactly the building and rendering will work together with the backend. There are several possibilities how to structure the Playlist_



<div title="ProblemsTodo" modifier="Ichthyostega" modified="200806030155" created="200708050524" tags="design discuss" changecount="16">
Open issues, Things to be worked out, Problems still to be solved... 

==== Parameter Handling
The requirements are not quite clear; obviously Parameters are the foundation for getting automation right and for providing effect editing interfaces, so it seems to me we need some sort of introspection, i.e. Parameters need to be discovered, enumerated and described at runtime. ( -> see ,tag:automation<<renderengine-automation>>)

*Automation Type*: Directly connected is the problem of handling the _type_ of parameters sensible, including the value type of automation data. My first (somewhat naive) approach was to 'make everything a double'. But this soon leads into quite some of the same problems haunting the automation solution implemented in the current Cinelerra codebase. What makes the issue difficult is the fact we both need static diversity as well as dynamic flexibility. Usually, when combining hierarchies and templates, one has to be very careful; so I just note the problem down at the moment and will revisit it later, when I have a more clear understanding of the demands put onto the ,ProcNode>>s

==== Treatment of Time (points) and Intervals
At the moment we have no clear picture what is needed and what problems we may face in that domain.
From experience, mainly with other applications, we can draw the following conclusions
* drift and rounding errors are dangerous, because time in our context usually is understood as a fixed grid (Frames, samples...)
* fine grained time values easily get very large
* Cinelerra currently uses the approach of simply counting natural values for each media type separately. In an environment mixing several different media types freely, this seems a bit too simplistic (because it actually brings in the danger of rounding errors, just think at drop frame TC)

==== Organizing of Output Channels
How to handle the simultaneous rendering of several output streams (video, audio channels). Shall we treat the EDL as one entity containing different output channels, or should it rather be seen as a composite of several sub-EDLs, each for only one output channel? This decision will be reflected in the overall structure of the network of render nodes: We could have a list of channel-output generating pipelines in each processor (for every segment), or we could have independently segmented lists of Processors for every output channel/type. The problem is, _it is not clear what approach to prefer at the moment_  because we are just guessing.

==== Tracks, Channels, Layers
Closely related to this is the not-so-obvious problem how to understand the common global structures found in most audio and video editing applications. Mostly, they stem from imitating hardware recording and editing solutions, thus easing the transition for professionals grown up with analogue hardware based media. But as digital media are the de-facto standard nowadays, we could rethink some of this accidental complexity introduced by sticking to the hardware tool metaphor.
* is it really necessary to have fixed global tracks?
* is it really helpful to feed 'source tracks' into global processing busses/channels?
Users accustomed with modern GUI applications typically expect that _everything is a object_  and can be pulled around and  manipulated individually. This seems natural at start, but raises the problem of providing a efficient workflow for handling larger projects and editing tasks. So, if we don't have a hard wired multitrack+bus architecture, we need some sort of templating to get the standard editing use case done efficiently.

==== Compound and Multiplicity
Simple relations can be hard wired. But, on the contrary, it would be as naive to define a Clip as having a Video track and two audio tracks, as it would be naive to overlook the problem of holding video and corresponding audio together. And, moreover, the default case has to be processed in a _straight forward_ fashion, with as few tests and decisions as possible. So, basically each component participating in getting the core processing done has to mirror the structure pattern of the other parts, so that  processing can be done without testing and forking. But this leaves us with the problem where to put the initial knowledge about the structural pattern used for building up the compound structures and -- especially -- the problem how to treat different kinds of structural patterns, how to detect the pattern to be applied and how to treat multiple instances of the same structural pattern.

One example of this problem is the ,handling of multichannel media<<renderengine-MultichannelMedia>>. Following the above reasoning, we end with having a ,'structural processing pattern'<<renderengine-ProcPatt>>, typically one video stream with MPEG decoder and a pair of audio streams which need either to be routed to some 'left' and 'right' output pipes, or have to be passed through a panning filter accordingly. Now the problem is: _create a new instance of this structure for each new media, or detect which media to subsume under a existing pattern instance._

==== Parallelism
We need to work out guidelines for dealing with operations going on simultaneously. Certainly, this will divide the application in several different regions. As always, the primary goal is to avoid multithread problems altogether. Typically, this can be achieved by making matters explicit: externalizing state, make the processing subsystems stateless, queue and schedule tasks, use isolation layers.
* the StateProxy is a key for the individual render processes state, which is managed in separate ,StateFrame>>s in the backend. The ,processing network<<renderengine-ProcNode>> is stateless.
* the ,Fixture>> provides an isolation layer between the renderengine and the Session/EDL
* all EditingOperations are not threadsafe intentionally, because they are ,scheduled<<renderengine-ProcLayerScheduler>>



<div title="Proc-Layer" modifier="Ichthyostega" created="200802031814" tags="def" changecount="1">
The current Lumiera architecture separates functionality into three Layers: +GUI+, +Proc+ and +Backend+.

While the Backend is responsible for Data access and management and for carrying out the computation intensive media opteratons, the middle Layer or Proc-Layer contains ,assets<<renderengine-Asset>> and ,Session>>, i.e. the user-visible data model and provides configuration and behaviour for these entities. Besides, he is responsible for ,building and configuring<<renderengine-Builder>> the ,render engine<<renderengine-RenderEngine>> based on the current Session state.



<div title="ProcAsset" modifier="Ichthyostega" modified="200906071810" created="200709221343" tags="def classes img" changecount="2">
All Assets of kind asset::Proc represent _processing algorithms_ in the bookkeeping view. They enable loading, browsing and maybe even parametrizing all the Effects, Plugins and Codecs available for use within the Lumiera Session.

Besides, they provide an 'inward interface' for the ,ProcNode>>s, enabling them to dispatch the actual processing call while rendering. Actually, this interface is always accessed via an Effect-MObject; mostly it is investigated and queried in the build process when creating the corresponding processor nodes.  -> see EffectHandling for details

[red]+todo: the naming scheme??+

image::Asset Classess../uml/fig131077.png[]



<div title="ProcLayer" modifier="Ichthyostega" modified="200708100338" created="200708100333" tags="def" changecount="2">
The middle Layer of our current Architecture plan, i.e. the layer managing all processing and manipulation, while the actual data handling is done in the backend and the user interaction belongs to the GUI Layer.

-> see the ,Overview>>



<div title="ProcLayer and Engine" modifier="Ichthyostega" modified="200907212232" created="200706190056" tags="overview" changecount="19">
The Render Engine is the part of the application doing the actual video calculations. Its operations are guided by the Objects and Parameters edited by the user in ,the EDL<<renderengine-EDL>> and it retrieves the raw audio and video data from the ,Data backend<<renderengine-backend.html>>. Because the inner workings of the Render Engine are closely related to the structures used in the EDL, this design covers ,the aspect of objects placed into the EDL<<renderengine-MObjects>> as well.
<<renderengine-<
*Status*: started out as design draft in summer '07, Ichthyo is now in the middle of a implementing the foundations and main structures in C++
* basic ,AssetManager>> working
* ,Builder>> implementation is on the way ( ->,more<<renderengine-BuilderPrimitives>>)
* made a first draft of how to wire and operate procesing nodes ( ->,more<<renderengine-RenderMechanics>>)
* first attempts to ,integrate with the GUI<<renderengine-GuiIntegration>>
* first outline of the ,session implementation<<renderengine-PlanningSessionInMem>>
<<renderengine-<

=== Summary
We have several kinds of 'things' organized as ,assets<<renderengine-Asset>> in the AssetManager, like media, clips, effects, codecs, configuration templates. Within the context of the ,EDL>>, we can use these as ,+Media Objects+<<renderengine-MObjects>> -- especially, we can ,place<<renderengine-Placement>> them in various kinds within the EDL and relative to one another. Basically, this is the ,editing work<<renderengine-EditingOperations>> done by the user.

Now, from any given configuration within the EDL, we create sort or a frozen- and tied-down snapshot, here called ,+Fixture+<<renderengine-Fixture>>, containing all currently active MObjects, broken down to elementary parts and made explicit if necessary. This Fixture acts as a isolation layer towards the Render Engine. We will hand it over to the  ,Builder>>, which in turn will transform it into a network of connected ,render nodes<<renderengine-ProcNode>>. This network _implements_ the ,Render Engine<<renderengine-OverviewRenderEngine>>.

The system is *open* inasmuch every part mirrors the structure of corresponding parts in adjacent subsystems, and the transformation of any given structure from one subsystem (e.g. Asset) to another (e.g. Render Engine) is done with minimal 'magic'. So the whole system should be able to handle completely new structures mostly by adding new configurations and components, without much need of rewriting basic workings.

==== see also
-> ,Overview>> of Subsystems and Components, and DesignGoals
-> ,An Introduction<<renderengine-WalkThrough>> discussing the central points of this design
-> ,Overview Session (high level model)<<renderengine-SessionOverview>>
-> ,Overview Render Engine (low level model)<<renderengine-OverviewRenderEngine>>
-> BuildProcess and RenderProcess
-> ,Two Examples<<renderengine-Examples>> (Object diagrams) 
-> how ,Automation>> works
-> ,Problems<<renderengine-ProblemsTodo>> to be solved and notable ,design decisions<<renderengine-DesignDecisions>>
-> ,Implementation Details<<renderengine-ImplementationDetails>> [red]+WIP+



<div title="ProcNode" modifier="Ichthyostega" modified="200806211606" created="200706220409" tags="def spec" changecount="6">
A data processing node within the Render Engine. Its key feature is the possibility to pull from it one (freely addressable) ,Frame>> of calculated data. Further, each ProcNode has the ability to be wired with other nodes and ,Parameter Providers<<renderengine-ParamProvider>>

====  [red]+open questions+
* how to address a node
* how to type them
* how to discover the number and type of the ports
* how to discover the possible parameter ports
* how to define and query for additional capabilities

-> see also the ,open design process draft<<renderengine-http:_www.pipapo.org/pipawiki/Lumiera/DesignProcess/DesignRenderNodesInterface>>
-> see ,mem management<<renderengine-ManagementRenderNodes>>
-> see RenderProcess



<div title="ProcPatt" modifier="Ichthyostega" modified="200805260329" created="200709212315" tags="def design" changecount="9">
This special type of ,structural Asset<<renderengine-StructAsset>> represents information how to build some part of the render engine's processing nodes network. Processing patterns can be thought of as a blueprint or micro program for construction. Most notably, they are used for creating nodes reading, decoding and delivering source media material to the render network, and they are used for building the output connection via faders, summation or overlay nodes to the global pipes (busses). Each ,media Asset<<renderengine-MediaAsset>> has associated processing patterns describing the codecs and other transformations needed to get at the media data of this asset. (and because media assets are typically compound objects, the referred ProcPatt will be compound too). Similarily, for each stream kind, we can retrieve a processing pattern for making output connections. Obviously, the possibilities opened by using processing patterns go far beyond.

Technically, a processing pattern is a list of building instructions, which will be _executed_ by the ,Builder>> on the render node network under construction. This implies the possibility to define further instruction kinds when needed in future; at the moment the relevant sorts of instructions are
* attach the given sequence of nodes to the specified point
* recursively execute a nested ProcPatt
More specifically, a sequence of nodes is given by a sequence of prototypical effect and codec assets (and from each of them we can create the corresponding render node). And the point to attach these nodes is given by an identifier -- in most cases just '+current+', denoting the point the builder is currently working at, when treating some placed MObject which in turn yielded this processing pattern in question.

Like all ,structural assets<<renderengine-StructAsset>>, ProcPatt employs a special naming scheme within the asset name field, which directly mirrors its purpose and allows to bind to existing processing pattern instances when needed. [red]+TODO: that's just the general idea, but really it will rather use some sort of tags. Yet undefined as of 5/08+ The idea is letting all assets in need of a similar processing pattern refer to one shared ProcPatt instance. For example, within a MPEG video media asset, at some point there will be a ProcPatt labeled +stream(mpeg)+. In consequence, all MPEG video will use the same pattern of node wiring. And, of course, this pattern could be changed, either globally, or by binding a single clip to some other processing pattern (for making a punctual exception from the general rule) 

==== defining Processing Patterns
The basic working set of processing patterns can be expected to be just there (hard wired or default configuration, mechanism for creating an sensible fallback). Besides, the idea is that new processing patterns can be added via rules in the session and then referred to by other rules controlling the build process. Any processing pattern is assembled by adding individual build instructions, or by including another (nested) processing pattern.

==== retrieving a suitable Processing Pattern
For a given situation, the necessary ProcPatt can be retrieved by issuing a ,configuration query<<renderengine-ConfigQuery>>. This query should include the needed capabilities in predicate form (technically this query is a Prolog goal), but it can leave out informations by just requesting 'the default'  -> see DefaultsManagement

==== how does this actually work?
Any processing pattern needs the help of a passive holder tool suited for a specific ,building situation<<renderengine-BuilderPrimitives>>; we call these holder tools ,building moulds<<renderengine-BuilderMould>>. Depending on the situation, the mould has been armed up by the builder with the involved objects to be connected and extended. So, just by issuing the _location ID_ defined within the individual build instruction (in most cases simply +'current'+), the processing pattern can retrieve the actual render object to use for building from the mould it is executed in.

==== errors and misconfiguration
Viewed as a micro program, the processing patterns are *weak typed* -- thus providing the necessary flexibility within an otherwise strong typed system. Consequently, the builder assumes they are configured _the right way_ -- and will just bail out when this isn't the case, marking the related part of the high-level model as erroneous.
-> see BuilderErrorHandling for details



<div title="Processors" modifier="Ichthyostega" created="200706220412" tags="def" changecount="1">
a given Render Engine configuration is a list of Processors. Each Processor in turn contains a Graph of ProcNode.s to do the acutal data processing. In order to cary out any calculations, the Processor needs to be called with a StateProxy containing the state information for this RenderProcess



<div title="QueryImplProlog" modifier="Ichthyostega" modified="200802291526" created="200801202321" tags="draft design" changecount="17">
_obviously, getting this one to work requires quite a lot of technical details to be planned and implemented._ This said...
The intention is to get much more readable ('declarative') and changeable configuration as by programming the decision logic literately within the implementation of some object.

=== Draft
As an example, specifying how a Track can be configured for connecting automatically to some +mpeg+ bus (=pipe)

[source,cpp]
----
resolve(O, Cap) :- find(O), capabilities(Cap).
resolve(O, Cap) :- make(O), capabilities(Cap).
capabilities(Q) :- call(Q).

stream(T, mpeg) :- type(T, track), type(P, pipe), resolve(P, stream(P,mpeg)), place_to(P, T).
----

Then, running the goal +:-resolve(T, stream(T,mpeg)).+ would search a Track object, try to retrieve a pipe object with stream-type=mpeg and associate the track with this pipe. This relies on a predicate +stream(P,mpeg)+ implemented (natively) for the pipe object. So, +Cap+ is the query issued from calling code -- here +stream(T,mpeg)+, the type guard +type(T, track)+ will probably be handled or inserted automatically, while the predicate implementations for find/1, make/1, stream/2, and place_to/2 are to be provided by the target types.
* 'The supporting system' had to combine several code snippets into one rule system to be used for running queries, with some global base rules, rules injected by each individual participating object kind and finally user provided rules added by the current session. The actual query is bound to +Cap+ (and consequently run as a goal by +call(Q)+). The implementation needs to provide a symbol table associating variable terms (like +T+ or +P+) to C/C++ object types, enabling the participating object kinds to register their specific predicate implementations. This is crucial, because there can be no general scheme of object-provided predicates (for each object kind different predicates make sense, e.g. ,pipes<<renderengine-PipeHandling>> have other possibilities than ,wiring requests<<renderengine-WiringRequest>>). Basically, a query issues a Prolog goal, which in turn evaluates domain specific predicates provided by the participating objects and thus calls back into C/C++ code. The supporting system maintains the internal connection (via the 'type' predicate) such that from Prolog viewpoint it looks as if we were binding Variables directly to object instances. (there are some nasty technical details because of the backtracking nature of Prolog evaluations which need to be hidden away)
* Any 'participating object kind' needs a way to declare domain specific predicates, thus triggering the registration of the necessary hooks within the supporting system. Moreover, it should be able to inject further prolog code (as shown in the example above with the +strem(T, mpeg)+ predicate. For each of these new domain specific predicates, there needs to be a functor which can be invoked when the C implementation of the predicate is called from Prolog (in some cases even later, when the final solution is 'executed', e.g. a new instance has been created and now some properties need to be set).

==== a note on Plugins
In the design of the Lumiera Proc Layer done thus far, we provide _no possibility to introduce a new object kind_ into the system via plugin interface. The system uses a fixed collection of classes intended to cover all needs (Clip, Effect, Track, Pipe, Label, Automation, Macro-Clips). Thus, plugins will only be able to provide new parametrisations of existing classes. This should not be any real limitation, because the whole system is designed to achieve most of its functionality by freely combining rather basic object kinds. As a plus, it plays nicely with any plain-C based plugin interface. For example, we will have C++ adapter classes for the most common sorts of effect plugin (pull system and synchronous frame-by-frame push with buffering) with a thin C adaptation layer for the specific external plugin systems used. Everything beyond this point can be considered 'condiguration data' (including the actual plugin implementation to be loaded)



<div title="RelationClipAsset" modifier="Ichthyostega" modified="200710212327" created="200710191541" tags="design decision img" changecount="7">
What is the Role of the asset::Clip and how exactly are Assets and (Clip)-MObjects related?

First of all: MObjects are the dynamic/editing/manipulation view, while Assets are the static/bookkeeping/searching/information view of the same entities. Thus, the asset::Clip contains the general configuration, the ref to the media and descriptive properties, while all parameters being 'manipulated' belong to the session::Clip (MObject). Besides that, the practical purpose of asset::Clip is that you can save and remember some selection as a Clip (Asset), maybe even attach some informations or markup to it, and later be able to (re)create a editable representation in the Session (the GUI could implement this by allowing to drag from the asset::Clip GUI representation to the timeline window)

==== dependencies
The session::Clip (frequently called 'clip-MO', i.e. the MObject) _depends on the Asset._ It can't exist without the Asset, because the Asset is needed for rendering. The other direction is different: the asset::Clip knows that there is a dependant clip-MO, there could be _at most one_ such clip-MO depending on the Asset, but the Asset can exist without the clip-MO (it gives the possibility to re-create the clip-MO).

==== deletions
When the Asset or the corresponding asset::Media is deleted, the dependant clip-MO has to disappear. And the opposite direction?
* Model-1: asset::Clip has a weak ref to the clip-MO. Consequently, the clip-MO can go out of scope and disappear, so the asset::Clip has to maintain the information of the clip's dimensions (source position and length) somewhere. Because of MultichannelMedia, this is not so simple as it may look at first sight.
* Model-2: asset::Clip holds a smart ptr to the clip-MO, thus effectively keeping it alive. 'obviously the better choice'
In either case, we have to solve the *problem of clip asset proliferation*

==== multiplicity and const-ness
The link between MObject and Asset should be +const+, so the clip can't change the media parameters. Because of separation of concerns, it would be desirable that the Asset can't _edit_ the clip either (meaning +const+ in the opposite direction as well). But unfortunately the asset::Clip is in power to delete the clip-MO and, moreover, handles out a smart ptr (,Placement>>) referring to the clip-MO, which can (and should) be used to place the clip-MO within the EDL and to manipulate it consequently...
image:../Outline of the Build Process../uml/fig131333.png[]
At first sight the link between asset and clip-MO is a simple logical relation between entities, but it is not strictly 1:1 because typical media are ,multichannel<<renderengine-MultichannelMedia>>. Even if the media is compound, there is _only one asset::Clip_, because in the logical view we have only one 'clip-thing'. On the other hand, in the Session/EDL, we have a compound clip MObject comprised of several elementary clip objects, each of which will refer to its own sub-media (channel) within the compound media (and don't forget, this structure can be tree-like)
[red]+open question:+ do the clip-MO's of the individual channels refer directly to asset::Media? does this mean the relation is different from the top level, where we have a relation to a asset::Clip??



<div title="RenderEngine" modifier="Ichthyostega" modified="200802031835" created="200802031820" tags="def" changecount="2">
Conceptually, the Render Engine is the core of the application. But -- surprisingly -- we don't even have a distinct »RenderEngine« component in our design. Rather, the engine is formed by the cooperation of several components spread over two layers (Backend and Proc-Layer): The ,Builder>> creates a network of ,render nodes<<renderengine-ProcNode>>, which is used by the Backend to pull individual Frames.
-> OverviewRenderEngine



<div title="RenderEntities" modifier="Ichthyostega" modified="200906071810" created="200706190715" tags="Rendering classes img" changecount="10">
The ,Render Engine<<renderengine-Rendering>> only carries out the low-level and performance critical tasks. All configuration and decision concerns are to be handled by ,Builder>> and ,Controller>>. While the actual connection of the Render Nodes can be highly complex, basically each Segment of the Timeline with uniform characteristics is handled by one Processor, which is a graph of ,Processing Nodes<<renderengine-ProcNode>> discharging into a ExitNode. The Render Engine Components as such are _stateless_ themselves; for the actual calculations they are combined with a StateProxy object generated by and connected internally to the ,Controller>>, while at the same time holding the Data Buffers (Frames) for the actual calculations.

image::Entities comprising the Render Engine../uml/fig128389.png[]



<div title="RenderImplDetails" modifier="Ichthyostega" modified="200810140245" created="200806220211" tags="Rendering impl img" changecount="22">
Below are some notes regarding details of the actual implementation of the render process and processing node operation. In the description of the ,render node operation protocol<<renderengine-NodeOperationProtocol>> and the ,mechanics of the render process<<renderengine-RenderMechanics>>, these details were left out deliberately.

=== Layered structure of State
State can be seen as structured like an onion. All the ,StateAdapter>>s in one call stack are supposed to be within one layer: they all know of a 'current state', which in turn is a StateProxy (and thus may refer yet to another state, maybe accros the network or in the backend or whatever). The actual +process()+ function 'within' the individual nodes just sees a single StateAdapter and thus can be thought to be a layer below.

=== Buffer identification
For the purpose of node operation, Buffers are identified by a _Buffer-handle,_ which contains both the actual buffer pointer and an internal indes and classification of the source providing the buffer; the latter information is used for deallocation. Especially for calling the +process()+ function (which is supposed to be plain C) the respective StateAdapter provides an array containing just the output and input buffer pointers

=== Problem of multi-channel nodes
Some data processors simply require to work on multiple channels simultanously, while others work just on a single channel and will be replicated by the builder for each channel invoved. Thus, we are struck with the nasty situation that the node graph may go through some nodes spanning the chain of several channels. Now the decision is _not to care for this complexity within a single chain calculating a single channel._ We rely solely on the cache to avoid duplicated calculations. When a given node happens to produce multiple output buffers, we are bound to allocate them for the purpose of this nodes +process()+ call, but we just 'let go' the buffers not needed immediately for the channel acutally to be processed. For this to work, it is supposed that the builder has wired in a caching, and that the cache will hit when we touch the same node again for the other channels.

Closely related to this is the problem how to number and identify nodes and thus to be able to find calculated frames in cache ( -> ,here<<renderengine-NodeFrameNumbering>>)

=== Configuration of the processing nodes
image:../uml/fig132357.png[]
Every node is actually decomposed into three parts
* an interface container of a ProcNode subclass
* an +const+ WiringDescriptor, which is actually parametrized to a subtype encoding details of how to carry out the intended operation
* the Invocation state created on the stack for each +pull()+ call. It is comprised of references to an StateAdapter object  and the current overall process state, the WiringDescriptor, and finally a table of suitable buffer handles
Thus, the outer container can be changed polymorphically to support the different kinds of nodes (large-scale view). The actual wiring of the nodes is contained in the WiringDescriptor, including the +process()+ function pointer. Additionally, this WiringDescriptor knows the actual type of the operation Strategy, and this actual type has been chosen by the builder such as to select details of the desired operation of this node, for example caching / no caching or maybe OpenGL rendering or the special case of a node pulling directly from a source reader. Most of this configuration is done by selecting the right template specialisation within the builder; thus in the critical path most of the calls can be inlined

==== composing the actual operation Strategy
As shown in the class diagram to the right, the actual implementation is assembled by chaining together the various policy classes governing parts of the node operation, like Caching, in-Place calculation capability, etc. ( -> see ,here<<renderengine-WiringDescriptor>> for details). The rationale is that the variable part of the Invocation data is allocated at runtime directly on the stack, while a precisely tailored call sequence for 'calculating the predecessor nodes' can be defined out of a bunch of simple building blocks. This helps avoiding 'spaghetti code', which would be especially dangerous because of the large number of different execution paths to get right. Additionally, a nice side effect of this implementation technique is that a good deal of the implementation is eligible to inlining.
We _do employ_ some virtual calls for the buffer management in order to avoid coupling the policy classes to the actual number of in/out buffers. (As of 6/2008, this is mainly a precaution to be able to control the number of generated template instances. If we ever get in the region of several hundred individual specialisations, we'd need to separate out the allocation of the 'buffer table' into a hand-made stack-like buffer allocated from the heap.)

=== Rules for buffer allocation and freeing
* only output buffers are allocated. It is _never necessary_ to allocate input buffers!
* buffers are to be allocated as late as possible, typically just before invoking +process()+
* buffers are allways allocated by calling to the preceeding StateAdapter in the callstack ('parent state'), because of the possibility of writing the result to cache.
* +pull()+ returns a handle for the single output requested by this call. Using this ID, the caller may retrieve the actual buffer holding the result from the 'current state' StaeProxy.
* any other buffers filled with results in the course of the same +process()+ call can be released immediately before returning from the +pull()+
* similar, and input buffers are to be released immediately after the +process()+ call, but before returing from this +pull()+
* buffers are allways released by calling to the 'current state' (which is a StateProxy), providing the buffer-ID to be released

@@clear(right):display(block):@@



<div title="RenderMechanics" modifier="Ichthyostega" modified="200906071809" created="200806030230" tags="Rendering impl dynamic img" changecount="28">
While the render process, with respect to the dependencies, the builder and the processing function is sufficiently characterized by referring to the *pull principle* and by defining a ,protocol<<renderengine-NodeOperationProtocol>> each node has to adhere to -- for actually get it coded we have to care for some important details, especially _how to manage the buffers._ It may well be that the length of the code path necessary to invoke the individual processing functions is finally not so important, compared with the time spent at the inner pixel loop within these functions. But my guess is (as of 5/08), that the overall number of data moving and copying operations _will be_ of importance.

=== requirements
* operations should be 'in place' as much as possible
* because caching necessitates a copy, the points where this happens should be controllable.
* buffers should accommodate automatically to provide the necessary space without clipping the image.
* the type of the media data can change while passing through the network, and so does the type of the buffers.
On the other hand, the processing function within the individual node needs to be shielded from these complexities. It can expect to get just _NI_ input buffers and _NO_ output buffers of required type. And, moreover, as the decision how to organize the buffers certainly depends on non-local circumstances, it should be preconfigured while building.

=== data flow
image:../uml/fig131973.png[]
Not everything can be preconfigured though. The pull principle opens the possibility for the node to decide on a per call base what predecessor(s) to pull (if any). This decision may rely on automation parameters, which thus need to be accessible prior to requesting the buffer(s). Additionally, in a later version we plan to have the node network calculate some control values for adjusting the cache and backend timings -- and of course at some point we'll want to utilize the GPU, resulting in the need to feed data from our processing buffers into some texture representation.

=== buffer management
Besides the StateProxy representing the actual render process and holding a couple of buffer (refs), we employ a lightweight adapter object in between. It is used _for a single +pull()+-call_ -- mapping the actual buffers to the input and output port numbers of the processing node and for dealing with the cache calls. While the StateProxy manages a pool of frame buffers, this interspersed adapter allows us to either use a buffer retrieved from the cache as an input, possibly use a new buffer located within the cache as output, or (in case no caching happens) to just use the same buffer as input and output for 'in-place'-processing. The idea is that most of the configuration of this adapter object is prepared in the wiring step while building the node network.

The usage patern of the buffers can be stack-like when processing nodes require multiple input buffers. In the standard case, which also is the simplest case, a pair of buffers (or a single buffer for 'in-place' capable nodes) suffices to calculate a whole chain of nodes. But -- as the recursive descent means depth-first processing -- in case multiple input buffers are needed, we may encounter a situation where some of these input buffers already contain processed data, while we have to descend into yet another predecessor node chain to pull the data for the remaining buffers. Care has to be taken _to allocate the buffers as late as possible,_ otherwise we could end up holding onto a buffer almost for each node in the network. Effectively this translates into the rule to allocate output buffers only after all input buffers are ready and filled with data; thus we shouldn't allocate buffers when _entering_ the recursive call to the predecessor(s), rather we have to wait until we are about to return from the downcall chain.
Besides, these considerations also show we need a means of passing on the current buffer usage pattern while calling down. This usage pattern not only includes a record of what buffers are occupied, but also the intended use of these occupied buffers, especially if they can be modified in-place, and at which point they may be released and reused.
Note: this process outlined here and below is still an simplification. The actual implementation has some additional ,details to care for<<renderengine-RenderImplDetails>>

==== Example: calculating a 3 node chain
. Caller invokes calculation by pulling from exit node, providing the top-level StateProxy
. node1 (exit node) builds StateAdapter and calls retrieve() on it to get the desired output result
. this StateAdapter (ad1) knows he could get the result from Cache, so he tries, but it's a miss
. thus he pulls from the predecessor node2 according to the ,input descriptor<<renderengine-ProcNodeInputDescriptor>> of node1
. node2 builds its StateAdapter and calls retrieve()
. but because StateAdapter (ad2) is configured to directly forward the call down (no caching), it pulls from node3
. node3 builds its StateAdapter and calls retrieve()
. this StateAdapter (ad3) is configured to look into the Cache...
. this time producing a Cache hit
. now StateAdapter ad2 has input data, but needs a output buffer location, which re requests from its _parent state_ (ad1)
. and, because ad1 is configured for Caching and is 'in-place' capable, it's clear that this output buffer will be located within the cache
. thus the allocation request is forwarded to the cache, which provides a new 'slot'
. now node2 has both a valid input and a usable output buffer, thus the process function can be invoked
. and after the result has been rendered into the output buffer, the input is no longer needed
. and can be 'unlocked' in the Cache
. now the input data for node1 is available, and as node1 is in-place-capable, no further buffer allocation is necessary prior to calculating
. the finished result is now in the buffer (which happens to be also the input buffer and is actually located within the Cache)
. thus it can be marked as ready for the Cache, which may now provide it to other processes (but isn't allowed to overwrite it)
. finally, when the caller is done with the data, it signalles this to the top-level State object
. which forwards this information to the cache, which in turn may now do with the released Buffer as he sees fit.
image::..uml/fig132229.png[]

.See also
* the ,Entities involved in Rendering<<renderengine-RenderEntities>>
* additional ,implementation details<<renderengine-RenderImplDetails>>
* ,Memory management for render nodes<<renderengine-ManagementRenderNodes>>
* the protocol ,how to operate the nodes<<renderengine-NodeOperationProtocol>>



<div title="RenderProcess" modifier="Ichthyostega" modified="200806130009" created="200706190705" tags="Rendering dynamic" changecount="27">
For each segment (of the effective timeline), there is a Processor holding the exit node(s) of a processing network, which is a +Directed Acyclic Graph+ of small, preconfigured, stateless ,processing nodes<<renderengine-ProcNode>>. This network is operated according to the *pull principle*, meaning that the rendering is just initiated by 'pulling' output from the exit node, causing a cascade of recursive downcalls. Each node knows its predecessor(s) an can pull the necessary input from there. Consequently, there is no centralized 'engine object' which may invoke nodes iteratively or table driven -- rather, the rendering can be seen as a passive service provided for the backend, which may pull from the exit nodes at any time, in any order (?), and possibly multithreaded.
All State necessary for a given calculation process is encapsulated and accessible by a StateProxy object, which can be seen as the representation of 'the process'. At the same time, this proxy provides the buffers holding data to be processed and acts as a gateway to the backend to handle the communication with the Cache. In addition to this _top-level State,_ each calculation step includes a small ,state adapter object<<renderengine-StateAdapter>> (stack allocated), which is pre-configured by the builder and serves the purpose to isolate the processing function from the detals of buffer management.


.See also
* the ,Entities involved in Rendering<<renderengine-RenderEntities>>
* the ,mechanics of rendering and buffer management<<renderengine-RenderMechanics>>
* the protocol ,how to operate the nodes<<renderengine-NodeOperationProtocol>>



<div title="Rendering" modifier="Ichthyostega" modified="200806010307" created="200806010248" tags="def overview" changecount="3">
The rendering of input sources to the desired output ports happens within the >*Render Engine*<, which can be seen as a collaboration of Proc-Layer, Backend together with external/library code for the actual data manipulation. In preparation of the RenderProcess, the ,Builder>> as wired up a network of ,processing nodes<<renderengine-ProcNode>> called the *low-level model* (in contrast to the high-level model of objects placed within the EDL/Session). Generally, this network is a +Directed Acyclic Graph+ starting at the _exit nodes_ (output ports) and pointing down to the _source readers._ In Lumiera, rendering is organized according to the *pull principle*: when a specific frame of rendered data is requested from an exit node, a recursive calldown happens, as each node asks his predecessor(s) for the necessary input frame(s). This may include pulling frames from various input sources and for several time points, thus pull rendering is more powerful (but also more difficult to understand) than push rendering, where the process would start out with a given source frame.

Rendering can be seen as a passive service available to the Backend, which remains in charge what to render and when. Render processes may be running in parallel without any limitations. All of the storage and data management falls into the realm of the Backend. The render nodes themselves are *completely stateless* -- if some state is necessary for carrying out the calculations, the backend will provide a _state frame_ in addition to the data frames.



<div title="STypeManager" modifier="Ichthyostega" created="200809220230" changecount="1">
A facility allowing the Proc-Layer to work with abstracted ,media stream types<<renderengine-StreamType>>, linking (abstract or opaque) ,type tags<<renderengine-StreamTypeDescriptor>> to an ,library<<renderengine-MediaImplLib>>, which provides functionality for acutally dealing with data of this media stream type. Thus, the stream type manager is a kind of registry of all the external libraries which can be bridged and accessed by Lumiera (for working with media data, that is). The most basic set of libraries is instelled here automatically at application start, most notably the ,GAVL>> library for working with uncompressed video and audio data. _Later on, when plugins will introduce further external libraries, these need to be registered here too._



<div title="Session" modifier="Ichthyostega" modified="200904252215" created="200712100525" tags="def SessionLogic" changecount="4">
The Session contains all informations, state and objects to be edited by the User. From a users view, the Session is synonymous to the _current Project_. It can be saved and loaded. The individual Objects within the Session, i.e. Clips, Media, Effects, are contained in one (or several) collections within the Session, which we call ,EDL (Edit Decision List)<<renderengine-EDL>>.  -> ,Session design overview<<renderengine-SessionOverview>>

=== Session structure
The Session object is a singleton -- actually it is a »PImpl«-Facade object (because the actual implementation object can be swapped for (re)loading Sessions).<br/>The Session is comprised of
* a collection of *EDL* objects
* a *current EDL*, which can be switched and accessed [red]+TODO not sure if I keep this design...+
* a collection of *global Pipes*
* the *Fixture* with a possibility to ,(re)build it<<renderengine-PlanningBuildFixture>>



<div title="SessionDataMem" modifier="Ichthyostega" modified="200905082325" created="200904252303" tags="impl design SessionLogic draft" changecount="2">
The Layout of the Session datastructure in memory is bound by several dependencies: GUI, HighLevelModel, SessionStorage, ,Builder>>.
Currently as of 5/09, this is an ongoing ,implementation and planning effort<<renderengine-PlanningSessionInMem>>

[red]+TODO...+



<div title="SessionInterface" modifier="Ichthyostega" modified="200906071959" created="200904242108" tags="SessionLogic GuiIntegration design draft discuss" changecount="7">
+Session Interface+, when used in a more general sense, denotes a compound of several interfaces and facilities, together forming the primary access point to the user visible contents and state of the editing project.
* the API of the session class
* the accompanying management interface (SessionManager API)
* an LayerSeparationInterfaces allowing to access these interfaces from outside the Proc-Layer
* the primary public APIs exposed on the objects to be queried and retrieved via the session class API
** Sequence
** Placement
** Clip
** Track
** Effect
** Automation
* the ,command<<renderengine-CommandHandling>> interface, including the ,undo<<renderengine-UndoManager>> facility

[red]+WIP+



<div title="SessionLogic" modifier="Ichthyostega" modified="200906072005" created="200904242110" tags="overview" changecount="13">
The Session contains all informations, state and objects to be edited by the User ( ->,def<<renderengine-Session>>).
As such, the SessionInterface is the main entrance point to Proc-Layer functionality, both for the primary EditingOperations and for playback/rendering processes.

Currently (as of 5/09), Ichthyo is ,targeting<<renderengine-PlanningSessionInMem>> a first preliminary implementation of the ,Session in Memory<<renderengine-SessionDataMem>>

=== Design and handling of Objects within the Session
Objects are attached and manipulated by ,placements<<renderengine-Placement>>; thus the organisation of these placements is part of the session data layout. Effectively, such a placement within the session behaves like an _instances_ of a given object, and at the same time it defines the 'non-substantial' properties of the object, e.g. its positions and relations. ,References<<renderengine-MObjectRef>> to these placement entries are handed out as parameters, both down to the ,Builder>> and from there to the render processes within the engine, but also to external parts within the GUI and in plugins. The actual implementation of these object references is built on top of the PlacementRef tags, thus relying on the PlacementIndex the session maintains to keep track of all placements and their relations. While -- using these references -- an external client can access the objects and structures within the session, any actual *mutations* should be done based on the CommandHandling: a single operation of a sequence of operations is defined as ,Command>>, to be ,dispatched as mutation operation<<renderengine-ProcDispatcher>>. Following this policy ensures integration with theSessionStorage and provides (unlimited) ,UNDO<<renderengine-UndoManager>>.



<div title="SessionOverview" modifier="Ichthyostega" modified="200811011805" created="200709272105" tags="design img" changecount="29">
<<renderengine-<
[red]+WARNING: Naming is currently being discussed (11/08)+
* ,EDL>> probably will be called *Sequence* (or maybe *Arrangement*)
* ,Session>> maybe renamed to *Project*
* there seems to be a new entity called ,Timeline>> which holds the global Pipes
<<renderengine-<
The ,Session>> (sometimes also called _Project_ ) contains all informations and objects to be edited by the User. It can be saved and loaded. The individual Objects within the Session, i.e. Clips, Media, Effects, are contained in one (or several) collections within the Session, which we call ,EDL (Edit Decision List)<<renderengine-EDL>>. Moreover, the sesion contains references to all the Media files used, and it contains various default or user defined configuration, all being represented as ,Asset>>. At any given time, there is _only one current session_ opened within the application.
The Session is close to what is visible in the GUI. From a user's perspective, you'll find a ,Timeline>>-like structure, containing an ,EDL (or Sequence)<<renderengine-EDL>>, where various Media Objects are arranged and placed. The available building blocks and the rules how they can be combined together form Lumiera's ,high-level data model<<renderengine-HighLevelModel>>. Basically, besides the ,media objects<<renderengine-MObjects>> there are data connections and all processing is organized around processing chains or ,pipes<<renderengine-Pipe>>, which can be either global (in the Session) or local (in real or virtual clips)

===== larger projects
For larger editing projects the simple structure of a session containing _the_ timeline is not sufficient. Rather
* we may have several ,EDLs (=Sequences)<<renderengine-EDL>>, e.g. one for each scene. These sequences can be even layered or nested (compositional work).
* within one project, there may be multiple, _independant Timelines_ -- each of which may have an associated Viewer or Monitor
To support these cases without making the default case more complicated, Lumiera introduces a _focus_ for selecting the _current EDL,_ which will receive all editing operations. ,Timelines<<renderengine-Timeline>> on the other hand are always top-level objects and can't be combined further. You can render a given timeline to output.
-> see ,Relation of Project, Timelines and Sequences<<renderengine-TimelineSequences>>

===== the definitive state
With all the structural complexities possible within such a session, we need an isolation layer to provide 'one' definitive state where all configuration has been made explicit. Thus the session manages one special object list, the ,Fixture>>, which can be seen as all currently active objects placed onto a single timeline.

===== organisational devices
The possibility of having multiple EDLs helps organizing larger projects. Each ,EDL>> is just a logical grouping; because all effective properties of any MObject within this EDL are defined by the MObject itself and the ,Placement>>, by which the object is anchored to some time point, some track, can be connected to some pipe, or linked to another object. In a similar manner, ,Tracks<<renderengine-Track>> are just another organisational aid for grouping objects, disabling them and defining common output pipes.

===== global pipes
image:../draw/Proc.builder1.png[] Any session should contain a number of global ,(destination) pipes<<renderengine-Pipe>>, typically video out and audio out. The goal is, to get any content producing or transforming object in some way connected to one of these outputs, either _by ,placing<<renderengine-Placement>> it directly_ to some pipe, or by _placing it to a track_ and having the track refer to some pipe. Besides the global destination pipes, we can use internal pipes to form busses or subgroups, either on a global (session) level, or by using the processing pipe within a ,virtual clip<<renderengine-VirtualClip>>, which can be placed freely within the EDL(s). Normally, pipes just gather and mix data, but of course any pipe can have an attached effect chain. ( -> see ,more on Tracks and Pipes within the EDL<<renderengine-TrackPipeEDL>>)

===== default configuration
While all these possibilities may seem daunting, there is a simple default configuration loaded into any pristine new session:
It will contain a global video and audio out pipe, just one EDL with a single track; this track will have a internal video and audio pipe (bus) configured with one fading device sending to the global output ports. So, by adding some clip with a simple absolute placement to this track and some time position, the clip gets connected and rendered, after ,(re)building<<renderengine-PlanningBuildFixture>> the ,Fixture>> and passing the result to the ,Builder>> -- and using the resulting render nodes network (Render Engine).

-> ,anatomy of the high-level model<<renderengine-HighLevelModel>>
-> considerations regarding ,Tracks and Pipes within the EDL<<renderengine-TrackPipeEDL>>
-> see ,Relation of Project, Timelines and Sequences<<renderengine-TimelineSequences>>



<div title="SiteSubtitle" modifier="Ichthyostega" modified="200802030406" created="200706190044" changecount="2">
Building a Render Nodes Network from Objects in the EDL
<div title="SiteTitle" modifier="Ichthyostega" modified="200708080212" created="200706190042" changecount="2">
Engine


<div title="StateAdapter" modifier="Ichthyostega" modified="200807132344" created="200806261912" tags="Rendering impl def" changecount="10">
A small (in terms of storage) and specifically configured StateProxy object which is created on the stack for each individual +pull()+ call. It is part of the invocation state of such a call and participates in the buffer management. Thus, in a calldown sequence of +pull()+ calls we get a corresponding sequence of 'parent' states. At each level, the  -> WiringDescriptor of the respective node defines a Strategy how the call is passed on.



<div title="StateProxy" modifier="Ichthyostega" modified="200806010404" created="200706220352" tags="def" changecount="4">
An Object representing a _Render Process_ and containing associated state information.
* it is created in the Controller subsystem while initiating the BuildProcess
* it is passed on to the generated Render Engine, which in turn passes it down to the individual Processors
* moreover, it contains methods to communicate with other state relevant parts of the system, thereby shielding the rendering code from any complexities of Thread communication if necessary. (thus the name Proxy)
* in a future version, it may also encapsulate the communication in a distributed render farm



<div title="StreamConversion" modifier="Ichthyostega" modified="200810060308" created="200810020337" tags="design spec" changecount="4">
Conversion of a media stream into a stream of another type is done by a processor module (plugin). The problem of finding such a module is closely related to the StreamType and especially ,problems of querying<<renderengine-StreamTypeQuery>> for such. (The builder uses a special Facade, the ConManager, to access this functionality). There can be different kinds of conversions, and the existance or non-existance of such an conversion can influence the stream type classification.
* different _kinds of media_ can be *transformed* into each other
* stream types _subsumed_ by a given prototype should be *lossless convertible* and thus can be considered _equivalent._
* besides, between different stream _implementation types,_ there can be a *rendering* (lossy conversion) -- or no conversion at all.



<div title="StreamPrototype" modifier="Ichthyostega" modified="200809120021" created="200808152042" tags="def spec" changecount="10">
The stream Prototype is part of the specification of a media stream's type. It is a semantic (or problem domain oriented) concept and should be distinguished from the actual implementation type of the media stream. The latter is provided by an ,library implementation<<renderengine-StreamTypeImplFacade>>. While there are some common predefined prototypes, mostly, they are defined within the concrete ,Session>> according to the user's needs. 

Prototypes form an open (extensible) collection, though each prototype belongs to a specific media kind (+VIDEO, IMAGE, AUDIO, MIDI,...+).
The *distinguishing property* of a stream prototype is that any ,Pipe>> can process _streams of a specific prototype only._ Thus, two streams with different prototype can be considered 'something quite different' from the users point of view, while two streams belonging to the same prototype can be considered equivalent (and will be converted automatically when their implementation types differ). Note this definition is _deliberately fuzzy,_ because it depends on the actual situation of the project in question.

Consequently, as we can't get away with an fixed Enum of all stream prototypes, the implementation must rely on a query interface. The intention is to provide a basic set of rules for deciding queries about the most common stream prototypes; besides, a specific session may inject additional rules or utilize a completely different knowledge base. Thus, for a given StreamTypeDescriptor specifying a prototype
* we can get a ,default<<renderengine-DefaultsManagement>> implementation type
* we can get a default prototype to a given implementation type by a similar query
* we can query if a implementation type in question can be _subsumed_ by this prototype
* we can determine if another prototype is _convertible_

==== Examples
NTSC and PAL video, video versus digitized film, HD video versus SD video, 3D versus flat video, cinemascope versus 4:3, stereophonic versus monaural, periphonic versus panoramic sound, Ambisonics versus 5.1, dolby versus linear PCM...




<div title="StreamType" modifier="Ichthyostega" modified="200908302143" created="200808060244" tags="spec discuss draft" changecount="11">
_how to classify and describe media streams_
Media data is supposed to appear structured as stream(s) over time. While there may be an inherent internal structuring, at a given perspective *any stream is a unit and homogeneous*. In the context of digital media data processing, streams are always *quantized*, which means they appear as a temporal sequence of data chunks called *frames*.

===  Terminology
* 'Media' is comprised of a set of streams or channels
* 'Stream' denotes a homogeneous flow of media data of a single kind
* 'Channel' denotes a elementary stream, which can't be further separated in the given context
* all of these are delivered and processed in a smallest unit called 'Frame'. Each frame corresponds to a _time interval._
* a 'Buffer' is a data structure capable of holding a Frame of media data.
* the 'Stream-Type' describes the kind of media data contained in the stream

===  Problem of Stream Type Description
Media types vary largely and exhibit a large number of different properties, which can't be subsumed under a single classification scheme. But on the other hand, we want to deal with media objects in a uniform and generic manner, because generally all kinds of media behave somewhat similar. But the problem is, these similarities disappear when describing media with logical precision. Thus we are forced into specialized handling and operations for each kind of media, while we want to implement a generic handling concept.

===  Stream Type handling in the Proc-Layer
====  Identification
A stream type is denoted by a StreamTypeID, which is an identifier, acting as an unique key for accessing information related to the stream type. It corresponds to an StreamTypeDescriptor record, containing an -- _not necessarily complete_ -- specification of the stream type, according to the classification detailed below.

====  Classification
Within the Proc-Layer, media streams are treated largely in a similar manner. But, looking closer, note everything can be connected together, while on the other hand there may be some classes of media streams which can be considered _equivalent_ in most respects. Thus, it seems reasonable to separate the distinction between various media streams into several levels
* Each media belongs to a fundamental 'kind' of media, examples being +Video+, +Image+, +Audio+, +MIDI+,... Media streams of different kind can be considered somewhat 'completely separate' -- just the handling of each of those media kinds follows a common _generic pattern_ augmented with specialisations. Basically, it is _impossible to connect_ media streams of different kind. Under some circumstances there may be the possibility of a _transformation_ though. For example, a still image can be incorporated into video, sound may be visualized, MIDI may control a sound synthesizer.
* Below the level of distinct kinds of media streams, within every kind we have an open ended collection of *prototypes*, which, when compared directly, may each be quite distinct and different, but which may be _rendered_ into each other. For example, we have stereoscopic (3D) video and we have the common flat video lacking depth information, we have several spatial audio systems (Ambisonics, Wave Field Synthesis), we have panorama simulating sound systems (5.1, 7.1,...), we have common stereophonic and monaural audio. It is considered important to retain some openness and configurability within this level of distinction, which means this classification should better be done by rules then by setting up a fixed property table. For example, it may be desirable for some production to distinguish between digitized film and video NTSC and PAL, while in another production everything is just 'video' and can be converted automatically. The most noticeable consequence of such a distinction is that any Bus or ,Pipe>> is always limited to a media stream of a single prototype. ( -> ,more<<renderengine-StreamPrototype>>)
* Besides the distinction by prototypes, there are the various media *implementation types*. This classification is not necessarily hierarchically related to the prototype classification, while in practice commonly there will be some sort of dependency. For example, both stereophonic and monaural audio may be implemented as 96kHz 24bit PCM with just a different number of channel streams, but we may as well get a dedicated stereo audio stream with two channels multiplexed into a single stream. For dealing with media streams of various implementation type, we need _library_ routines, which also yield a _type classification system._ Most notably, for raw sound and video data we use the ,GAVL>> library, which defines a classification system for buffers and streams.
* Besides the type classification detailed thus far, we introduce an *intention tag*. This is a synthetic classification owned by Lumiera and used for internal wiring decisions. Currently (8/08), we recognize the following intention tags: +Source+, +Raw+, +Intermediary+ and +Target+. Only media streams tagged as +Raw+ can be processed.

====  Media handling requirements involving stream type classification
* set up a buffer and be able to create/retrieve frames of media data.
* determine if a given media data source and sink can be connected, and how.
* determine and enumerate the internal structure of a stream.
* discover processing facilities
-> see StreamTypeUse
-> ,querying types<<renderengine-StreamTypeQuery>>




<div title="StreamTypeDescriptor" modifier="Ichthyostega" modified="200809130314" created="200808151505" tags="def" changecount="7">
A description and classification record usable to find out about the properties of a media stream. The stream type descriptor can be accessed using an unique StreamTypeID. The information contained in this descriptor record can intentionally be _incomplete,_ in which case the descriptor captures a class of matching media stream types. The following information is maintained:
* fundamental *kind* of media: +VIDEO, IMAGE, AUDIO, MIDI,...+
* stream *prototype*: this is the abstract high level media type, like NTSC, PAL, Film, 3D, Ambisonics, 5.1, monaural,...
* stream *implementation type* accessible by virtue of an StreamTypeImplFacade
* the *intended usage category* of this stream: +SOURCE, RAW, INTERMEDIARY, TARGET+.
-> see >,Stream Type<<renderengine-StreamType>>< detailed specification
-> notes about ,using stream types<<renderengine-StreamTypeUse>>
-> more ,about prototypes<<renderengine-StreamPrototype>>



<div title="StreamTypeID" modifier="Ichthyostega" created="200808151510" tags="def" changecount="1">
This ID is an symbolic key linked to a StreamTypeDescriptor. The predicate +stream(ID)+ specifies a media stream with the StreamType as detailed by the corresponding descriptor (which may contain complete or partial data defining the type).



<div title="StreamTypeImplConstraint" modifier="Ichthyostega" modified="200810020230" created="200809220248" tags="def" changecount="2">
A special kind of media stream ,implementation type<<renderengine-StreamTypeImplFacade>>, which is not fully specified. As such, it is supposed there _actually is_ an concrete implementation type, while only caring for some part or detail of this implementation to exhibit a specific property. For example, using an type constraint we can express the requirement of the actual implementation of a video stream to be based on RGB-float, or to enforce a fixed frame size in pixels.

An implementation constraint can _stand-in_ for a completely specified implementation type (meaning it's a sub interface of the latter). But actually using it in this way may cause a call to the ,defaults manager<<renderengine-DefaultsImplementation>> to fill in any missing information. An example would be to call +createFrame()+ on the type constraint object, which means being able to allocate memory to hold a data frame, with properties in compliance with the given type constraint. Of cousre, then we need to know all the properties of this stream type, which is where the defaults manager is queried. This allows session customisation to kick in, but may fail under certain cicumstances.



<div title="StreamTypeImplFacade" modifier="Ichthyostega" modified="200809251940" created="200808151520" tags="def" changecount="7">
Common interface for dealing with the implementation of media stream data. From a high level perspective, the various kinds of media (+VIDEO, IMAGE, AUDIO, MIDI,...+) exhibit similar behaviour, while on the implementation level not even the common classification can be settled down to a complete general and useful scheme. Thus, we need separate library implementations for deailing with the various sorts of media data, all providing at least a set of basic operations:
* set up a buffer
* create or accept a frame
* get an tag describing the precise implementation type
* ...?

-> see also >,Stream Type<<renderengine-StreamType>><
_Note:_ there is a sort-of 'degraded' variant just requiring some  -> ,implementation constraint<<renderengine-StreamTypeImplConstraint>> to hold




<div title="StreamTypeQuery" modifier="Ichthyostega" modified="200810060525" created="200809280129" tags="spec draft" changecount="23">
Querying for media stream type information comes in various flavours
* you may want to find a structural object (pipe, output, processing patten) associated with / able to deal with a certain stream type
* you may need a StreamTypeDescriptor for an existing stream given as implementation data
* you may want to build or complete type information from partial specification.
Mostly, those queries involve the ConfigRules system in some way or the other. The ,prototype-<<renderengine-StreamPrototype>> and ,implementation type<<renderengine-StreamTypeImplFacade>>-interfaces themselves are mostly a facade for issuing appropriate queries. Some objects (especially ,pipes<<renderengine-Pipe>>) are tied to a certain stream type and thus store a direct link to type information. Others are just associated with a type by virtue of the DefaultsManagement.

The _problem_ with this pivotal role of the config rules is that -- from a design perspective -- not much can be said specifically, besides _'you may be able to find out...', '...depends on the defaults and the session configuration'._ This way, a good deal of crucial behaviour is pushed out of the core implementation (and it's quite intentionally being done this way). What can be done regarding the design of the core is mostly to setup a framework for the rules and determine possible *query situations*.

=== the kind of media
the information of the fundamental media kind (video, audio, text, MIDI,...) is assiciated with the prototype, for technical reasons. Prototype information is mandatory for each StreamType, and the impl facade provides a query function (because some implementation libraries, e.g. ,GAVL>>, support multiple kinds of media).

=== query for a prototype
'Situation': given an implementation type, find a prototype to subsume it.
Required only for building a complete StreamType which isn't known at this point.
The general case of this query is _quite hairy,_ because the solution is not necessary clear and unique. And, worse still, it is related to the semantics, requiring semantic information and tagging to be maintained somewhere. For example, while the computer can't 'know' what stereopohinc audio is (only a human can by listening to a stereophoic playback and deciding if it actually does convey a spatical sound image), in most cases we can overcome this problem by using the _heuristical rule_ of assuming the prototype 'stereophonic' when given two identically typed audio channels. This example also shows the necessity of ordering heuristic rules to be able to pick a best fit.

We can inject two different kinds of fallback solutions for this kind of query:
* we can always build a 'catch-all' prototype just based on the kind of media (e.g. +prototype(video).+). This should match with lowest priority
* we can search for existing StreamTypes with the same impl type, or an impl type which is _equivalent convertible_ (see  -> StreamConversion). 
The latter case can yield multiple solutions, which isn't any problem, because the match is limited to classes of equivalent stream implementation, which would be subsumed under the same prototype anyway. Even if the registry holds different prototypes linked to the same implementation type, they would be convertible and thus could _stand-in_ for one another. Together this results in the implementation
. try to get a direct match to an existing impl type which has an associated (complete) StreamType, thus bypassing the ConfigRules system altogether
. run a +Query<Prototype>+ for the given implementation type
. do the search within equivalence class as described above
. fall back to the media kind.
[red]+TODO: how to deal with the problem of hijacking a prototype?+  -> see ,here<<renderengine-StreamTypeUse>>

=== query for an implementation
'Situation 1': given an partially specified StreamType (just an ,constraint<<renderengine-StreamTypeImplConstraint>>)
'Situation 2': find an implementation for a given prototype (without any further impl type guidlines)
Both cases have to go though the ,defaults manager<<renderengine-DefaultsManagement>> in some way, in order to give any default configuration a chance to kick in. This is _one of the most important use cases_ of the defaults system: the ability to configure a default fromat for all streams with certain semantic classification. +prototype(video)+ by default is RGBA 24bit non-interlaced for example.
But after having queried the defaults system, there remains the problem to build a new solution (which will then automatically become default for this case). To be more precise: invoking the defaults system (as implemented in Lumiera) means first searching through existing objects encountered as default, and then issuing an general query with the capabilities in question. This general query in turn is conducted by the query type handler and usually consists of first searching existing objects and then creating a new object to match the capabilities. But, as said, the details depend on the type (and are defined by the query handler installed for this type). Translated to our problem here in question, this means _we have to define the basic operations from which a type query handler can be built._ Thus, to start with, it's completely sufficient to wire a call to the DefaultsManagement and assume the current session configuration contains some rules to cover it. Plus being prepared for the query to fail (throw, that is).

Later on this could be augmented by providing some search mechanisms:
* search through existing stream type implementations (or a suitable pre filtered selection) and narrow down the possible result(s) by using the constraint as a filter. Obviously this requires support by the MediaImplLib facade for the implementation in question. (This covers 'Situation 1')
* relate a protoype in question to the other existing prototypes and use the convertibility / subsumption as a filter mechanism. Finally pick an existing impl type which is linked to one of the prototypes found thus far.
Essentially, we need a search mechanism for impltypes and prototypes. This search mechanism is best defined by rules itself, but needs some primitive operations on types, like ennumerating all registered types, filter those selections and match against a constraint.

=== query for an (complete) StreamType
All situations discussed thus far can also occur wrapped into and triggered by a query for a complete type. Depending on what part is known, the missing bits will be queried. 
Independent from these is 'another Situation' where we query for a type *by ID*.
* a simple symbolic ID can be found by searching through all existing stream types (Operation supported by the type registry within STypeManager)
* a special *classificating* ID can be parsed into the components (media kind, prototype, impltype), resulting in sub searches for these.
[red]+not sure if we want to support queries by symboic ID+...problem is the impl type, because probably the library needs to support describing any implementation type by a string. Seemingly GAVL does, but requiring it for every lib?



<div title="StreamTypeUse" modifier="Ichthyostega" modified="200809280320" created="200809130312" tags="draft discuss dynamic" changecount="21">
Questions regarding the use of StreamType within the Proc-Layer.
* what is the relation between Buffer and Frame?
* how to get the required size of a Buffer?
* who does buffer allocations and how?

Mostly, stream types are used for querying, either to decide if they can be connected, or to find usable processing modules.
Even building a stream type from partial information involves some sort of query.
-> more on ,media stream type queries<<renderengine-StreamTypeQuery>>

=== creating stream types
seemingly stream types are created based on an already existing media stream (or a Frame of media data?). [red]+really?+
The other use case seems to be that of an _incomplete_ stream type based on a ,Prototype<<renderengine-StreamPrototype>>

=== Prototype
According to my current understanding, a prototype is merely a classification entity. But then -- how to bootstrap a Prototype?
And how to do the classification of an existing implementation type.

Besides, there is the problem of _hijacking a prototype:_ when a specific implementation type gets tied to a rather generic protoype, like +protoype(video)+, how to comply to the rule of prototypes subsuming a class of equivalent implementations?

=== Defaults and partial specification
A StreamType need not be completely defined. It is sufficient to specify the media kind and the Prototype. The implementation type may be just given as a constraint, thus defining some properties and leaving out others. When creating a frame buffer based upon such an _incomplete type,_ ,defaults<<renderengine-DefaultsManagement>> are queried to fill in the missing parts.
Constraints are objects provided by the Lumiera core, but specialized to the internals of the actual implementation library.
For example there might be a constraint implementation to force a specific +gavl_pixelformat_t+.

=== the ID problem
Basically I'd prefer the IDs to be real identifiers. So they can be used directly within rules. At least the Prototypes _can_ have such a textual identifier. But the implementation type is problematic, and consequently the ID of the StreamType as well. Because the actual implementation should not be nailed down to a fixed set of possibilities. And, generally, we can't expect an implementation library to yield textual identifiers for each implementation type. _Is this really a problem? [red]+what are the use cases?+_
As far as I can see, in most cases this is no problem, as the type can be retrieved or derived from an existing media object. Thus, the only problematic case is when we need to persist the type information without being able to guarantee the persistence of the media object this type was derived from. For example this might be a problem when working with proxy media. But at least we should be able to create a constraint (partial type specification) to cover the important part of the type information, i.e. the part which is needed to re-create the model even when the original media isn't there any longer.
Thus, _constraints may be viewed as type constructing functors._

--------------
=== use cases
* pulling data from a media file
* connecting pipes and similar wiring problems
* describing the properties of an processor plugin

====  pulling data from a media file
To open the file, we need _type discovery code,_ resulting in a handle to some library module for accessing the contents, which is in compliance with the Lumiera application. Thus, we can determine the possible return values of this type discovery code and provide code which wires up a corresponding StreamTypeImplFacade. Further, the +control::STypeManager+ has the ability to build a complete or partial StreamType from
* an ImplFacade
* a Prototype
* maybe even from some generic textual IDs?
Together this allows to associate a StreamType to each media source, and thus to derive the Prototype governing the immediately connected ,Pipe>>
A pipe can by design handle data of one Prototype solely.

====  wiring problems
When deciding if a connection can be made, we can build up the type information starting out from the source. (this requires some work, but it's _possible,_ generally speaking.). Thus, we can allways get an ImplType for the 'lower end' of the connection, and at least a Prototype for the 'output side' -- which should be enough to use the query functions provided by the stream type interfaces

====  describing properties
[red]+currently difficult to define+ as of 9/2008, because the property description of plugins is not planned yet.
My Idea was to use ,type implementation constraints<<renderengine-StreamTypeImplConstraint>> for this, which are a special kind of ImplType




<div title="StrongSeparation" modifier="Ichthyostega" modified="200907220311" created="200706220452" tags="design" changecount="6">
This design lays great emphasis on separating all those components and subsystems, which are considered not to have a _natural link_ of their underlying concepts. This often means putting some additional constraints on the implementation, so basically we need to rely on the actual implementation to live up to this goal. In many cases it may seem to be more natural to 'just access the necessary information'. But on the long run this coupling of not-directly related components makes the whole codebase monolithic and introduces lots of _accidental complexity._

Instead, we should try to just connect the various subsystems via Interfaces and -- instead of just using some information, rather use some service to be located on an Interface to query other components for this information. The best approach of course is always to avoid the dependency altogether.

=== Examples
* There is a separation between the 'high level ,EDL>> view' and the ,Fixture>>: the latter only accesses the MObjects and the Placement Interfaces.
* same holds true for the Builder: it just uses the same Interfaces. The actual coupling is done rather _by type_, i.e. the Builder relies on several types of MObjects to exist and treats them via overloaded methods. He doesn't rely on a actual object structure layout in the EDL besides the requirement of having a ,Playlist>>
* the Builder itself is a separation layer. Neither do the Objects in the EDL access directly ,Render Nodes<<renderengine-ProcNode>>, nor do the latter call back into the EDL. Both connections seem to be necessary at first sight, but both can be avoided by using the Builder Pattern
* another separation exists between the Render Engine and the individual Nodes: The Render Engine doesn't need to know the details of the data types processed by the Nodes. It relies on the Builder having done the correct connections and just pulls out the calculated results. If there needs to be additional control information to be passed, then I would prefer to do a direct wiring of separate control connections to specialized components, which in turn could instruct the controller to change the rendering process.
* to shield the rendering code of all complexities of thread communication and synchronization, we use the StateProxy




<div title="StructAsset" modifier="Ichthyostega" modified="200906071813" created="200709221353" tags="def classes img" changecount="9">
Structural Assets are intended mainly for internal use, but the user should be able to see and query them. They are not 'loaded' or 'created' direcly, rather they _leap into existance _ by creating or extending some other structures in the EDL/Session, hence the name. Some of the structural Asset parametrisation can be modified to control of some aspects of the Proc Layer's (default) behaviour.
* ,Processing Patterns<<renderengine-ProcPatt>> encode information how to set up some parts of the render network to be created automatically: for example, when building a clip, we use the processing pattern how to decode and preprocess the actual media data.
* ,Tracks<<renderengine-Track>> are one of the dimensions used for organizing the EDL. They serve as an Anchor to attach parametrisation of output pipe, overlay mode etc. By ,placing<<renderengine-Placement>> to a track, some media object inherits placement properties from this track.
* ,Pipes<<renderengine-Pipe>> form -- at least as visible to the user -- the basic building block of the render network, because the latter appears to be a collection of interconnected processing pipelines. (this is the _outward view; _ in fact the render network consists of ,nodes<<renderengine-ProcNode>> and is ,built<<renderengine-Builder>> from the Pipes, clips, effects...)
image:../Asset Classess../uml/fig131205.png[]

=== naming scheme
The Asset name field of structural Assets utilizes a special naming scheme, which allows to derive the name based on the capabilities of the structural asset. For example, by default all media clips with a given media stream type (e.g. H264) will use the same ,processing Pattern<<renderengine-ProcPatt>> for rendering. [red]+todo: work out the details of this naming scheme??+

=== querying
Structural assets can be queried by specifying the specific type (Pipe, Track, ProcPatt) and a query goal, which means in the current implementation just querying for some predicate defined with the structural asset. For example, you can +Query<Pipe> (+stream(mpeg)+)+, yieliding the first pipe found which declares to have stream type +mpeg+



<div title="TickService" modifier="Ichthyostega" created="200902080629" tags="def" changecount="1">
A service generating _periodic ticks_ -- repetedly invoking a callback with a given frequency.
* defined 1/2009 as part of the PlayerDummy (design sketch)
* probably to be implemented later on based on Posix timers
* will probably be later on integrated into a synchronisation framework (display sync, audio sync, MTC master/slave...)



<div title="Timeline" modifier="Ichthyostega" modified="200811022215" created="200706250721" tags="def" changecount="6">
Timeline is the top level element within the ,Session (Project)<<renderengine-Session>>. It is visible within a _timeline view_ in the GUI and represents the effective (resulting) arrangement of media objects, to be rendered for output or viewed in a Monitor (viewer window). A timeline is comprised of:
* a time axis in abolute time (WIP: not clear if this is an entity or just a conceptual definition) 
* a PlayControler
* a list of global Pipes representing the possible outputs (master busses)
* _exactly one_ top-level ,EDL (Sequence)<<renderengine-EDL>>, which in turn may contain further nested EDLs (Sequences).
Please note especially that following this design _a timeline doesn't define tracks._ ,Tracks form a Tree<<renderengine-Track>> and are part of the individual EDLs (Sequences), together with the media objects placed to these tracks.

Within the Project, there may be *multiple timelines*, to be viewed and rendered independently. But, being the top-level entities, multiple timelines may not be combined further. You can always just render (or view) one specific timeline. A given sequence may be referred directly or indirectly from multiple timelines though.

Note: in early drafts of the design (2007) there was an entity called +Timeline+ within the ,Fixture>>. This entity seems superfluous and has been dropped. It never got any relevance in existing code and at most was used in some code comments.



<div title="TimelineSequences" modifier="Ichthyostega" modified="200811022211" created="200811011836" tags="design draft discuss img" changecount="14">
There is a three-level hierarchy: ,Project<<renderengine-Session>>, ,Timeline>>, ,Sequence<<renderengine-EDL>>. Each project can contain *multiple timelines*, to be viewed and rendered independently. But, being the top-level entities, these timelines may not be combined further. You can always just render (or view) one specific timeline. Each of those timelines refers to a Sequence, which is a bunch of ,media objects<<renderengine-MObject>> placed to a tree of ,tracks<<renderengine-Track>>. Of course it is possible to use Sub-EDLs (Sub-sequences) within the top-level sequence within a timeline to organize a movie into several scenes or chapters. 

image:../uml/fig132741.png[Relation of Timelines, Sequences and MObjects within the Project]
As stated in the ,definition<<renderengine-Timeline>>, a timeline refers to exactly one EDL (Sequence), and the latter defines a tree of ,tracks<<renderengine-Track>> and a bunch of media objects placed to these tracks. A Sequence may optionally also contain nested sequences as ,meta-clips<<renderengine-VirtualClip>>. Moreover, obviously several timelines (top-level entities) may refer to the same Sequence without problems.
This is because the top-level entities (Timelines) are not permitted to be combined further. You may play or render a given timeline, you may even play several timelines simultaneously in different monitor windows, and these different timelines may incorporate the same sequence in a different way. The Sequence just defines the relations between some objects and may be placed relatively to another object (clip, label,...) or similar reference point, or even anchored at an absolute time if desired. In a similar open fashion, within the track-tree of a sequence, we may define a specific signal routing, or we may just fall back to automatic output wiring.

=== Attaching output
The Timeline owns a list of global ,pipes (busses)<<renderengine-Pipe>> which are used to collect output. If the track tree of a sequence doesn't contain specific routing advice, then connections will be done directly to these global pipes in order and by matching StreamType (i.e. typically video to video master, audio to stereo audio master). When a monitor (viewer window) is attached to this timeline, similar output connections are made from those global pipes, i.e. the video display will take the contents of the first video (master) bus, and the first stereo audio pipe will be pulled and sent to system audio out. The timeline owns a *play control* shared by all attached viewers and coordinating the rendering-for-viewing. Similarly, a render task may be attached to the timeline to pull the pipes needed for a given kind of generated output. The actual implementation of the play controller and the coordination of render tasks is located in the Backend, which uses the service of the Proc-Layer to pull the respective exit nodes of the render engine network.

=== Timeline versus Timeline View
Actually, what the GUI creates and uses is the _view_ of a given timeline. This makes no difference to start with, as the view is modelled to be a sub-concept of +timeline+ and thus can stand-in. All different views of the _same_ timeline also share one single play control instance, i.e. they all have one single playhead position. Doing it this way should be the default, because it's the least confusing. Anyway, it's also possible to create multiple _independent timelines_ -- in an extreme case even so when referring to the same top-level sequence. This configuration gives the ability to play the same arrangement in parallel with multiple independent play controllers (and thus independent playhead positions)

To complement this possibilities, I'd propose to give the _timeline view_ the possibility to be re-linked to a sub-sequence. This way, it would stay connected to the main play control, but at the same time show a sub-sequence _in the way it will be treated as  embedded_ within the top-level sequence. This would be the default operation mode when a meta-clip is opened (and showed in a separate tab with such a linked timeline view). The reason for this proposed handling is again to give the user the least surprising behaviour. Because, when -- on the contrary -- the sub-sequence would be opened as _separate timeline,_ a different absolute time position and a different signal routing may result; doing such should be reserved for advanced use, e.g. when multiple editors cooperate on a single project and a sequence has to be prepared in isolation prior to being integrated in the global sequence (featuring the whole movie).



<div title="Track" modifier="Ichthyostega" modified="200811011843" created="200801062320" tags="def design decision" changecount="5">
Tracks are just a structure used to organize the Media Objects within the EDL. Tracks are associated allways to a specific EDL and the Tracks of an EDL form a _tree._ They can be considered to be an organizing grid, and besides that, they have no special meaning. They are grouping devices, not first-class entities. A track doesn't _have_ a port or pipe or _is_ a video track and the like; it can be configured to behave in such manner by using placements.

The Track-IDs are assets on their own, but they can be found within a given EDL. So, several EDLs can share a single track or each EDL can have tracks with their own, separate identity. (the latter is the default)
* Like most MObjects, tracks have a asset view: you can find a track asset (a track ID) in the asset manager.
* and they have an object view: there is an track MObject which can be ,placed<<renderengine-Placement>>, thus defining properties of this track within one EDL, e.g. the starting point in time
Of course, we can place other MObjects relative to some track (that's the main reason why we want to have tracks). In this sense, the ,handling of Tracks<<renderengine-TrackHandling>> is somewhat special: the placement of some track can be found directly within the EDL (and not within the general collection of placed objects which form the contents of any EDL). This placement defines properties of the track, which will be inherited if necessary by all MObjects placed to this track. For example, if placing (=plugging) a track to some global ,Pipe>>, and if placing a clip to this track, without placing the clip directly to another pipe, the associated-to-pipe information of the track will be fetched by the builder when needed to make the output connection of the clip.
-> ,Handling of Tracks<<renderengine-TrackHandling>>
-> ,Handling of Pipes<<renderengine-PipeHandling>>

-> ,Anatomy of the high-level model<<renderengine-HighLevelModel>>



<div title="TrackHandling" modifier="Ichthyostega" modified="200804110259" created="200804110013" tags="spec" changecount="23">
What _exactly_ is denoted by >Track< -- _basically_ a working area to group media objects placed at this track at various time positions -- varies depending on context:
* viewed as ,structural asset<<renderengine-StructAsset>>, tracks are nothing but global identifiers (possibly with attached tags and description)
* regarding the structure _within each ,EDL>>,_ tracks form a tree-like grid, the individual track being attached to this tree by a ,Placement>>, thus setting up properties of placement (time reference origin, output connection, layer, pan) which will be inherited down to any objects located on this track and on child tracks, if not overridden more locally.
* with respect to _object identity,_ a given track-ID can have an incarnation or manifestation as real track-object within several ,EDLs<<renderengine-EDL>> (meaning you could select, disable or choose for rendering all objects in any EDL placed onto this track). Moreover, the track-_object_ and the _placement_ of this track within the tree of tracks of a given EDL are two distinguishable entities (meaning a given track -- with a couple of objects located on it -- could be placed differently several times within the same EDL, for example with different start offset or with different layering, output mode or pan position)

=== Identification
Tracks thus represent a blend of several concepts, but depending on the context it is allways clear which aspect is meant. Seen as ,assets<<renderengine-Asset>>, tracks are known by a unique track-ID, which can be either ,queried<<renderengine-ConfigQuery>>, or directly refered to by use of the asset-ID (which is a globally known hash). Usually, all referrals are via track-ID, including when you ,place<<renderengine-Placement>> an object onto a track. 
Under some cincumstances though, especially from within the ,Builder>>, we refer to a +Placement<Track>+ rather, denoting a specific instantiation located at a distinct node within the tree of tracks of a given EDL. These latter referrals are always done by direct object reference, e.g. while traversing the track tree (generally there is no way to refer to a placement by name).

=== creating tracks
Similar to ,pipes<<renderengine-Pipe>> and ,processing patterns<<renderengine-ProcPatt>>, track-assets need not be created, but rather leap into existence on first referral. On the contrary, you need to explicitly create the +Placement<Track>+ for attaching it to some node within the tree of tracks of an EDL. The public access point for creating such a placement is +asset::Struct::create(trackID+ (i.e. the +asset::StructFactory+), which, as a convenience shortcut, can also be accessed from the interface of the track-objects within the EDL for adding new child tracks to a given track.

=== removal
Deleting a Track is an operation with drastic consequences, as it will cause the removal of all child tracks and the deletion of _all object placements to this track,_ which could cause the resepctive objects to go out of scope (being deleted automatically by the placements or other smart pointer classes in charge of them). On the contrary, removing a +Placement<Track>+ from the tree of tracks of an EDL will just cause all objects placed onto this track to disappear (because they are no longer reachable for the build process). On re-adding it, they will show up again. (This is how things behave based on how we defined the relations of the entities to be. Another question is if we want to make this functionality available to the user. Judging from the use of Ardour's <Playlists>, such a feature may be quite helpful).

=== using Tracks
The '*Track Asset*' is a rather static object with limited capabilities. It's main purpose is to be a point of referral. Track assets have a description field and you may assign a list of ,tags<<renderengine-Tag>> to them (which could be used for binding ConfigRules).  Note that track assets are globally known within the session, they can't be limited to just one EDL (but you are allways free not to refer to some track from a given EDL). By virtue of this global nature, you can utilize the track assets to enable/disable a bunch of objects irrespective of what EDL they are located in, and probably it's a good idea to allow the selection of specific tracks for rendering.
Matters are quite different for the placement of a Track within the tree of tracks of a given EDL, and for placing some media object onto a given track. The track placement defines properties which will be inherited to all objects on this track and on all child tracks and thus plays a key role for wiring the objects up to some output pipe. Typically, the top level track of each EDL has a placement-to _the_ video and _the_ audio master pipe.

==== details to note
* Tracks are global, but the placement of a track is local within one EDL
* when objects are placed onto a track, this is done by referal to the global track asset ID. But because this placement of some media object is allways inherently contained within one EDL, the _meaning_ of such a placement is to connect to the properties of any track-placement of this given track _within this EDL._
* thus tracks-as-ID appear as something global, but tracks-as-propperty-carrier appear to the user as something local and object-like.
* in an extreme case, you'll add two different placements of a track at different points within the track tree of an EDL. And because the objects placed onto a track refer to the global track-ID, every object _on_ this track _within this EDL_ will show up two times independently and possibly with different inherited properties (output pipe, layering mode, pan, temporal position)
* an interesting configuration results from the fact that you can use an EDL as a ,+meta clip+ or +virtual clip+<<renderengine-VirtualClip>> within another EDL. In this case, you'll probably configure the tracks of the _inner_ EDL such as to send their output not to a global pipe but rather to the ,source ports<<renderengine-ClipSourcePort>> of the virtual clip (which are effectively local pipes). Thus, within the _outer_ EDL, you could attach effects to the virutal clip, combine it with transitions and place it onto another track, and any missing properties of this latter placement are to be resolved within the _outer_ EDL <br/>(it would be perfectly legal to construct a contrieved example when using the same track-ID within _inner_ and the _outer_ EDL. Because the Placement of this track will probably be different in the both EDLs, the behaviour of this placement could be quite different in the _inner_ and the _outer_ EDL. All of this may seem weird when discussed here in a textual and logical manner, but when viewed within the context and meaning of the various entities of the application, it's rather the way you'd expect it to be: you work locally and things behave as defined locally)
* note further, the root of the tree of tracks within each EDL _is itself again a_ +Placement<Track>+. There is no necessitiy for doing it this way, but it seemed more stright forward and logical to Ichthyo, as it allowes for an easy way of configuring some things (like ouput connections) as a default within one EDL. As every track can have a list of child tracks, you'll get the 'list of tracks' you'd expect.
* a nice consequence of the latter is: if you create a new EDL, it automatically gets one top-level track to start with, and this track will get a default configured placement (according to what is defined as ,default<<renderengine-DefaultsManagement>> within the current ConfigRules) -- typically starting at t=0 and being plugged into the master video and master audio pipe
* nothing prevents us from putting several objects at the same temporal location within one track. If the builder can't derive any additional layering information (which could be provided by some other configuration rules), then _there is no layering precedence_ -- simply the object encountered first (or last) wins.
* obviously, one wants the 'edit function' used to create such an overlapping placement also to create an ,transition<<renderengine-TransitionsHandling>> between the overlapping objects. Meaning this edit function will automatically create an transition processor object and provide it with a placement such as to attach it to the region of overlap.



<div title="TrackPipeEDL" modifier="Ichthyostega" modified="200904242058" created="200711300405" tags="design discuss def decision Builder" changecount="27">
*towards a definition of »Track«*. We don't want to tie ourself to some naive and overly simplistic definition, just because it is convenient. For classical (analogue) media, tracks are physical entities dictated by the nature of the process by which the media works. Especially, Tape machines have read/writing heads, which creates fixed tracks to which to route the signals. This is a practical geometric necessity. For digital media, there is no such necessity. We are bound primarily by the editor's habits of working.

===== Assessment of Properties
Media are used as Clips (contiguous chunks), they are a compound of several elementary streams, and they have a temporal extension (length). Indeed, the temporal dimension is the only fundamental property that can't be changed. Orthogonal to this dimension, we find one or more organisational dimensions forming a grid:
* a media stream may be sent to one of several possible output destinations (image or sound, subgroup busses, MIDI instruments)
* for any given output destination there may be variations in the _way of connecting_ (overlay mode and layer, pan position, MIDI channel)
* besides, we often just want to stash away some clip without using it, e.g. as an alternative or for later referral
This is to say we have _several degrees of freedom_ within this organisational grid. Just because some sound is located on this track doesn't mean he will be sent to a given output, rather the clip is located on this track _and_ is connected to that output _and_ -- supposed we have full-periphonic surround sound -- it is located 60° to the right and with 30° elevation. Combined with the (always contiguous) temporal dimension, this discrete grid is thus extruded to form something like discrete Tracks.

===== do we really need Tracks?
Starting with the assumption 'everything is just connected processing nodes', Tracks may seem superfluous. The problem with this approach is: it doesn't scale well. While it is fine to be able to connect clips and effects as we see fit (indeed, we want to build such a system), it is clearly not feasible to wire every clip manually to the output ports or add a panner effect to each and every audio sample. Because while editing, most of the time things are done in a fairly regular and systematic manner. Preferably we use the tracks as _preconfigured group setup_ and just _place media onto them;_ any such ,Placement>> can do the necessary wiring semi-automatic (rule-based).

===== the constant part
there seems to be some non time-varying part in each EDL, that doesn't fit well with the simple model 'objects on a timeline'. Tracks seen as an organisational grid fall into this category: they are a global property of the given EDL. They could be associated to the Session as a whole, but effectively this would subvert the concept of having ,several EDLs<<renderengine-SessionOverview>>. On the other hand, 
,pipes<<renderengine-Pipe>> for Video and Sound output are obviously a global property of the Session. There can be several global pipes forming a matrix of subgroup busses. We could add ports or pipes to tracks by default as well, but we don't do this, because, again, this would run counter to our attempt of treating tracks as merely organisational entities. We have special ,source ports<<renderengine-ClipSourcePort>> on individual clips though, and we will have ports on ,virtual clips<<renderengine-VirtualClip>> too.

=== Design
,Tracks<<renderengine-Track>> are just a structure used to organize the Media Objects within the EDL. They form a grid, and besides that, they have no special meaning. It seems convenient to make the tracks not just a list, but allow grouping (tree structure) right from start. +MObjects+ are *placed* rather than wired. The wiring is derived from the +Placement+. Placing can happen in several dimensions:
* placing in time will define when to activate and show the object.
* placing onto a track associates the MObject with this track; the GUI will show it on this track and the track may be used to resolve other properties of the object.
* placing to a +Pipe+ brings the object in conjunction with this pipe for the build process. It will be considered when building the render network for this pipe. Source-like objects (clips and exit nodes of effect chains) will be connected to the pipe, while transforming objects (effects) are inserted at the pipe. (you may read 'placed to pipe X' as 'plug into pipe X')
* depending on the nature of the pipe and the source, placing to some pipe may create additional degrees of freedom, demanding the object to be placed in this new, additional dimensions: Connecting to video out e.g. creates an overlay mode and a layer position which need to be specified, while connecting to a spatial sound system creates the necessity of a pan position. On the other hand, placing a mono clip onto a mono Pipe creates no additional degrees of freedom.
Placements are 'resolved' resulting in an ExplicitPlacement. In most cases this is just a gathering of properties, but as Placements can be incomplete and relative, there is room for real solving. The resolving mechanism tries to 'derive missing properties' from the 'context': When a clip isn't placed to some pipe but to a Track, than the Track and its parents will be inspected. If some of them has been placed to a pipe, the object will be connected to this pipe. Similar for layers and pan position. This is done by ,Placement>> and LocatingPin; as the ,Builder>> uses ExplicitPlacements, he isn't concerned with this resolving and uses just the data they deliver to drive the ,basic building operations<<renderengine-BasicBuildingOperations>>
-> ,Definition<<renderengine-Track>> and ,handling of Tracks<<renderengine-TrackHandling>>
-> ,Definition<<renderengine-Pipe>> and ,handling of Pipes<<renderengine-PipeHandling>>



<div title="TransitionsHandling" modifier="Ichthyostega" modified="200805300100" created="200712080417" tags="def design" changecount="3">
Transitions combine the data from at least two processing chains and do this combining in a time varying fashion. So, any transition has
* N input connections
* either one or N output connections
* temporal coordinates (time, length)
* some control data connection to a ParamProvider, because in the most general case the controling curves are treated similar to  ,automation data<<renderengine-AutomationData>>

===== how much output ports?
The standard case of a transition is sort of mixing together two input streams, like e.g. a simple dissolve. For this to be of any use, this input streams need to be connected to the same ouput destination before and after the transition (with regards to the timeline), i.e. the inputs and the transition share placement to the same output pipe. In this case, when the transition starts, the direct connections can be suspended and the transition will switch in seamlessly.

Using transitions is a very basic task and thus needs viable support by the GUI. Handling of transitions need to be very convienient, because it is so important. Because of this, it is compelling to subsume a more complicated situation and treat this more complicated case similar. This is the case, when two (or N) elements have to be combined in the way of a transition, but their further processing in the processing chain _after_ the transition needs to be different, maybe they belong to differnent subgroups, have to appear on different layers or with different pan positions. Of courese, the workaround is simple, at least _simple_ from the programmers point of view. It is _not_ simple from the editor's point of view the moment the editor has to do this simple thing (changing the wiring and add manualy synced fade curves to the individual parts) a dozend or even a hundred times in some larger project.

Because of this experience, ichthyo wants to support a more general case of transitions, which have N output connections, behave similar to their _simple_ counterpart, but leave out the mixing step. As a plus, such transitions can be inserted at the source ports of N clips or between any intermediary or final output pipes as well. Any transition processor capable of handling this situation should provide some flag, in order to decide if he can be placed in such a manner. (wichin the builder, encountering a  inconsistently placed transition is just an ,building error<<renderengine-BuildingError>>)




<div title="VirtualClip" modifier="Ichthyostega" modified="200808160257" created="200804110321" tags="def" changecount="13">
A *Meta-Clip* or *Virtual Clip* (both are synonymous) denotes a clip which doesn't just pull media streams out of a source media asset, but rather provides the results of rendering a complete sub-network. In all other respects it behaves exactly like a _real_ clip, i.e. it has ,source ports<<renderengine-ClipSourcePort>>, can have attached effects (thus forming a local render pipe) and can be placed and combined with other clips. Depending on what is wired to the source ports, we get two flavours:
* a 'placeholder clip' has no 'embedded' content. Rather, by virtue of placements and wiring requests, the output of some other pipe somewhere in the session will be wired to the clip's source ports. Thus, pulling data from this clip will effectively pull from these source pipes wired to it.
* a 'nested EDL' is like the other EDLs in the Session, just that any missing placement properties will be derived from the Virtual Clip, which is thought as to 'contain' the objects of the nested EDL. Typically, this also ,configures the tracks<<renderengine-TrackHandling>> of the 'inner' EDL such as to connect any output to the source ports of the Virtual Clip.

Like any _real_ clip, Virtual Clips have a start offset and a length, which will simply translate into an offset of the frame number pulled from the Virtual Clip's source connection or embedded EDL, making it possible to cut, splice, trim and roll them as usual. This of course implies we can have several instances of the same virtual clip with different start offset and length placed differently. The only limitation is that we can't handle cyclic dependencies for pulling data (which has to be detected and flagged as an error by the builder)



<div title="VisitingToolImpl" modifier="Ichthyostega" modified="200802031822" created="200801032003" tags="impl excludeMissing" changecount="21">
The *Visitor Pattern* is a special form of _double dispatch_ -- selecting the function actually to be executed at runtime based both on the concrete type of some tool object _and _ the target this tool is applied to. The rationale is to separate some specific implementation details from the basic infrastructure and the global interfaces, which can be limited to describe the fundamental properties and operations, while all details relevant only for some specific sub-problem can be kept together encapsulated in a tool implementation class. Typically, there is some iteration mechanism, allowing to apply these tools to all objects in a given container, a collection or object graph, without knowing the exact type of the target and tool objects. See the ,Visitor Pattern design discussion<<renderengine-VisitorUse>>

=== Problems with Visitor
The visitor pattern is not very popular, because any implementation is tricky, difficult to understand and often puts quite some burden on the user code. Even Erich Gamma says that on his list of bottom-ten patterns, Visitor is at the very bottom. This may be due to the fact that this original visitor implementation (often called the *GoF visitor*) causes a cyclic dependency between the target objects and the visiting tool objects, and includes some repetitive code (which results in silent failure if forgotten). Robert Martin invented 1996 an interesting variation (commonly labled *acyclic visitor*). By using a marker base class for each concrete target type to be treated by the visitor, and by applying a dynamic cast, we can get rid of the cyclic dependencies. The top level +Visitor+ is reduced to a mere empty marker interface in this design, while the actual visiting capabilities for a concrete target object is discovered at application time by the aforementioned dynamic cast. Besides the runntime cost of such a cast, the catch is that now the user code has still more responsibilities, because the need to maintain consistently two parallel object hierarchies. 
Long time there seemed to be not much room for improvement, at least before the advent of generic programming and the discovery of template metaprogramming. *Loki* (Alexandrescu, 2000) showed us how to write a library implementation which hides away the technicallities of the visitor pattern and automatically generates most of the repetitive code.

=== Requirements
* cyclic dependencies should be avoided or at least restricted to some central, library related place.
* The responsibilities for user code should be as small as possible. Especially, we should minimize the necessity to do corresponding adjustments to separate code locations, e.g. the necessity to maintain parallel hierarchies.
* Visitor is about _double dispatch,_ thus we can't avoid using some table lookup implementation -- more specifically we can't avoid using the cooperating classes vtables. We can expect at least two table lookups for each call dispatch. Besides that, the implementation should not be too wasteful...
* individual 'visiting tool' implementation classes should be able to opt in or opt out on implementing functions treating some of the visitable subclasses.
* there should be a safe fallback mechanism backed by the visitable object's hierarchy relations. If some concrete visiting tool class decides not to implement a +treat(..)+-function for some concrete target type, the call should fall back to the next best match according to the target object's hierarchy, i.e. the next best +treat(..)+-function should be used.
The last requirement practically rules out the Loki acyclic visitor, because this implementation calls a general fallback function when an exact match based on the target object's type is not possible. Considering our use of the visitor pattern within the render engine builder, such a solution would not be of much use: Some specific builder tool may implement a +treat(CompoundClip&amp;)+-function, while most of the other builder tools just implement a +treat(Clip&amp;)+-function, thus handling any multichannel clip via the general clip interface. This is exactly the reason why we want to use visitor at first place. Isolate specific treatment, implement against interfaces.

=== Implementation Notes
A good starting point for understanding our library implementation of the visitor pattern is +tests/components/common/visitingtoolconcept.cpp+, which contains a all-in-one-file proof of concept implementation, on which the real implementation (++common/visitor.hpp++) was based.
* similar to Loki, we use a +Visitable+ base class and a +Tool+ base class (we prefer the name +Tool+ over +Visitor+, because it makes the intended use more clear).
* the objects in the +Visitable+ hierarchy implement an +apply(Tool&amp;)+-function. This function needs to be implemented in a very specific manner, thus the +DEFINE_PROCESSABLE_BY+ macro should be used when possible to insert the definition into a concrete +Visitable+ class.
* similar to the acyclic visitor, the concrete visiting tool classes inherit from +Applicable<TARGET, ...>+ marker base classes, where the template parameter +TARGET+ is the concrete Visitable type this tool wants to treat, either directly by defining a +treat(ConcreteVisitable&amp;)+, or by falling back to some more general +treat(...)+ function.
* consequently our implementation is _rather not acyclic_ -- the concrete tool implementation depends on the full definition (header) of all concrete Visitables, but we avoid cyclic dependencies on the interface level. By using a typelist technique inspired by Loki, we can concentrate these dependencies in one single header file, which keeps things maintainable.
* we use the +Applicable<TARGET, ...>+ marker base classes to drive the generation of Dispatcher classes, each of which holds a table of trampoline functions for carrying out the actual double dispatch at call time. Each concrete Visitable using the +DEFINE_PROCESSABLE_BY+-macro generates a separate Dispatcher table containing slots for each concrete tool implementation class. To store and access the index position for these 'call slots', we use a tag associated with the concrete visiting tool class, which can be retrieved by going though the tool's vtable
* 'runtime cost': the concrete tool's ctor stores the trampoline pointers (this could be optimized to be a 'once per class' initialisation). Then, for each call, we have 2 virtual function calls and a lookup and call of the trampoline function, typically resulting in another virtual function call for resolving the +treat(..)+ function on the concrete tool class
* 'extension possibilities': while this system may seem complicated, you should note that it was designed with special focus on extension and implementing against interfaces:
** not every Visitable subclass needs a separate Dispatcher. As a rule of thumb, only when some type needs to be treated separately within some concrete visiting tool (i.e. when there is the need of a +treat(MySpecialVisitable&amp;)+), then this class should use the +DEFINE_PROCESSABLE_BY+-macro and thus define it's own +apply()+-function and Dispatcher. In all other cases, it is sufficient just to extend some existing Visitable, which thus will act as an interface with regards to the visiting tools.
** because the possibility of utilizing virtual +treat(...)+ functions, not every concrete visiting tool class needs to define a set of +Applicable<...>+ base classes (and thus get a separate dispatcher slot). We need such only for each _unique set_ of Applicables. All other concrete tools can extend existing tool implementations, sharing and partially extending the same set of virtual +treat()+-functions.
** when adding a new 'first class' Visitable, i.e. a concrete target class that needs to be treated separately in some visiting tool, the user needs to include the +DEFINE_PROCESSABLE_BY+ macro and needs to make sure that all existing 'first class' tool implementation classes include the Applicable base class for this new type. In this respect, our implementation is clearly 'cyclic'. (Generally speaking, the visitor pattern should not be used when the hierarchy of target objects is frequently extended and remoulded). But, when using the typelist facillity to define the Applicable base classes, we'll have one header file defining these collection of Applicables and thus we just need to add our new concrete Visitable to this header and recompile all tool implementation classes.
** when creating a new 'Visitable-and-Tool' hierarchy, the user should derive (or typedef) and parametrize the +Visitable+, +Tool+ and +Applicable+ templates, typically into a new namespace. An example can be seen in +proc/mobject/builder/buildertool.hpp+



<div title="VisitorUse" modifier="Ichthyostega" modified="200906072024" created="200711280302" tags="impl discuss" changecount="17">
Using the *Visitor Design Pattern* is somewhat controversial, mainly because this pattern is rather complicated, requires certain circumstances to be usefull, and -- especially when used in the original form described by Gamma et al -- puts quite some burden on the implementor. Contrary to some patterns commonly used today (like e.g. singleton or factory), visitor is by no way a popular pattern. Ichthyo thinks that until now it's potential stregths remain still to be discovered, due to obvious and well known weaknesses. This problems can be ameliorated a good deal by using template based library implementation techniques along the lines of the ,Loki library<<renderengine-http:_loki-lib.sourceforge.net/>>.
-> ,implementation deatails<<renderengine-VisitingToolImpl>>

=== why bothering with visitor?
In the Lumiera Proc layer, Ichthyo uses the visitor pattern to overcome another notorious problem when dealing with more complex class hierarchies: either, the _interface_ (root class) is so unspecific to be almost useless, or, in spite of having a useful contract, this contract will effectively be broken by some subclasses ('problem of elliptical circles'). Initially, when designing the classes, the problems aren't there (obviously, because they could be taken as design flaws). But then, under the pressure of real features, new types are added later on, which _need to be in this hierarchy_ and at the same time _need to have this and that special behaviour_ and here we go ...
Visitor helps us to circumvent this trap: the basic operations can be written against the top level interface, such as to include visiting some object collection internally. Now, on a case-by-case base, local operations can utilise a more specific sub interface or the given concrete type's public interface. So visitor helps to encapsulate specific technical details of cooperating objects within the concrete visiting tool implementation, while still forcing them to be implemented against some interface or sub-interface of the target objects.

==== well suited for using visitors
generally speaking, visitors are preferable when the underlying element type hierarchy is rather stable, but new operations are to be added frequently.
* Implementation of the builder. All sorts of special treatments of some MObject kinds can be added later
* Operating on the Object collection within the EDL. Effectively, this decouples the invoking interface on the EDL completely from the special operations to be carried out on individual objects.

To see an simple example of our 'visiting tool', have a look at +tests/components/common/visitingtooltest.cpp+
</pre>
</div>


<div title="WalkThrough" modifier="Ichthyostega" modified="200805300124" created="200706210625" tags="overview" changecount="41">
The Intention of this text is to help you understanding the design and to show some notable details.

=== Starting Point
Design is an experiment to find out how things are related. We can't _plan_ a Design top down, rather we have to start at some point with some hypothesis and look how it works out. The point of origin for Ichthyo's design is the observation that the Render Engine needs some Separation of Concerns to get the complexity down. And especially, this design *uses three different Levels* or Layers within the Render Engine and EDL.
* the 'high level' within the EDL uses uniformly treated MObjects which are assembled/glued together by a network of ,Placements<<renderengine-Placement>>.<br> It is supposed that the GUI will present this and _only this view_ to the user, giving him the ability to work with the objects
* the 'builder level' works on a stripped-down subset of this MObject network: it uses the _same Object instances_ but only assembled by ,Explicit Placements<<renderengine-ExplicitPlacement>> which locate the objects _on a simple (track, time) grid._ It's the job of the builder to create out of this simplified Network the Configuration of ,Render Nodes<<renderengine-ProcNode>> needed to do the actual rendering
* the 'engine level' uses solely ,Render Pipeline Nodes (ProcNode)<<renderengine-ProcNode>>, i.e. a Graph of interconnected processing nodes. The important constraint here is that _any decisions are ruled out_. The core Render Engine with all its nodes is lacking the ability to do any tests and checks and has no possibility to branch or reconfigure anything. (this is an especially important lesson I draw from studying the current Cinelerra source code)

=== Performance Considerations
* within the Engine the Render Nodes are containing the *inner loop*, whose contents are to be executed hundred thousands to million times per frame. Every dispensable concern, which is not strictly necessary to get the job done, is worth the effort of factoring out here.
* performance pressure at the builder level is far lower, albeit still existent. Compared to the effort of calculating a single processing step, looping even over some hundred nodes and executing quite some logic is negligible. Danger bears on creating memory pressure or causing awkward execution patterns (in the backend) rather. So the main concern should be the ability of reconfiguring different aspects separately without much effort. If for example a given render strategy works out to create lots of deadlocks and waitstates in the backend, the design should account for the possibility to exchange it with another strategy without having to modify the inner workings of the build process.<br>On the other hand, I wouldn't be overly concerned to trigger the build process yet another time to get some specific problem solved. However, the possibility to share one Render configuration for, say, 20 sec of video, instead of triggering the build process 500 times for every frame in this timespan, would sure be worth considering if it's not overly complicated to achieve.
* contrary to this, the EDL level is harmless with respect to performance. Getting acceptable responsiveness on user interactions is sufficient. We could consider using high level languages here, for it is much more important being able to express and handle complicated object relationships with relative ease. The only (indirect) concern is to avoid generating memory pressure inadvertently. Edit actions generating memory peaks could interfere with an ongoing background render process. If we decide to use lots of relation objects or transient objects, we should use an object pool or still better an garbage collector.

=== Concepts and Interfaces
This design strives to build each level and subsystem around some central concepts, which are directly expressed as Interfaces. Commonly used Interfaces clamp the different layers.
* MObject gives an uniform view on all the various entities to be arranged in the EDL.
* all the arranging and relating of MObjects is abstracted as ,Placement>>. The contract of a Placement is that it always has a related Subject, that we can change the _way of placement_ by adding and removing ,'locating pins'<<renderengine-LocatingPin>>, call some test methods on it (still to be defined), and, finally, that we can get an ExplicitPlacement from it.
* albeit being a special form of a Placement, the ExplicitPlacement is treated as a separate concept. With respect to edit operations within the EDL, it can stand for any sort of Placement. On the other hand the Builder takes a list of ExplicitPlacements as input for building up the Render Engine(s). This corresponds to the fact that the render process needs to organize the things to be done on a simple two dimensional grid of (output channel / time). The (extended) contract of an ExplicitPlacement provides us with this (output,time).
* on the lower end of the builder, everything is organized around the Concept of a ProcNode, which enables us to _pull_ one (freely addressable) Frame of calculated data. Further, the ProcNode has the ability to be wired with other nodes and ,Parameter Providers<<renderengine-ParamProvider>>
* the various types of data to be processed are abstracted away under the notion of a ,Frame>>. Basically, a Frame is an Buffer containing an Array of raw data and it can be located by some generic scheme, including (at least) the absolute starting time (and probably some type or channel id).
* All sorts of (target domain) ,parameters<<renderengine-Parameter>> are treated uniformly. There is a distinction between Parameters (which _could_ be variable) and Configuration (which is considered to be fixed). In this context, ,Automation>> just appears as a special kind of ParamProvider.
* and finally, the calculation _process_ together with its current state is represented by a StateProxy. I call this a 'proxy', because it should encapsulate and hide all tedious details of communication, be it even asynchronous communication with some Controller or Dispatcher running in another Thread. In order to maintain a view on the current state of the render process, it could eventually be necessary to register as an observer somewhere or to send notifications to other parts of the system.

=== Handling Diversity
An important goal of this approach is to be able to push down the treatment of variations and special cases. We don't need to know what kind of Placement links one MObject to another, because it is sufficient for us to get an ExplicitPlacement. The Render Engine doesn't need to know if it is pulling audio Frames or video Frames or GOPs or OpenGL textures. It simply relies on the Builder wiring together the correct node types. And the Builder in turn does so by using some overloaded function of an iterator or visitor. At many instances, instead of doing decisions in-code or using hard wired defaults, a system of ,configuration rules<<renderengine-ConfigRules>> is invoked to get a suitable default as a solution (and, as a plus, this provides points of customisation for advanced users). At engine level, there is no need for the video processing node to test for the colormodel on every screen line, because the Builder has already wired up the fitting implementation routine. All of this helps reducing complexity and quite some misconceptions can be detected already by the compiler.

=== Explicit structural differences
In case it's not already clear: we don't have _the_ Render Engine, rather we construct a Render Engine for each structurally differing part of the timeline. (please relate this to the current Cinelerra code base, which constructs and builds up the render pipeline for each frame separately). No need to call back from within the pipeline to find out if a given plugin is enabled or to see if there are any automation keyframes. We don't need to pose any constraints on the structuring of the objects in the EDL, besides the requirement to get an ExplicitPlacement for each. We could even loosen the use of the common metaphor of placing media sequences on fixed tracks, if we want to get at a more advanced GUI at some point in the future.

=== Stateless Subsystems
The >current setup< of the objects in the EDL is sort of a global state. Same holds true for the Controller, as the Engine can be at playback, it can run a background render or scrub single frames. But the whole complicated subsystem of the Builder and one given Render Engine configuration can be made *stateless*. As a benefit of this we can run this subsystems multi-threaded without the need of any precautions (locking, synchronizing). Because all state information is just passed in as function parameters and lives in local variables on the stack, or is contained in the StateProxy which represents the given render _process_ and is passed down as function parameter as well. (note: I use the term 'stateless' in the usual, slightly relaxed manner; of course there are some configuration values contained in instance variables of the objects carrying out the calculations, but this values are considered to be constant over the course of the object usage).


<div title="WiringDescriptor" modifier="Ichthyostega" modified="200807132352" created="200807132338" tags="Rendering impl spec dynamic" changecount="3">
Each ,processing node<<renderengine-ProcNode>> contains a stateless (+const+) descriptor detailing the inputs, outputs and predecessors. Moreover, this descriptor contains the configuration of the call sequence yielding the >data pulled from predecessor(s)<. The actual type of this object is composed out of several building blocks (policy classes) and placed by the builder as a template parameter on the WiringDescriptor of the individual ProcNode. This happens in the WiringFactory in file +nodewiring.cpp+, which consequently contains all the possible combinations (pre)generated at compile time.

=== building blocks
* *Caching*: whether the result frames of this processing step will be communicated to the Cache and thus could be fetched from there instead of actually calculating them.
* *Process*: whether this node does any calculations on it's own or just pulls from a source
* *Inplace*: whether this node is capable of processing the result 'in-place', thereby overwriting the input buffer
* *Multiout*: whether this node produces multiple output channels/frames in one processing step

==== implementation
===== Caching
When a node participates in *caching*, a result frame may be pulled immediately from cache instead of calculating it. Moreover, _any output buffer_ of this node will be allocated _within the cache._ Consequently, caching interferes with the ability of the next node to calculate 'in-Place'. In the other case, when *not using the cache*, the +pull()+ call immediately starts out with calling down to the predecessor nodes, and the allocation of output buffer(s) is always delegated to the parent state (i.e. the StateProxy pulling results from this node). 

Generally, buffer allocation requests from predecessor nodes (while being pulled by this node) will either be satisfied by using the 'current state', or treated as if they were our own output buffers when this node is in-Place capable.

===== Multiple Outputs
Some simplifications are possible in the default case of a node producing just *one single output* stream. Otherwise, we'd have to allocate multiple output buffers, and then, after processing, select the one needed as a result and deallocate the superfluous further buffers.

===== in-Place capability
If a node is capable of calculating the result by *modifying it's input* buffer(s), an important performance optimization is possible, because in a chain of in-place capable nodes, we don't need any buffer allocations. But, on the other hand, this optimization may collide with the caching, because a frame retrieved from cache must not be modified.
Without this optimization, in the base case each processing needs an input and an output. Exceptionally, we could think of special nodes which _require_ to process in-place, in which case we'd need to provide it with a copy of the input buffer to work on. 

===== Processing
If *not processing* we don't have any input buffers, instead we get our output buffers from an external source.
Otherwise, in the default case of actually *processing* out output, we have to organize input buffers, allocate output buffers, call the +process()+ function of the WiringDescriptor and finally release the input buffers.


<div title="WiringRequest" modifier="Ichthyostega" modified="200810060404" created="200810060344" tags="def spec" changecount="2">
[red]+This is an early draft as of 9/08+
Wiring requests rather belong to the realm of the high-level model, but play an important role in the build process, because the result of 'executing' a wiring request will be to establish an actual low-level data connection. Wiring requests will be created automatically in the course of the build process, but they can be created manually and attached to the media objects in the high-level model as a *wiring plug*, which is a special kind of LocatingPin ( -> ,Placement>>)

Wiring requests are small stateful value objects. They will be collected, sorted and processed ordered, such as to finish the building and get a completely wired network of processing nodes. Obviously, there needs to be some reference or smart pointer to the objects to be wired, but this information remains opaque.

-> ConManager


<div title="automation" modifier="Ichthyostega" modified="200805300125" created="200805300057" tags="overview" changecount="8">
The purpose of automation is to vary a parameter of some data processing instance in the course of time while rendering. Thus, automation encompasses all the variability within the render network _which is not a structural change_.


=== Parameters and Automation

<<renderengine-Automation>> is treated as a function over time. Everything beyond this definition is considered an implementation detail of the ,parameter provider<<renderengine-ParamProvider>> used to yield the value. Thus automation is closely tied to the concept of a ,Parameter>>, which also plays an important role in the communication with the GUI and while ,setting up and wiring the render nodes<<renderengine-BuildRenderNode>> in the course of the build process ( -> see ,tag:Builder<<renderengine-Builder>>)

<div title="def" modifier="Ichthyostega" created="200902080726" changecount="1">
Definition of commonly used terms and facilities...
